{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Quantization Tutorial\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE as part of the MASERT framework. The following demonstrations were run on a NVIDIA RTX A2000 GPU with a Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz CPU.\n",
    "\n",
    "## Section 1. INT8 Quantization\n",
    "Firstly, we will show you how to do a int8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each using their own individual pass, and are explained in depth at each subsection:\n",
    "\n",
    "1. [Fake quantization](#section-11-fake-quantization): `tensorrt_fake_quantize_transform_pass`\n",
    "2. [Calibration](#section-12-calibration): `tensorrt_calibrate_transform_pass`\n",
    "3. [Quantized Aware Training](#section-13-quantized-aware-training-qat): `tensorrt_fine_tune_transform_pass`\n",
    "4. [Quantization](#section-14-tensorrt-quantization): `tensorrt_engine_interface_pass`\n",
    "5. [Analysis](#section-15-performance-analysis): `tensorrt_analysis_pass`\n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"src\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.utils import to_numpy_if_tensor\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.tools import get_cf_args, get_dummy_input\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model, get_tokenizer\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
    "from chop.passes.graph import (\n",
    "    summarize_quantization_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    runtime_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dependency (the dependent package \"cuda\" refers to \"cuda-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mExtension: All dependencies for TensorRT pass are available.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chop.tools.check_dependency import check_deps_tensorRT_pass\n",
    "check_deps_tensorRT_pass(silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}\n",
      "{'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "# Path to your TOML file\n",
    "# JSC_TOML_PATH = 'toy_INT8_quantization_by_type.toml'\n",
    "JSC_TOML_PATH = 'resnet18_INT8_quantization_by_type.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(JSC_TOML_PATH, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt' section and its children\n",
    "tensorrt_config = pass_args.get('passes', {}).get('tensorrt', {})\n",
    "print(tensorrt_config)\n",
    "# Extract the 'passes.runtime_analysis' section and its children\n",
    "runtime_analysis_config = pass_args.get('passes', {}).get('tensorrt', {}).get('runtime_analysis', {})\n",
    "print(runtime_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a model and training it using the toml configuration model arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n"
     ]
    }
   ],
   "source": [
    "from chop.dataset import MaseDataModule\n",
    "from chop.models import get_model_info\n",
    "from chop.models import get_model\n",
    "from chop.tools.get_input import InputGenerator\n",
    "\n",
    "# Load the basics in\n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Add the data_module and other necessary information to the configs\n",
    "configs = [tensorrt_config, runtime_analysis_config]\n",
    "for config in configs:\n",
    "    config['task'] = pass_args['task']\n",
    "    config['dataset'] = pass_args['dataset']\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    # task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the `jsc-toy` model using the machop `train` action with the config from the toml file.\n",
    "\n",
    "Run the following command to train a jsc-toy.\n",
    "\n",
    "```bash\n",
    "cd machop\n",
    "./ch train --config ./configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0311 00:15:51.512130 140276942296128 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File | Manual Override |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                 |           cls            |\n",
      "| load_name               |           None           |              |                 |           None           |\n",
      "| load_type               |            mz            |              |                 |            mz            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                 |            64            |\n",
      "| to_debug                |          False           |              |                 |          False           |\n",
      "| log_level               |           info           |              |                 |           info           |\n",
      "| report_to               |       tensorboard        |              |                 |       tensorboard        |\n",
      "| seed                    |            0             |              |                 |            0             |\n",
      "| quant_config            |           None           |              |                 |           None           |\n",
      "| training_optimizer      |           adam           |              |                 |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                 |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                 |          0.001           |\n",
      "| weight_decay            |            0             |              |                 |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                 |            10            |\n",
      "| max_steps               |            -1            |              |                 |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                 |            1             |\n",
      "| log_every_n_steps       |            50            |              |                 |            50            |\n",
      "| num_workers             |            20            |              |                 |            20            |\n",
      "| num_devices             |            1             |              |                 |            1             |\n",
      "| num_nodes               |            1             |              |                 |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                 |           gpu            |\n",
      "| strategy                |           auto           |              |                 |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                 |          False           |\n",
      "| github_ci               |          False           |              |                 |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                 |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                 |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                 |           100            |\n",
      "| is_pretrained           |          False           |              |                 |          False           |\n",
      "| max_token_len           |           512            |              |                 |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                 | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                 |         e_output         |\n",
      "| project                 |           None           |              |                 |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                 |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                 |         cifar10          |\n",
      "| t_max                   |            20            |              |                 |            20            |\n",
      "| eta_min                 |          1e-06           |              |                 |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-11\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m##### WEIGHT DECAY ##### 0\u001b[0m\n",
      "I0311 00:15:51.711618 140276942296128 rank_zero.py:63] Using 16bit Automatic Mixed Precision (AMP)\n",
      "I0311 00:15:51.718478 140276942296128 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0311 00:15:51.718928 140276942296128 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0311 00:15:51.718978 140276942296128 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0311 00:15:54.418565 140276942296128 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0311 00:15:54.708575 140276942296128 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0:  53%|▌| 411/782 [00:11<00:10, 35.12it/s, v_num=0, train_acc_step=0.234]I0311 00:16:07.596064 140276942296128 rank_zero.py:63] \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 ./ch train --config /workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_INT8_quantization_by_type.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load in the checkpoint. You will have to adjust this according to where it has been stored in the mase_output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done!\n",
      "dummy in done\n",
      "_ done\n",
      "init_metadata_analysis_pass done\n",
      "add_common_metadata_analysis_pass done\n",
      "add_software_metadata_analysis_pass done\n",
      "metadata_value_type_cast_transform_pass done\n",
      "using safe deepcopy\n",
      "deep copy done\n"
     ]
    }
   ],
   "source": [
    "# Load in the trained checkpoint - change this accordingly\n",
    "JSC_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "\n",
    "model = load_model(load_name=JSC_CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "print(\"load model done!\")\n",
    "\n",
    "# Initiate metadata\n",
    "dummy_in = next(iter(input_generator))\n",
    "print(\"dummy in done\")\n",
    "\n",
    "_ = model(**dummy_in)\n",
    "print(\"_ done\")\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "print(\"init_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "print(\"add_common_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "print(\"add_software_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})\n",
    "print(\"metadata_value_type_cast_transform_pass done\")\n",
    "\n",
    "# Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
    "mg_original = deepcopy_mase_graph(mg)\n",
    "print(\"deep copy done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1 Fake Quantization\n",
    "\n",
    "Firstly, we fake quantize the module in order to perform calibration and fine tuning before actually quantizing - this is only used if we have int8 calibration as other precisions are not currently supported within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library.\n",
    "\n",
    "This is acheived through the `tensorrt_fake_quantize_transform_pass` which goes through the model, either by type or by name, replaces each layer appropriately to a fake quantized form if the `quantize` parameter is set in the default config (`passes.tensorrt.default.config`) or on a per name or type basis. \n",
    "\n",
    "Currently the quantizable layers are:\n",
    "- Linear\n",
    "- Conv1d, Conv2d, Conv3d \n",
    "- ConvTranspose1d, ConvTranspose2d, ConvTranspose3d \n",
    "- MaxPool1d, MaxPool2d, MaxPool3d\n",
    "- AvgPool1d, AvgPool2d, AvgPool3d\n",
    "- LSTM, LSTMCell\n",
    "\n",
    "To create a custom quantized module, click [here](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#document-tutorials/creating_custom_quantized_modules).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<chop.ir.graph.mase_graph.MaseGraph at 0x7f1f81d4c290>, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_config)\n",
    "# summarize_quantization_analysis_pass(mg_original, mg)\n",
    "summarize_quantization_analysis_pass(mg, pass_args={\"save_dir\": \"quantize_summary_res18\", \"original_graph\": mg_original})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have succesfully fake quantized all linear layers inside `jsc-toy`. This means that we will be able to simulate a quantized model in order to calibrate and fine tune it. This fake quantization was done on typewise i.e. for linear layers only. See [Section 4](#section-4-layer-wise-mixed-precision) for how to apply quantization layerwise - i.e. only first and second layers for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2 Calibration\n",
    "\n",
    "Next, we perform calibration using the `tensorrt_calibrate_transform_pass`. Calibration is achieved by passing data samples to the quantizer and deciding the best amax for activations. \n",
    "\n",
    "Calibrators can be added as a search space parameter to examine the best performing calibrator. The calibrators have been included in the toml as follows.\n",
    "For example: `calibrators = [\"percentile\", \"mse\", \"entropy\"]`\n",
    "\n",
    "Note: \n",
    "- To use `percentile` calibration, a list of percentiles must be given\n",
    "- To use `max` calibration, the `histogram` weight and input calibrators must be removed and replaced with `max`. This will use global maximum absolute value to calibrate the model.\n",
    "- If `post_calibration_analysis` is set true the `tensorrt_analysis_pass` will be run for each calibrator tested to evaluate the most suitable calibrator for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0310 23:09:30.086828 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.087807 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.088736 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.089525 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.090272 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.091339 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.092823 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.093685 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.094829 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.095562 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.096313 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.097071 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.097931 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.098931 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.100046 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.100876 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.101746 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.102971 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.103817 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.104693 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.107774 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.108904 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.109791 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.111651 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.113092 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.114372 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.116872 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.118120 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.119297 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.120401 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.121531 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.122604 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.125342 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.126914 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.128034 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.129766 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.131002 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.132203 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.133131 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.134725 139783100556352 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.135974 139783100556352 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0310 23:09:30.137032 139783100556352 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0310 23:09:30.139275 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0310 23:09:30.139766 139783100556352 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.142063 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.144186 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.145938 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.148636 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1024 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.150660 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.152981 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8246 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.154275 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.156760 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1390 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.158783 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.160536 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4689 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.162154 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.163935 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2644 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.165740 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.167748 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4689 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.170103 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.172066 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5989 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.173789 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.175827 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2767 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.177701 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.179032 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5476 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.181187 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.182896 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6613 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.184620 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.187221 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5476 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.189477 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.191732 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.193868 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.196177 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3432 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.197993 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.200202 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2839 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.201850 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.203709 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5832 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.206027 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.208611 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2839 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.211241 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.213699 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.0753 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.215933 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.218058 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1169 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.220320 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.221609 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:30.224400 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.7276    |\n",
      "|      Average Precision       |   0.71602   |\n",
      "|        Average Recall        |   0.71999   |\n",
      "|       Average F1 Score       |   0.71693   |\n",
      "|         Average Loss         |   0.81591   |\n",
      "|       Average Latency        |  39.783 ms  |\n",
      "|   Average GPU Power Usage    |  13.935 W   |\n",
      "| Inference Energy Consumption | 0.15399 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/mase_graph/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0310 23:09:38.387604 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.390047 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.391639 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4934 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.395589 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.397815 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2385 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.399990 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.402276 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.2434 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.403875 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.405268 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1565 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.406885 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.409100 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.0129 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.411905 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.414245 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3047 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.416527 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.418289 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.0129 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.420490 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.423034 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.424909 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.426834 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.4220 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.429757 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.431776 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.9093 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.433438 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.437365 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1279 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.439249 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.441494 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.9093 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.443979 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.445443 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6160 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.447357 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.450999 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.9239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.453023 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.454699 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.457731 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.459625 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7431 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.461667 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.465142 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.1965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.466928 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.469274 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3925 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.473378 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.475291 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1547 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.481039 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.482376 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:38.483850 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72792   |\n",
      "|      Average Precision       |   0.71837   |\n",
      "|        Average Recall        |   0.72165   |\n",
      "|       Average F1 Score       |   0.71922   |\n",
      "|         Average Loss         |   0.80667   |\n",
      "|       Average Latency        |  46.205 ms  |\n",
      "|   Average GPU Power Usage    |  12.912 W   |\n",
      "| Inference Energy Consumption | 0.16572 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/mase_graph/version_6/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0310 23:09:47.640414 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0610 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.642315 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.644246 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0836 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.646260 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.648053 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.649617 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.651127 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.7650 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.652663 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.654086 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.656054 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.658293 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7930 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.659837 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.662069 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2385 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.664213 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.666040 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7930 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.667738 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.669416 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7748 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.671767 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.673291 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4696 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.674923 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.677077 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.678779 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.680620 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.683523 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.685490 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.687136 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.689284 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5885 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.691333 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.692881 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0886 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.695231 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.697283 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2668 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.699506 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.701560 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7827 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.703221 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.705513 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.2668 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.706990 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.708636 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.0930 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.710474 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.712227 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6869 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.714486 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.715742 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:47.717634 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72838   |\n",
      "|      Average Precision       |   0.71933   |\n",
      "|        Average Recall        |   0.72248   |\n",
      "|       Average F1 Score       |    0.72     |\n",
      "|         Average Loss         |   0.80858   |\n",
      "|       Average Latency        |  36.405 ms  |\n",
      "|   Average GPU Power Usage    |  14.205 W   |\n",
      "| Inference Energy Consumption | 0.14364 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/mase_graph/version_7/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0310 23:09:55.564292 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3192 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:55.745403 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:56.090878 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4197 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:56.267316 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:56.533744 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8187 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:56.732277 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:57.028774 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0291 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:57.200337 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:57.530667 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.7298 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:57.708557 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:57.980620 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9698 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:58.158600 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:58.497549 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8276 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:58.678534 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:58.987776 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.9698 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:59.179262 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:59.455926 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.5025 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:59.636518 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:09:59.904904 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9682 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:00.077858 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:00.377648 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4216 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:00.552951 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:00.839324 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.7552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:01.013775 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:01.324856 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.4216 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:01.500630 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:01.674690 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0426 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:01.846192 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:02.109366 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:02.287894 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:02.543788 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.5107 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:02.723290 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:02.893208 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7396 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:03.063703 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:03.334080 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.5107 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:03.514607 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:03.692192 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7871 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:03.877250 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:04.052996 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:04.233922 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:04.237072 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:04.238130 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72871   |\n",
      "|      Average Precision       |   0.71876   |\n",
      "|        Average Recall        |   0.72206   |\n",
      "|       Average F1 Score       |   0.71949   |\n",
      "|         Average Loss         |   0.80976   |\n",
      "|       Average Latency        |  35.992 ms  |\n",
      "|   Average GPU Power Usage    |  13.388 W   |\n",
      "| Inference Energy Consumption | 0.13385 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/mase_graph/version_8/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0310 23:10:13.885905 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1254 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:15.287774 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:17.869325 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:19.262587 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:20.904940 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.9200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:22.302920 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:24.416126 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.7048 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:25.835671 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:27.987887 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.7832 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:29.518685 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:31.574753 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.7331 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:33.095922 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:35.666243 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.6078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:37.218079 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:39.248548 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.7331 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:40.673342 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:42.627535 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9309 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:44.090715 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:45.892155 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4361 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:47.433379 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:49.605613 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.4459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:51.061112 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:52.997444 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4981 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:54.391563 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:56.475762 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.4459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:57.888820 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:10:59.321276 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4642 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:00.732987 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:02.396314 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7289 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:03.777295 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:05.552287 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:07.004003 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:08.473033 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4155 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:09.909753 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:11.696107 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.8710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:13.243765 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:14.752644 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.7624 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:16.266199 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:17.824260 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:19.366901 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:19.370389 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0310 23:11:19.371969 139783100556352 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72909   |\n",
      "|      Average Precision       |   0.71899   |\n",
      "|        Average Recall        |   0.72248   |\n",
      "|       Average F1 Score       |   0.71988   |\n",
      "|         Average Loss         |   0.81183   |\n",
      "|       Average Latency        |  40.784 ms  |\n",
      "|   Average GPU Power Usage    |  13.082 W   |\n",
      "| Inference Energy Consumption | 0.14821 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/mase_graph/version_9/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, the 99% `percentile` clips too many values during the amax calibration, compromising the loss. However 99.99% demonstrates higher validation accuracy alongside `mse` and `entropy` for `jsc-toy`. For such a small model, the methods are not highly distinguished, however for larger models this calibration process will be important for ensuring the quantized model still performs well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3 Quantized Aware Training (QAT)\n",
    "\n",
    "The `tensorrt_fine_tune_transform_pass` is used to fine tune the quantized model. By default, when running the `tensorrt_engine_interface_pass` the fake quantized model will go through fine tuning however you stop this by setting the `fine_tune` in `passes.tensorrt.fine_tune` to false.\n",
    "\n",
    "For QAT it is typical to employ 10% of the original training epochs, starting at 1% of the initial training learning rate, and a cosine annealing learning rate schedule that follows the decreasing half of a cosine period, down to 1% of the initial fine tuning learning rate (0.01% of the initial training learning rate). However this default can be overidden by setting the `epochs`, `initial_learning_rate` and `final_learning_rate` in `passes.tensorrt.fine_tune`.\n",
    "\n",
    "The fine tuned checkpoints are stored in the ckpts/fine_tuning folder:\n",
    "\n",
    "```\n",
    "mase_output\n",
    "└── tensorrt\n",
    "    └── quantization\n",
    "        └──model_task_dataset_date\n",
    "            ├── cache\n",
    "            ├── ckpts\n",
    "            │   └── fine_tuning\n",
    "            ├── json\n",
    "            ├── onnx\n",
    "            └── trt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0310 23:11:28.264083 139783100556352 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0310 23:11:28.264981 139783100556352 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0310 23:11:28.265347 139783100556352 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0310 23:11:28.266793 139783100556352 rank_zero.py:63] You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "I0310 23:11:30.870971 139783100556352 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0310 23:11:30.883136 139783100556352 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac258df8edf49cb982c05864d4e5e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ea1cc64de34588a5459f343f95d1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96ef7512bf647319a25942501cea79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8b8b348fe6491f95d99d2961f9c338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0310 23:14:28.804015 139783100556352 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4 TensorRT Quantization\n",
    "\n",
    "After QAT, we are now ready to convert the model to a tensorRT engine so that it can be run with the superior inference speeds. To do so, we use the `tensorrt_engine_interface_pass` which converts the `MaseGraph`'s model from a Pytorch one to an ONNX format as an intermediate stage of the conversion.\n",
    "\n",
    "During the conversion process, the `.onnx` and `.trt` files are stored to their respective folders shown in [Section 1.3](#section-13-quantized-aware-training-qat).\n",
    "\n",
    "This interface pass returns a dictionary containing the `onnx_path` and `trt_engine_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/2025-03-10/version_32/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/2025-03-10/version_33/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/2025-03-10/version_34/model.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.5 Performance Analysis\n",
    "\n",
    "To showcase the improved inference speeds and to evaluate accuracy and other performance metrics, the `tensorrt_analysis_pass` can be used.\n",
    "\n",
    "The tensorRT engine path obtained the previous interface pass is now inputted into the the analysis pass. The same pass can take a MaseGraph as an input, as well as an ONNX graph. For this comparison, we will first run the anaylsis pass on the original unquantized model and then on the int8 quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72899   |\n",
      "|      Average Precision       |   0.71928   |\n",
      "|        Average Recall        |   0.72258   |\n",
      "|       Average F1 Score       |   0.72016   |\n",
      "|         Average Loss         |   0.81163   |\n",
      "|       Average Latency        |  42.229 ms  |\n",
      "|   Average GPU Power Usage    |  14.051 W   |\n",
      "| Inference Energy Consumption | 0.16482 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/mase_graph/version_10/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10/2025-23:18:06] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |    0.7402    |\n",
      "|      Average Precision       |   0.74165    |\n",
      "|        Average Recall        |   0.74348    |\n",
      "|       Average F1 Score       |   0.74182    |\n",
      "|         Average Loss         |   0.73583    |\n",
      "|       Average Latency        |  26.487 ms   |\n",
      "|   Average GPU Power Usage    |   11.386 W   |\n",
      "| Inference Energy Consumption | 0.083774 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-10/tensorrt/version_0/model.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_, _ = runtime_analysis_pass(mg_original, pass_args=runtime_analysis_config)\n",
    "_, _ = runtime_analysis_pass(meta['trt_engine_path'], pass_args=runtime_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the latency has decreased around 6x with the `jsc-toy` model without compromising accuracy due to the well calibrated amax and quantization-aware fine tuning and additional runtime optimizations from TensorRT. The inference energy consumption has thus also dropped tremendously and this is an excellent demonstration for the need to quantize in industry especially for LLMs in order to reduce energy usage. \n",
    "\n",
    "## Section 2. FP16 Quantization\n",
    "\n",
    "We will now load in a new toml configuration that uses fp16 instead of int8, whilst keeping the other settings the exact same for a fair comparison. This time however, we will use chop from the terminal which runs all the passes showcased in [Section 1](#section-1---int8-quantization).\n",
    "\n",
    "Since float quantization does not require calibration, nor is it supported by `pytorch-quantization`, the model will not undergo fake quantization; for the time being this unfortunately means QAT is unavailable and only undergoes Post Training Quantization (PTQ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 02:21:21.056977 140494406755392 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.087997 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088167 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088266 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088348 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088433 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088517 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088604 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088695 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088779 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088858 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088949 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089029 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089111 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089190 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089273 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089351 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089435 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089513 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089596 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089676 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089760 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089838 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089920 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089999 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090083 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090162 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090245 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090325 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090409 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090490 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090573 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090651 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090734 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090822 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090906 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090986 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091068 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091146 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091232 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091306 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091391 140494406755392 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091465 140494406755392 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0315 02:21:32.091680 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0315 02:21:32.091738 140494406755392 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.091941 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092101 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092239 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092375 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092510 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092655 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8358 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092797 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092946 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1359 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093083 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093218 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093343 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093477 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2610 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093605 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093736 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093876 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094006 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5872 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094131 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094259 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2720 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094383 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094517 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094642 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094769 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6489 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094891 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095019 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095145 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095278 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4016 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095404 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095533 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095656 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095781 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095990 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096137 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096266 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096399 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096525 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096652 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.9013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096777 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096904 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.097026 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.097127 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.097379 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72608    |\n",
      "|      Average Precision       |   0.71486    |\n",
      "|        Average Recall        |   0.71875    |\n",
      "|       Average F1 Score       |   0.71577    |\n",
      "|         Average Loss         |   0.81619    |\n",
      "|       Average Latency        |  29.604 ms   |\n",
      "|   Average GPU Power Usage    |   9.368 W    |\n",
      "| Inference Energy Consumption | 0.077035 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_10/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:21:38.361274 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.362315 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.362808 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4700 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363099 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363385 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363618 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363879 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364109 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364330 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364542 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364771 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365005 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365243 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365498 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365726 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366071 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366341 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1995 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366575 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366806 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367028 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367249 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367468 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367899 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.368262 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.368669 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369010 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369326 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6513 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369634 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369944 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1650 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.370243 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.370656 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.370944 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.371239 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.371532 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.371831 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.372119 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.372408 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.372699 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373008 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373312 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373595 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373790 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72868    |\n",
      "|      Average Precision       |   0.71952    |\n",
      "|        Average Recall        |   0.72268    |\n",
      "|       Average F1 Score       |    0.7203    |\n",
      "|         Average Loss         |   0.80705    |\n",
      "|       Average Latency        |  27.485 ms   |\n",
      "|   Average GPU Power Usage    |   10.18 W    |\n",
      "| Inference Energy Consumption | 0.077725 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_11/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:21:44.078727 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0570 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.079337 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.079652 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8774 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.079928 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080175 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4204 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080457 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080696 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080967 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081194 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081415 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081642 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081875 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082105 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082331 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082558 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082784 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083009 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083226 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083447 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083670 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083903 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.084133 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.084362 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.084606 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.085184 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.085549 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.085789 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086019 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086479 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7914 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086699 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086930 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087149 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087424 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1205 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087640 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087871 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.088093 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.088314 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7121 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.088780 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089005 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089281 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089517 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089663 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72872    |\n",
      "|      Average Precision       |   0.71967    |\n",
      "|        Average Recall        |   0.72279    |\n",
      "|       Average F1 Score       |   0.72034    |\n",
      "|         Average Loss         |   0.80904    |\n",
      "|       Average Latency        |  27.262 ms   |\n",
      "|   Average GPU Power Usage    |   10.725 W   |\n",
      "| Inference Energy Consumption | 0.081218 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_12/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:21:50.013751 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.193699 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.532325 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.707737 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.983930 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1439 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.152282 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.475319 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.654181 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.912652 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9089 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.092157 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.271842 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.456134 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.721934 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.898514 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.075565 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.263721 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.444887 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.6229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.630287 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.871474 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0186 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.046728 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.381313 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.556874 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.814447 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.001569 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.321210 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.501133 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.778420 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.960818 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.138005 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7037 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.309161 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.489939 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.672837 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.844730 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.204922 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.382050 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.563570 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.743386 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.922971 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.099598 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.280104 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.281308 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.281645 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72848    |\n",
      "|      Average Precision       |   0.71898    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |    0.7197    |\n",
      "|         Average Loss         |   0.80978    |\n",
      "|       Average Latency        |  28.014 ms   |\n",
      "|   Average GPU Power Usage    |   10.197 W   |\n",
      "| Inference Energy Consumption | 0.079347 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_13/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:22:06.211271 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:07.648026 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:10.083604 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4599 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:11.497441 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:13.300668 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.8759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:14.737156 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:17.501459 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1156 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:18.972856 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:20.775787 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5661 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:22.233590 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:23.680495 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:25.133930 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:27.128596 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:28.610648 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:30.076291 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:31.524113 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:32.976151 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:34.432886 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:35.951512 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.6632 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:37.443416 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:39.848795 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:41.291870 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:43.188360 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.0554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:44.606407 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:46.981904 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:48.373767 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:50.386144 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:51.804554 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:53.229105 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:54.592639 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:56.062018 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:57.493883 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:58.974630 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4003 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:00.391778 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:01.820893 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:03.290762 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:04.740563 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4923 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:06.225100 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:07.687951 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8677 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:09.174981 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:09.175845 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:09.176056 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7293    |\n",
      "|      Average Precision       |   0.71953    |\n",
      "|        Average Recall        |   0.72299    |\n",
      "|       Average F1 Score       |   0.72043    |\n",
      "|         Average Loss         |   0.81129    |\n",
      "|       Average Latency        |  23.632 ms   |\n",
      "|   Average GPU Power Usage    |   13.855 W   |\n",
      "| Inference Energy Consumption | 0.090947 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_14/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0315 02:23:14.374530 140494406755392 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0315 02:23:14.374890 140494406755392 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0315 02:23:14.374947 140494406755392 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0315 02:23:16.972962 140494406755392 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0315 02:23:16.979569 140494406755392 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:29<00:00, 26.35it/s, v_num=13, train_acc_step=0.688\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 39.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 42.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 44.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 44.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 45.64it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 42.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 41.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 41.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 42.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 42.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 42.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 42.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 42.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 43.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 43.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 43.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 43.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 43.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 43.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 43.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 43.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 43.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 43.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 44.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 44.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 44.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 44.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 44.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 44.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 44.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 44.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 44.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 44.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 44.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 44.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 44.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:02, 44.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:02, 44.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 44.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 44.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 44.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 44.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 44.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 44.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 44.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 44.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 44.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 44.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 44.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 44.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 44.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 44.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 44.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 44.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 44.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 44.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 44.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 44.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 44.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 43.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 43.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 43.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 43.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 43.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 43.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 43.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 43.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 42.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 42.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 42.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 42.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 42.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 42.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 42.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 42.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 42.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 42.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 41.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 42.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 42.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 42.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 41.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 41.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 41.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 41.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 41.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 41.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 41.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 41.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 41.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 41.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 41.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 41.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 41.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 41.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 41.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 41.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 41.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:01, 41.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 41.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 41.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 41.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 41.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 40.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 40.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 40.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 40.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 40.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 40.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 40.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 40.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:03<00:00, 40.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:03<00:00, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:03<00:00, 40.59it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:31<00:00, 25.16it/s, v_num=13, train_acc_step=0.125\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 41.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 45.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 47.20it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 44.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 45.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 43.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 44.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 44.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 43.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 43.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 44.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 44.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 43.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 43.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 43.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 43.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 43.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 43.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 43.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 43.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 43.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 44.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 43.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 44.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 44.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 44.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 43.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 43.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 42.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 42.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 41.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 41.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 42.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 42.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 42.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 42.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 42.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 42.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 42.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 42.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:02, 41.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:02, 41.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 41.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 41.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 41.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 41.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 41.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 41.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 41.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 41.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 41.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 41.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 41.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 41.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 41.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 41.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 41.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 41.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 40.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 40.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 40.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 40.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 40.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 40.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 40.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:02, 40.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:02, 40.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:02, 40.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:02, 40.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:02, 40.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:02, 40.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:02, 40.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 40.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 40.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 40.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:01, 39.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:01, 39.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:01, 39.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:01, 39.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:01, 39.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:01, 39.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 39.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 39.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 39.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 39.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 39.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 39.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 39.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 39.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 39.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 39.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 39.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 39.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 39.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 39.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 39.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 39.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 38.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 38.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 38.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 38.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 38.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 38.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 38.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:01, 38.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 38.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 38.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:03<00:01, 38.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:03<00:00, 38.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:03<00:00, 38.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 38.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 38.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 38.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 38.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 38.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 38.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 38.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 38.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 38.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 38.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 38.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 38.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 38.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 38.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 38.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 38.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 38.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:04<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:04<00:00, 38.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:04<00:00, 38.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:04<00:00, 38.34it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:35<00:00, 22.01it/s, v_num=13, train_acc_step=0.125\u001b[AI0315 02:24:27.561993 140494406755392 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:35<00:00, 21.88it/s, v_num=13, train_acc_step=0.125\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_17/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_18/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_19/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.68146   |\n",
      "|      Average Precision       |   0.7399    |\n",
      "|        Average Recall        |   0.70312   |\n",
      "|       Average F1 Score       |   0.70984   |\n",
      "|         Average Loss         |    0.895    |\n",
      "|       Average Latency        |  48.939 ms  |\n",
      "|   Average GPU Power Usage    |  8.1492 W   |\n",
      "| Inference Energy Consumption | 0.11078 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_15/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-02:27:22] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73975    |\n",
      "|      Average Precision       |   0.74038    |\n",
      "|        Average Recall        |   0.74242    |\n",
      "|       Average F1 Score       |   0.74044    |\n",
      "|         Average Loss         |   0.73944    |\n",
      "|       Average Latency        |   3.601 ms   |\n",
      "|   Average GPU Power Usage    |   13.729 W   |\n",
      "| Inference Energy Consumption | 0.013733 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "JSC_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_INT8_quant.toml\"\n",
    "JSC_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {JSC_INT8_BY_TYPE_TOML} --load {JSC_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 01:23:45.774668 139703610053696 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922260 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922491 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922657 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922785 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922956 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923104 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923228 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923357 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923478 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923589 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923706 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923818 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923936 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924046 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924172 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924284 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924398 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924507 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924622 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924733 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924848 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924958 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925071 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925192 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925306 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925416 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925532 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925656 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925778 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925884 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926000 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926108 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926219 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926326 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926456 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926565 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926676 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926784 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926894 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.927001 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.927117 139703610053696 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.927211 139703610053696 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0315 01:23:58.927518 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0315 01:23:58.927609 139703610053696 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.927929 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928180 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928388 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928645 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928863 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929070 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8358 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929276 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929511 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1359 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929744 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929947 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930136 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930328 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2610 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930509 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930694 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930871 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931056 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5872 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931236 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931422 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2720 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931605 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931797 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931978 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932162 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6489 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932338 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932523 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932701 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932884 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4016 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933084 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933270 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933448 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933627 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933794 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933966 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934165 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934376 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934560 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934740 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.9013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934911 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935086 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935261 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935403 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935742 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72608   |\n",
      "|      Average Precision       |   0.71486   |\n",
      "|        Average Recall        |   0.71875   |\n",
      "|       Average F1 Score       |   0.71577   |\n",
      "|         Average Loss         |   0.81619   |\n",
      "|       Average Latency        |  34.633 ms  |\n",
      "|   Average GPU Power Usage    |  13.806 W   |\n",
      "| Inference Energy Consumption | 0.13282 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_0/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:06.038734 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.039483 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.039825 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4700 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040095 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040351 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040592 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040854 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041092 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041334 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041567 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041802 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042031 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042269 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042513 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042753 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043049 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043301 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1995 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043533 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043810 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.044165 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.044557 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.044898 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.045269 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.045599 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.045931 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.046232 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.046531 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6513 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.046845 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047185 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1650 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047479 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047739 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047980 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.048222 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.048454 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.048927 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.049339 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.049886 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.050177 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.050515 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.050834 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.051083 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.051247 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72868    |\n",
      "|      Average Precision       |   0.71952    |\n",
      "|        Average Recall        |   0.72268    |\n",
      "|       Average F1 Score       |    0.7203    |\n",
      "|         Average Loss         |   0.80705    |\n",
      "|       Average Latency        |  28.964 ms   |\n",
      "|   Average GPU Power Usage    |   10.53 W    |\n",
      "| Inference Energy Consumption | 0.084719 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_1/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:11.926549 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0570 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.927209 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.927555 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8774 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.927806 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928054 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4204 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928285 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928526 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928769 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929061 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929307 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929539 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929760 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930126 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930378 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930610 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930837 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.931249 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.931474 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932123 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932453 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932724 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932990 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933232 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933460 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933697 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933957 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934198 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934422 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934650 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7914 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934872 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935144 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935390 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935625 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1205 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935845 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936084 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936304 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936535 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7121 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936811 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937063 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937299 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937504 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937657 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72872    |\n",
      "|      Average Precision       |   0.71967    |\n",
      "|        Average Recall        |   0.72279    |\n",
      "|       Average F1 Score       |   0.72034    |\n",
      "|         Average Loss         |   0.80904    |\n",
      "|       Average Latency        |  26.359 ms   |\n",
      "|   Average GPU Power Usage    |   10.624 W   |\n",
      "| Inference Energy Consumption | 0.077789 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_2/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:17.712724 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:17.894579 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.251268 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.431818 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.694564 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1439 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.881787 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.219688 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.399970 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.650830 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9089 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.819225 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.001526 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.172828 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.438764 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.627349 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.827448 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.013975 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.211982 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.6229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.391825 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.612207 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0186 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.784920 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.150280 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.329423 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.599805 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.787929 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.105024 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.295989 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.579686 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.766986 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.944687 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7037 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.128643 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.316481 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.499539 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.684113 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.054862 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.232592 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.412049 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.592864 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.771907 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.950548 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:26.129124 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:26.130245 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:26.130683 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72848   |\n",
      "|      Average Precision       |   0.71898   |\n",
      "|        Average Recall        |   0.72227   |\n",
      "|       Average F1 Score       |   0.7197    |\n",
      "|         Average Loss         |   0.80978   |\n",
      "|       Average Latency        |  33.931 ms  |\n",
      "|   Average GPU Power Usage    |  9.4373 W   |\n",
      "| Inference Energy Consumption | 0.08895 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:35.326884 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:36.899141 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:39.525205 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4599 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:41.033430 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:42.986601 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.8759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:44.495841 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:47.389332 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1156 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:48.954459 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:50.742857 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5661 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:52.182403 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:53.631896 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:55.090106 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:57.066362 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:58.536308 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:00.002814 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:01.434386 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:02.874977 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:04.346352 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:05.848518 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.6632 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:07.320424 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:09.730585 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:11.231554 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:13.152189 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.0554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:14.590133 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:17.024239 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:18.439450 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:20.490301 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:21.951633 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:23.403716 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:24.795568 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:26.234439 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:27.662348 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:29.139847 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4003 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:30.580707 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:32.044255 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:33.526308 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:34.972633 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4923 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:36.432395 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:37.897637 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8677 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:39.397825 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:39.398750 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:39.399023 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7293    |\n",
      "|      Average Precision       |   0.71953    |\n",
      "|        Average Recall        |   0.72299    |\n",
      "|       Average F1 Score       |   0.72043    |\n",
      "|         Average Loss         |   0.81129    |\n",
      "|       Average Latency        |  26.884 ms   |\n",
      "|   Average GPU Power Usage    |   10.213 W   |\n",
      "| Inference Energy Consumption | 0.076263 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_4/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0315 01:25:45.103971 139703610053696 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0315 01:25:45.104289 139703610053696 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0315 01:25:45.104348 139703610053696 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0315 01:25:47.751724 139703610053696 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0315 01:25:47.763391 139703610053696 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:31<00:00, 24.52it/s, v_num=12, train_acc_step=0.688\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:04, 31.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:04, 32.68it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 34.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 34.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 32.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 33.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:04, 33.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:04, 32.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:04, 32.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:04, 32.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:04, 32.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 31.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:04, 31.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:04, 31.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:04, 31.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:04, 31.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:04, 31.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:04, 31.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:04, 30.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:04, 30.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:04, 30.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:04, 31.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:04, 30.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:04, 30.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:04, 30.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:04, 30.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:04, 29.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:04, 29.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:04, 29.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:01<00:04, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:01<00:04, 29.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:01<00:04, 29.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:01<00:04, 29.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:01<00:04, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:04, 29.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:04, 29.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:04, 28.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:04, 28.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:01<00:04, 28.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:01<00:04, 28.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:04, 28.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:04, 28.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:03, 28.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:03, 28.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:03, 28.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:03, 28.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:03, 28.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:03, 28.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:03, 28.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:03, 28.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:03, 28.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:03, 28.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:03, 29.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:03, 29.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:03, 29.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:03, 29.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:03, 29.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:03, 29.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:02<00:03, 29.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:02<00:03, 29.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:02<00:03, 29.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:02<00:03, 29.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:02<00:03, 29.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:02<00:03, 28.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:02<00:03, 28.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:02<00:03, 28.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:02<00:03, 28.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:02<00:03, 28.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:02<00:03, 28.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:02<00:02, 29.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:02<00:02, 29.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:02<00:02, 29.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:02<00:02, 29.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:02<00:02, 29.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:02<00:02, 29.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:02<00:02, 29.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:02<00:02, 29.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:02<00:02, 29.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:02<00:02, 29.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:02, 29.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:02, 29.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:02, 29.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:02, 29.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:02, 29.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:02, 29.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:02, 29.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:03<00:02, 29.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:03<00:02, 29.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:03<00:02, 29.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:03<00:02, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:03<00:02, 29.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:03<00:02, 29.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:03<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:03<00:02, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:03<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:03<00:02, 29.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:03<00:01, 29.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:03<00:01, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:03<00:01, 29.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:03<00:01, 29.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:03<00:01, 29.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:03<00:01, 29.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:03<00:01, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:03<00:01, 29.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:03<00:01, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:03<00:01, 29.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:03<00:01, 29.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:03<00:01, 29.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:03<00:01, 29.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:03<00:01, 29.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:03<00:01, 29.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:03<00:01, 29.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:03<00:01, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 29.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 29.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:04<00:01, 29.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:04<00:01, 29.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:04<00:01, 29.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:04<00:01, 29.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:04<00:01, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:04<00:01, 29.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:04<00:01, 29.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:04<00:01, 29.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:04<00:01, 29.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:04<00:01, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:04<00:00, 29.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:04<00:00, 29.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:04<00:00, 29.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:04<00:00, 29.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:04<00:00, 29.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:04<00:00, 29.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:04<00:00, 29.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:04<00:00, 29.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:04<00:00, 29.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:04<00:00, 29.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:04<00:00, 29.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:04<00:00, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:04<00:00, 29.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:04<00:00, 29.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:04<00:00, 29.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:04<00:00, 29.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:05<00:00, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:05<00:00, 29.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:05<00:00, 29.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:05<00:00, 29.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:05<00:00, 29.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:05<00:00, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:05<00:00, 29.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:05<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:05<00:00, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:05<00:00, 29.63it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:37<00:00, 20.61it/s, v_num=12, train_acc_step=0.125\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:04, 34.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:05, 27.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 28.20it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 31.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 32.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 32.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:04, 33.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:04, 34.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:04, 34.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:04, 33.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:04, 33.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 33.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:04, 33.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:04, 33.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:04, 32.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:04, 32.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:04, 32.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:04, 32.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:04, 33.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:04, 33.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:04, 33.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:04, 33.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:04, 33.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 33.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 33.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 33.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 33.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 33.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 33.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 33.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 33.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 33.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:03, 34.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:03, 34.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:03, 34.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:03, 33.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:03, 33.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:03, 34.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:01<00:03, 34.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:01<00:03, 33.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:03, 34.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:03, 34.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:03, 34.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:03, 34.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:03, 34.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:03, 34.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:03, 34.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:03, 34.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:03, 33.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:03, 33.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:03, 33.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:03, 33.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:03, 33.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:03, 33.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:03, 33.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:03, 33.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 33.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 33.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 33.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 32.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 32.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 32.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 32.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 32.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:02<00:02, 32.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:02<00:02, 32.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:02<00:02, 32.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:02<00:02, 32.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:02<00:02, 32.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:02<00:02, 32.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:02<00:02, 32.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:02<00:02, 31.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:02<00:02, 31.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:02<00:02, 31.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:02<00:02, 31.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:02<00:02, 31.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:02<00:02, 31.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:02<00:02, 31.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:02<00:02, 31.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:02, 31.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:02, 31.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:02, 31.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:02, 31.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:02, 31.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:02, 31.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:02, 31.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:02, 31.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:02, 31.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:02, 31.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:02, 31.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:02, 31.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:02, 30.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:03<00:02, 30.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:03<00:02, 31.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:03<00:02, 30.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:03<00:01, 30.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:03<00:01, 30.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:03<00:01, 30.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:03<00:01, 30.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:03<00:01, 30.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:03<00:01, 30.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:03<00:01, 30.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:03<00:01, 30.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:03<00:01, 30.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:03<00:01, 30.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:03<00:01, 30.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:03<00:01, 30.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:03<00:01, 30.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:03<00:01, 30.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:03<00:01, 30.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:03<00:01, 30.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:03<00:01, 30.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:03<00:01, 30.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:03<00:01, 29.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:03<00:01, 29.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 29.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 29.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:03<00:01, 29.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:03<00:01, 30.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:04<00:01, 29.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:04<00:01, 29.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:04<00:01, 29.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:04<00:01, 29.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:04<00:01, 29.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:04<00:01, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:04<00:01, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:04<00:01, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:04<00:00, 29.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:04<00:00, 29.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:04<00:00, 29.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:04<00:00, 29.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:04<00:00, 29.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:04<00:00, 29.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:04<00:00, 29.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:04<00:00, 29.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:04<00:00, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:04<00:00, 29.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:04<00:00, 29.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:04<00:00, 29.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:04<00:00, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:04<00:00, 29.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:04<00:00, 29.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:04<00:00, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:05<00:00, 29.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:05<00:00, 29.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:05<00:00, 29.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:05<00:00, 29.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:05<00:00, 29.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:05<00:00, 29.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:05<00:00, 29.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:05<00:00, 29.89it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:43<00:00, 17.95it/s, v_num=12, train_acc_step=0.125\u001b[AI0315 01:27:10.410279 139703610053696 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:43<00:00, 17.84it/s, v_num=12, train_acc_step=0.125\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_1/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_2/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73031   |\n",
      "|      Average Precision       |   0.72099   |\n",
      "|        Average Recall        |   0.72423   |\n",
      "|       Average F1 Score       |   0.7218    |\n",
      "|         Average Loss         |   0.81107   |\n",
      "|       Average Latency        |  42.757 ms  |\n",
      "|   Average GPU Power Usage    |  9.3992 W   |\n",
      "| Inference Energy Consumption | 0.11163 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-01:30:18] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.74089    |\n",
      "|      Average Precision       |   0.74216    |\n",
      "|        Average Recall        |   0.74431    |\n",
      "|       Average F1 Score       |   0.74236    |\n",
      "|         Average Loss         |    0.7364    |\n",
      "|       Average Latency        |  4.4764 ms   |\n",
      "|   Average GPU Power Usage    |   12.884 W   |\n",
      "| Inference Energy Consumption | 0.016021 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_0/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "JSC_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_INT8_quant.toml\"\n",
    "JSC_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {JSC_INT8_BY_TYPE_TOML} --load {JSC_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 01:36:29.907291 139755539158080 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mprecision=fp16, skipping int8 calibration/fine-tune...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_7/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_8/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_9/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72852    |\n",
      "|      Average Precision       |   0.71886    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |   0.71961    |\n",
      "|         Average Loss         |   0.80975    |\n",
      "|       Average Latency        |  8.0633 ms   |\n",
      "|   Average GPU Power Usage    |   13.007 W   |\n",
      "| Inference Energy Consumption | 0.029134 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_7/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-01:36:52] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+---------------+\n",
      "|      Metric (Per Batch)      |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.74178    |\n",
      "|      Average Precision       |    0.7423     |\n",
      "|        Average Recall        |    0.74514    |\n",
      "|       Average F1 Score       |    0.74268    |\n",
      "|         Average Loss         |    0.73414    |\n",
      "|       Average Latency        |   1.0096 ms   |\n",
      "|   Average GPU Power Usage    |   9.6732 W    |\n",
      "| Inference Energy Consumption | 0.0027127 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_2/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "JSC_FP16_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_FP16_quant.toml\"\n",
    "JSC_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {JSC_FP16_BY_TYPE_TOML} --load {JSC_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 01:43:15.535419 140335826768960 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp32'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mprecision=fp32, skipping int8 calibration/fine-tune...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_13/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_14/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_15/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72852    |\n",
      "|      Average Precision       |   0.71886    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |   0.71961    |\n",
      "|         Average Loss         |   0.80975    |\n",
      "|       Average Latency        |  8.3068 ms   |\n",
      "|   Average GPU Power Usage    |   17.661 W   |\n",
      "| Inference Energy Consumption | 0.040752 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_9/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-01:43:32] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+---------------+\n",
      "|      Metric (Per Batch)      |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.74162    |\n",
      "|      Average Precision       |    0.74235    |\n",
      "|        Average Recall        |    0.74514    |\n",
      "|       Average F1 Score       |    0.7427     |\n",
      "|         Average Loss         |    0.73414    |\n",
      "|       Average Latency        |   2.706 ms    |\n",
      "|   Average GPU Power Usage    |   11.678 W    |\n",
      "| Inference Energy Consumption | 0.0087779 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_4/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "JSC_FP32_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_FP32_quant.toml\"\n",
    "JSC_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {JSC_FP32_BY_TYPE_TOML} --load {JSC_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `fp16` acheives a slighty higher test accuracy but a slightly lower latency (~30%) from that of int8 quantization; it is still ~2.5x faster than the unquantized model. Now lets apply quantization to a more complicated model.\n",
    "\n",
    "## Section 3. Type-wise Mixed Precision on Larger Model\n",
    "We will now quantize `vgg7` which includes both convolutional and linear layers, however for this demonstration we want to quantize all layer types except the linear layers.\n",
    "\n",
    "In this case, we set:\n",
    "\n",
    "- The `by` parameter to `type`\n",
    "- The `quantize` parameter to true for `passes.tensorrt.conv2d.config` and `precision` parameter to 'int8'.\n",
    "- The `input` and `weight` quantize axis for the conv2d layers.\n",
    "- The default `passes.tensorrt.default.config` precision to true. \n",
    "\n",
    "During the TensorRT quantization, the model's conv2d layers will be converted to an int8 fake quantized form, whilst the linear layers are kept to their default 'fp16'. Calibration of the conv2d layers and then fine tuning will be undergone before quantization and inference.\n",
    "\n",
    "You may either download a pretrained model [here](https://imperiallondon-my.sharepoint.com/:f:/g/personal/zz7522_ic_ac_uk/Emh3VT7Q_qRFmnp8kDrcgDoBwGUuzLwwKNtX8ZAt368jJQ?e=gsKONa), otherwise train it yourself as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0306 15:06:31.029283 140282827188032 seed.py:54] Seed set to 0\n",
      "+-------------------------+----------------------+--------------+-----------------+----------------------+\n",
      "| Name                    |       Default        | Config. File | Manual Override |      Effective       |\n",
      "+-------------------------+----------------------+--------------+-----------------+----------------------+\n",
      "| task                    |    \u001b[38;5;8mclassification\u001b[0m    |     cls      |                 |         cls          |\n",
      "| load_name               |         None         |              |                 |         None         |\n",
      "| load_type               |          mz          |              |                 |          mz          |\n",
      "| batch_size              |         \u001b[38;5;8m128\u001b[0m          |      64      |                 |          64          |\n",
      "| to_debug                |        False         |              |                 |        False         |\n",
      "| log_level               |         info         |              |                 |         info         |\n",
      "| report_to               |     tensorboard      |              |                 |     tensorboard      |\n",
      "| seed                    |          0           |              |                 |          0           |\n",
      "| quant_config            |         None         |              |                 |         None         |\n",
      "| training_optimizer      |         adam         |              |                 |         adam         |\n",
      "| trainer_precision       |       16-mixed       |              |                 |       16-mixed       |\n",
      "| learning_rate           |        \u001b[38;5;8m1e-05\u001b[0m         |    0.001     |                 |        0.001         |\n",
      "| weight_decay            |          0           |              |                 |          0           |\n",
      "| max_epochs              |          \u001b[38;5;8m20\u001b[0m          |      10      |                 |          10          |\n",
      "| max_steps               |          -1          |              |                 |          -1          |\n",
      "| accumulate_grad_batches |          1           |              |                 |          1           |\n",
      "| log_every_n_steps       |          50          |              |                 |          50          |\n",
      "| num_workers             |          20          |              |                 |          20          |\n",
      "| num_devices             |          1           |              |                 |          1           |\n",
      "| num_nodes               |          1           |              |                 |          1           |\n",
      "| accelerator             |         \u001b[38;5;8mauto\u001b[0m         |     gpu      |                 |         gpu          |\n",
      "| strategy                |         auto         |              |                 |         auto         |\n",
      "| is_to_auto_requeue      |        False         |              |                 |        False         |\n",
      "| github_ci               |        False         |              |                 |        False         |\n",
      "| disable_dataset_cache   |        False         |              |                 |        False         |\n",
      "| target                  | xcu250-figd2104-2L-e |              |                 | xcu250-figd2104-2L-e |\n",
      "| num_targets             |         100          |              |                 |         100          |\n",
      "| is_pretrained           |        False         |              |                 |        False         |\n",
      "| max_token_len           |         512          |              |                 |         512          |\n",
      "| project_dir             |  /root/local/mase-   |              |                 |  /root/local/mase-   |\n",
      "|                         |    gp/mase_output    |              |                 |    gp/mase_output    |\n",
      "| project                 |         None         |              |                 |         None         |\n",
      "| model                   |         \u001b[38;5;8mNone\u001b[0m         |  vgg7_cifar  |                 |      vgg7_cifar      |\n",
      "| dataset                 |         \u001b[38;5;8mNone\u001b[0m         |   cifar10    |                 |       cifar10        |\n",
      "| t_max                   |          20          |              |                 |          20          |\n",
      "| eta_min                 |        1e-06         |              |                 |        1e-06         |\n",
      "+-------------------------+----------------------+--------------+-----------------+----------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7_cifar'...\u001b[0m\n",
      "self.args.model is vgg7_cifar\n",
      "model_info is MaseModelInfo(name='vgg7_cifar', model_source=<ModelSource.VISION_OTHERS: 'vision_others'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining model 'vgg7_cifar'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m##### WEIGHT DECAY ##### 0\u001b[0m\n",
      "I0306 15:06:32.428992 140282827188032 rank_zero.py:53] Using 16bit Automatic Mixed Precision (AMP)\n",
      "I0306 15:06:32.450018 140282827188032 rank_zero.py:53] GPU available: True (cuda), used: True\n",
      "I0306 15:06:32.556042 140282827188032 rank_zero.py:53] TPU available: False, using: 0 TPU cores\n",
      "I0306 15:06:32.556262 140282827188032 rank_zero.py:53] IPU available: False, using: 0 IPUs\n",
      "I0306 15:06:32.556331 140282827188032 rank_zero.py:53] HPU available: False, using: 0 HPUs\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "I0306 15:07:33.638568 140282827188032 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0306 15:07:33.643197 140282827188032 model_summary.py:90] \n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | VGG7               | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | loss_val  | MeanMetric         | 0     \n",
      "4 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 782/782 [00:24<00:00, 31.63it/s, v_num=1, train_acc_step=0.250]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 70.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 75.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 52.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 60.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 63.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 42.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 48.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 52.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.73it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 57.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 52.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 54.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 53.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 54.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 56.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 57.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 59.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 60.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 61.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 62.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 64.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 65.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 66.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 67.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 67.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 68.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 69.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 70.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 70.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 71.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 72.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 72.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 74.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 74.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 75.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 75.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 76.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 76.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 76.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 77.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 77.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 77.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 77.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 76.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 76.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 76.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 76.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 76.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 76.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 76.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 76.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 76.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 76.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 76.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 76.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 76.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 76.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 76.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 76.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 76.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 76.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 76.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 76.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 76.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 76.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 76.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 76.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 76.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 76.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 76.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 76.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 76.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 76.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 76.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 76.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 76.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 76.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 76.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 76.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 76.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 76.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 76.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 76.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 76.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 76.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 76.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 76.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 76.71it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:24<00:00, 31.30it/s, v_num=1, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:01, 89.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:01, 90.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 48.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 54.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 58.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 58.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 61.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 64.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 66.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 69.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 71.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 71.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 69.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 70.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:01, 72.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 73.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 74.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 75.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 76.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 77.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 76.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 77.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 78.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 76.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 78.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 79.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 80.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 80.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 81.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 81.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 81.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 82.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 82.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 83.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 83.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 83.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 82.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 83.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 82.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 82.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 82.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 82.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 82.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 82.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 82.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 81.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 81.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 81.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 81.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 81.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 81.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 81.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 81.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 81.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 81.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 81.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 81.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 81.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 81.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 80.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 80.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 80.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 80.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 80.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 80.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 80.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 80.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 80.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 80.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 80.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 80.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 80.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 79.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 79.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 79.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 79.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 79.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 79.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 79.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 79.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 79.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 79.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 79.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 79.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 79.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 79.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 79.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 79.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 79.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 79.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 79.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 79.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 79.41it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 79.47it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 782/782 [00:24<00:00, 31.84it/s, v_num=1, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 73.55it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 62.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 27.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 33.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 38.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 42.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 44.75it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 48.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 51.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 56.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 59.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 61.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 62.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 64.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 65.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 67.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 68.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 69.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 71.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 72.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 73.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 69.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 70.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 71.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 72.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 73.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 75.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 75.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 76.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 77.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 78.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 79.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 79.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 80.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 81.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 81.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 81.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 81.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 81.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 81.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 81.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 81.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 81.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 81.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 81.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 80.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 81.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 80.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 80.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 80.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 80.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 80.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 80.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 80.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 80.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 80.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 80.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 80.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 80.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 80.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 80.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 80.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 80.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 80.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 80.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 79.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 79.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 79.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 79.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 79.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 79.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 79.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 79.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 79.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 79.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 79.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 79.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 79.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 79.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 79.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 79.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 79.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 79.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 79.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 79.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 78.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 79.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 78.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 79.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 79.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 79.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 78.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 78.98it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 782/782 [00:24<00:00, 31.84it/s, v_num=1, train_acc_step=0.625,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 41.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 52.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 59.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 64.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 68.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 71.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:01, 74.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:01, 76.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:01, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:01, 80.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:01, 82.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:01, 83.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:01, 84.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:01, 85.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:01, 86.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 86.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 87.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 78.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 79.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 80.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 81.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 82.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 82.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 82.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 83.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 83.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 83.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 84.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 84.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 83.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 83.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 84.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 84.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 84.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 84.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 84.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 84.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 84.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 84.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 84.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 83.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 83.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 83.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 83.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 83.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 83.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 83.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 83.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 83.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 83.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 83.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 82.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 82.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 82.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 82.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 82.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 82.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 82.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 82.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 82.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 82.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 82.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 81.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 81.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 81.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 81.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 81.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 81.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 81.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 81.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 81.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 81.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:00, 81.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 81.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 81.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 81.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 81.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:00<00:00, 81.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 81.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 80.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 80.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 80.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 80.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 80.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 80.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 80.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 80.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 80.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 80.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 80.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 80.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 80.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 80.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 80.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 80.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 80.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 80.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 80.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 80.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 80.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 80.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 80.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 81.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 81.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 81.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 81.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 81.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 81.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 81.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 80.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 80.99it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 782/782 [00:24<00:00, 31.58it/s, v_num=1, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 76.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 76.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:05, 30.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 34.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 42.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 49.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 51.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 54.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 56.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.11it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 59.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 61.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 58.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 60.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 61.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 62.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 64.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 65.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 66.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 67.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 66.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 67.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 68.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 69.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 70.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 70.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 71.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 71.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 72.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 72.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 73.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 73.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 74.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 74.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 75.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 75.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 75.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 75.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 74.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 75.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 74.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 74.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 74.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 74.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 75.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 75.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 75.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 75.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 75.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 75.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 74.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 75.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 74.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 74.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 74.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 74.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 74.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 74.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 74.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 74.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 74.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 74.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 74.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 73.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 73.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 74.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 74.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 74.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 74.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 74.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 74.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 74.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 74.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 74.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 74.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 74.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 74.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 74.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 74.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 74.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 74.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 74.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 74.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 74.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 74.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 74.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 74.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 74.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 74.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 74.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 74.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 74.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 74.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 74.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 74.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 74.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 74.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 74.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 74.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 74.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 74.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 74.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 74.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 74.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 74.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 74.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 74.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 74.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 74.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 74.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 74.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 74.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 75.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 75.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 75.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 75.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 75.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 75.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 75.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 75.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 75.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 75.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 75.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 75.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 75.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 75.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 75.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 75.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 75.77it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 75.85it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 782/782 [00:24<00:00, 31.28it/s, v_num=1, train_acc_step=0.625,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:05, 27.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 32.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 39.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 49.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 51.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 57.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 59.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 60.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 62.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 63.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 65.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 66.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 67.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 68.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 70.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 71.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 72.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 73.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 73.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 73.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 73.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 74.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 75.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 76.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 77.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 78.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 79.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 79.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 80.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 80.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 80.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 80.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 80.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 80.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 80.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 79.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 78.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 78.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 78.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 78.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 78.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 78.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 78.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 78.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 78.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:01, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:01, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:00, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 78.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 78.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 78.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 78.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 78.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 78.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 78.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 78.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 78.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 78.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 78.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 78.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 78.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 78.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 78.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 78.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 78.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 78.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 78.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 79.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 79.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 79.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 79.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 79.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 79.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 79.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 79.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 79.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 79.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 79.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 79.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 79.33it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 782/782 [00:24<00:00, 31.31it/s, v_num=1, train_acc_step=0.688,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 71.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 47.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 58.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 65.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 70.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 74.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:01, 77.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:01, 80.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:01, 82.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 60.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 56.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 60.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 62.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 64.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 65.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 67.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 68.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 69.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 70.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 71.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 71.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 72.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 73.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 75.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 75.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 76.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 77.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 77.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 78.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 79.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 80.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 80.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 80.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 80.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 80.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 80.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 81.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 81.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 81.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 80.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 80.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 80.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 80.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 80.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 80.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 80.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 80.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 80.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 80.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 80.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 80.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 80.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 80.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 80.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 79.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 79.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 79.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 79.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 79.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 79.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 79.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 80.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 80.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 80.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 80.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 80.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 80.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 79.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 79.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 79.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 79.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 79.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 79.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 79.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 79.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 79.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 79.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 79.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 79.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 79.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 80.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 80.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 80.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 80.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 80.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 80.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 80.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 80.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 80.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 80.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 80.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 80.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 80.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 80.36it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 80.40it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 782/782 [00:25<00:00, 31.23it/s, v_num=1, train_acc_step=0.438,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 75.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:01, 82.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:01, 82.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 44.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 48.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 51.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 54.43it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 57.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 59.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 62.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 63.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 65.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 66.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 68.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 69.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 70.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 71.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 72.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 73.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 74.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 75.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 76.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 72.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 73.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 73.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 74.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 75.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 75.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 76.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 78.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 78.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 79.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 78.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 78.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 78.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 78.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 78.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 78.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 78.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 78.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 78.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 78.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 78.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 78.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 78.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 78.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 78.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 78.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 78.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 78.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 78.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 77.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 77.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 77.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:01, 77.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:01, 78.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:00, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 78.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 78.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 78.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 78.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 77.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 77.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 77.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 77.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 78.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 78.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 77.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 78.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 78.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 78.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 78.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 78.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 78.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 78.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 78.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 78.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 78.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 78.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 78.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 78.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 78.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 78.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 78.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 78.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 78.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 78.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 78.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 78.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 78.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 78.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 79.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 79.03it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 782/782 [00:25<00:00, 31.22it/s, v_num=1, train_acc_step=0.750,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 70.43it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:01, 79.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:01, 81.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 68.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 73.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 66.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 64.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 53.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 56.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 56.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 58.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 60.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 62.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 64.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 66.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 67.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 68.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 69.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 70.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 71.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 72.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 73.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 73.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 74.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 74.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 75.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 75.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 72.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 72.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 72.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 73.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 74.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 74.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 75.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 76.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 76.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 77.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 78.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 78.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 78.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 78.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 78.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 78.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 78.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 78.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 78.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 78.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 78.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 78.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 78.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 78.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 78.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 78.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 78.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 78.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 77.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 77.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 77.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:01, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:00, 77.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 77.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 77.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 77.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 77.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 77.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 77.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 77.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 77.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 77.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 77.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 77.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 77.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 77.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 77.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 77.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 77.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 77.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 77.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 77.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 77.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 77.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 77.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 77.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 77.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 77.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 77.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 77.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 77.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 77.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 77.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 77.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 77.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 77.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 77.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 77.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 77.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 77.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 77.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 77.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 77.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 77.48it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 782/782 [00:25<00:00, 31.23it/s, v_num=1, train_acc_step=0.625,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.31it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:06, 25.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 31.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 31.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 36.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 42.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 48.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 51.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 53.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 55.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 58.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 60.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 61.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 63.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 64.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 65.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 67.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 67.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 69.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 69.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 66.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 67.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 67.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 68.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 68.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 69.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 70.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 70.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 71.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 72.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 73.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 75.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 75.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 76.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 76.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 76.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 76.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 76.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 76.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 76.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 76.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 76.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 76.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 76.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 76.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 76.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 76.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 76.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 76.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 76.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 76.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 76.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 76.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 76.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 76.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 76.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 76.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 76.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 75.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 75.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 75.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 75.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 75.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 75.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 75.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 75.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 75.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 75.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 75.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 75.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 75.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 75.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 75.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 75.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 75.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 75.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 76.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 75.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 75.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 76.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 76.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 76.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 76.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 76.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 76.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 76.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 76.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 76.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 76.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 76.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 76.44it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 76.48it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 782/782 [00:27<00:00, 28.21it/s, v_num=1, train_acc_step=0.625,\u001b[AI0306 15:13:40.852709 140282827188032 rank_zero.py:53] `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Epoch 9: 100%|█| 782/782 [00:35<00:00, 21.99it/s, v_num=1, train_acc_step=0.625,\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "VGG_TYPEWISE_TOML = \"vgg7_typewise_mixed_precision.toml\"\n",
    "\n",
    "!python /root/local/mase-gp/src/ch train --config {VGG_TYPEWISE_TOML}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the checkpoint in, quantize the model and compare it to the unquantized version as we did in [Section 1.5](#section-15-performance-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_TYPEWISE_TOML = \"vgg7_typewise_mixed_precision.toml\"\n",
    "# Change this checkpoint path accordingly\n",
    "VGG_CHECKPOINT_PATH = \"/root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06/software/training_ckpts/best.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0306 15:58:00.578649 140271219853120 seed.py:54] Seed set to 0\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |       Default        | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |    \u001b[38;5;8mclassification\u001b[0m    |     cls      |                          |           cls            |\n",
      "| load_name               |         \u001b[38;5;8mNone\u001b[0m         |              | /root/local/mase-gp/mase | /root/local/mase-gp/mase |\n",
      "|                         |                      |              | _output/vgg7_cifar_cls_c | _output/vgg7_cifar_cls_c |\n",
      "|                         |                      |              | ifar10_2025-03-06/softwa | ifar10_2025-03-06/softwa |\n",
      "|                         |                      |              | re/training_ckpts/best.c | re/training_ckpts/best.c |\n",
      "|                         |                      |              |           kpt            |           kpt            |\n",
      "| load_type               |          \u001b[38;5;8mmz\u001b[0m          |              |            pl            |            pl            |\n",
      "| batch_size              |         \u001b[38;5;8m128\u001b[0m          |      64      |                          |            64            |\n",
      "| to_debug                |        False         |              |                          |          False           |\n",
      "| log_level               |         info         |              |                          |           info           |\n",
      "| report_to               |     tensorboard      |              |                          |       tensorboard        |\n",
      "| seed                    |          0           |              |                          |            0             |\n",
      "| quant_config            |         None         |              |                          |           None           |\n",
      "| training_optimizer      |         adam         |              |                          |           adam           |\n",
      "| trainer_precision       |       16-mixed       |              |                          |         16-mixed         |\n",
      "| learning_rate           |        \u001b[38;5;8m1e-05\u001b[0m         |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |          0           |              |                          |            0             |\n",
      "| max_epochs              |          \u001b[38;5;8m20\u001b[0m          |      10      |                          |            10            |\n",
      "| max_steps               |          -1          |              |                          |            -1            |\n",
      "| accumulate_grad_batches |          1           |              |                          |            1             |\n",
      "| log_every_n_steps       |          50          |              |                          |            50            |\n",
      "| num_workers             |          20          |              |                          |            20            |\n",
      "| num_devices             |          1           |              |                          |            1             |\n",
      "| num_nodes               |          1           |              |                          |            1             |\n",
      "| accelerator             |         \u001b[38;5;8mauto\u001b[0m         |     gpu      |                          |           gpu            |\n",
      "| strategy                |         auto         |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |        False         |              |                          |          False           |\n",
      "| github_ci               |        False         |              |                          |          False           |\n",
      "| disable_dataset_cache   |        False         |              |                          |          False           |\n",
      "| target                  | xcu250-figd2104-2L-e |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |         100          |              |                          |           100            |\n",
      "| is_pretrained           |        False         |              |                          |          False           |\n",
      "| max_token_len           |         512          |              |                          |           512            |\n",
      "| project_dir             |  /root/local/mase-   |              |                          |    /root/local/mase-     |\n",
      "|                         |    gp/mase_output    |              |                          |      gp/mase_output      |\n",
      "| project                 |         None         |              |                          |           None           |\n",
      "| model                   |         \u001b[38;5;8mNone\u001b[0m         |  vgg7_cifar  |                          |        vgg7_cifar        |\n",
      "| dataset                 |         \u001b[38;5;8mNone\u001b[0m         |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |          20          |              |                          |            20            |\n",
      "| eta_min                 |        1e-06         |              |                          |          1e-06           |\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7_cifar'...\u001b[0m\n",
      "self.args.model is vgg7_cifar\n",
      "model_info is MaseModelInfo(name='vgg7_cifar', model_source=<ModelSource.VISION_OTHERS: 'vision_others'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'vgg7_cifar'...\u001b[0m\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'vgg7_cifar', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile'], 'percentiles': [99.0, 99.9], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'linear': {'config': {'quantize': False}}, 'conv2d': {'config': {'quantize': True, 'precision': 'int8', 'calibrator': 'histogram'}, 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06/software/training_ckpts/best.ckpt\u001b[0m\n",
      "tensor([[[[3.4464e-01, 6.9910e-01, 6.0161e-01, 5.1311e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [6.6311e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.8461e-01, 4.2010e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.7113e-01, 0.0000e+00],\n",
      "          [6.4409e-01, 8.5504e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [7.9605e-01, 2.5626e+00, 3.4395e+00, 3.1523e+00]],\n",
      "\n",
      "         [[8.0510e-01, 8.0670e-01, 1.3297e-01, 0.0000e+00],\n",
      "          [2.5205e-01, 5.3269e-01, 9.2034e-01, 9.8315e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 1.8874e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.9280e-01, 6.0597e-01, 1.4563e-01, 0.0000e+00],\n",
      "          [2.8448e-01, 7.9045e-01, 6.4207e-01, 1.2590e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1534e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 6.2839e-01, 8.3748e-01],\n",
      "          [0.0000e+00, 6.9968e-01, 2.5770e-01, 1.0547e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.6658e-01, 1.2895e-01, 2.4467e-01, 5.4618e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[3.4050e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.4691e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [4.9779e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.0582e-01, 4.2151e-01, 4.6160e-01, 3.9830e-01]],\n",
      "\n",
      "         [[0.0000e+00, 4.3441e-01, 1.1964e+00, 7.0995e-01],\n",
      "          [5.5145e-01, 2.4369e+00, 7.4727e-01, 0.0000e+00],\n",
      "          [1.4729e-01, 2.4310e+00, 1.3350e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2677e-01]],\n",
      "\n",
      "         [[4.6489e-02, 3.8742e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.3351e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.6955e-01, 6.7727e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.9228e-02, 2.2171e-01, 5.2112e-01, 8.8665e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4068e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 6.6314e-03, 1.0431e+00]],\n",
      "\n",
      "         [[2.7404e-01, 0.0000e+00, 0.0000e+00, 5.1160e-01],\n",
      "          [3.1893e-02, 0.0000e+00, 0.0000e+00, 1.7702e-01],\n",
      "          [9.6126e-01, 1.0711e+00, 0.0000e+00, 6.0377e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[7.8424e-01, 4.9169e-01, 0.0000e+00, 7.4105e-01],\n",
      "          [1.2236e-02, 0.0000e+00, 0.0000e+00, 5.8802e-01],\n",
      "          [1.5900e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.5348e-01, 0.0000e+00, 0.0000e+00, 5.2442e-01]]],\n",
      "\n",
      "\n",
      "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0309e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.2175e-01, 1.8725e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.1506e-01, 1.3181e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.5812e-02, 0.0000e+00, 3.7893e-02, 8.1935e-01],\n",
      "          [0.0000e+00, 1.1055e-02, 1.2085e+00, 2.1328e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 2.5364e-02, 1.1291e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 1.8021e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5488e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8639e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.4830e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.7112e-02, 0.0000e+00, 0.0000e+00, 4.6543e-02]],\n",
      "\n",
      "         [[4.4452e-01, 2.5066e-01, 4.8193e-01, 1.4843e-01],\n",
      "          [0.0000e+00, 7.2661e-01, 1.4449e+00, 1.2516e+00],\n",
      "          [0.0000e+00, 1.3113e-01, 2.1405e+00, 1.9119e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0507e-01]],\n",
      "\n",
      "         [[7.8332e-01, 5.0053e-02, 4.9883e-01, 4.6011e-01],\n",
      "          [8.3906e-01, 0.0000e+00, 0.0000e+00, 9.1762e-02],\n",
      "          [9.4349e-01, 3.7160e-01, 4.7023e-01, 5.2874e-01],\n",
      "          [7.8920e-01, 6.4321e-01, 5.4258e-01, 7.3443e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[2.0336e-01, 2.7439e-01, 2.9018e-01, 4.7235e-01],\n",
      "          [6.3775e-02, 7.5851e-02, 0.0000e+00, 2.6987e-01],\n",
      "          [7.0484e-04, 3.0807e-01, 2.1616e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.9833e-01, 3.4737e-01]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 1.0248e+00, 4.3724e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 8.7463e-01, 6.0394e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.8190e-02, 8.3974e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1064e-01]],\n",
      "\n",
      "         [[1.0165e-01, 0.0000e+00, 1.3019e-01, 2.4331e-01],\n",
      "          [1.7269e-01, 0.0000e+00, 0.0000e+00, 2.4024e-01],\n",
      "          [1.2771e-01, 0.0000e+00, 9.5308e-02, 5.7793e-01],\n",
      "          [2.2411e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 4.0006e-02, 5.6858e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9407e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 3.5714e-03, 3.9732e-01],\n",
      "          [0.0000e+00, 4.0023e-02, 1.8024e-01, 8.8047e-01]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3977e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 6.1690e-01, 8.3333e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.5187e-01, 1.4085e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 3.5957e-02, 0.0000e+00]],\n",
      "\n",
      "         [[3.0180e-01, 2.5117e-01, 5.2555e-02, 3.5095e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 2.2309e-01, 1.7721e-01],\n",
      "          [0.0000e+00, 2.1539e-01, 4.8005e-01, 0.0000e+00],\n",
      "          [2.0686e-01, 5.5049e-01, 6.9588e-01, 5.3676e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.3578e-01, 1.4642e-01, 4.3261e-01, 4.4860e-01],\n",
      "          [0.0000e+00, 7.3825e-03, 2.8840e-01, 2.4194e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.8339e-02, 2.4439e-01],\n",
      "          [8.7670e-01, 4.0882e-01, 9.3262e-02, 8.7157e-02]],\n",
      "\n",
      "         [[4.2232e-01, 1.2441e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0412e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.2080e-01, 3.9040e-01, 2.5791e-02, 0.0000e+00],\n",
      "          [7.0812e-01, 2.7725e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.2837e+00, 7.7720e-01, 0.0000e+00, 1.3088e-01],\n",
      "          [1.3061e+00, 1.1616e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.2070e-01, 7.3110e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 2.5153e-02, 7.3822e-02, 1.2710e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.8607e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.7074e-01, 6.4452e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [1.0762e+00, 9.5151e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0280e-02]],\n",
      "\n",
      "         [[9.8293e-01, 3.8151e-02, 0.0000e+00, 0.0000e+00],\n",
      "          [1.9849e+00, 1.3293e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [6.6384e-01, 7.4747e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[5.0846e-01, 1.3086e-01, 1.8741e-01, 4.5585e-01],\n",
      "          [3.5816e-01, 0.0000e+00, 0.0000e+00, 2.7572e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1126e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0979e-01]]],\n",
      "\n",
      "\n",
      "        [[[2.1595e-01, 1.1665e-01, 0.0000e+00, 1.5170e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8622e-02],\n",
      "          [7.6539e-02, 0.0000e+00, 0.0000e+00, 3.9237e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4927e-01]],\n",
      "\n",
      "         [[7.6885e-02, 0.0000e+00, 0.0000e+00, 2.2066e-02],\n",
      "          [2.1685e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.9086e-01, 1.4497e-01, 1.7847e-01, 0.0000e+00],\n",
      "          [4.2632e-01, 1.2138e+00, 8.4704e-01, 0.0000e+00]],\n",
      "\n",
      "         [[4.0809e-01, 0.0000e+00, 2.1326e-01, 3.9940e-01],\n",
      "          [5.2010e-01, 6.7214e-03, 3.6464e-01, 5.8390e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 4.8838e-01, 2.3755e-01],\n",
      "          [1.3527e-01, 0.0000e+00, 1.2194e-01, 0.0000e+00],\n",
      "          [1.0103e-01, 3.5915e-01, 0.0000e+00, 1.5934e-01],\n",
      "          [0.0000e+00, 1.3741e+00, 1.6263e+00, 6.2613e-01]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 2.2307e-01, 9.2222e-02],\n",
      "          [6.4332e-01, 1.3732e-01, 1.2238e+00, 5.5892e-01],\n",
      "          [1.7521e-01, 2.3195e-01, 4.3394e-01, 3.9164e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 5.9715e-01, 6.8072e-01]],\n",
      "\n",
      "         [[4.6217e-01, 6.5594e-01, 7.3069e-01, 8.3431e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.2466e-01, 1.2812e+00],\n",
      "          [1.0521e+00, 2.2856e-01, 0.0000e+00, 6.4906e-01],\n",
      "          [4.9690e-01, 3.0810e-01, 1.3593e+00, 1.1906e+00]]]], device='cuda:0',\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "!python /root/local/mase-gp/src/ch transform --config {VGG_TYPEWISE_TOML} --load {VGG_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By quantizing all convolutional layers to INT8 and maintaining fp16 precision for the linear layers we see a marginal decrease in latency whilst maintaining a comparable accuracy. By experimenting with precisions on a per type basis, you may find insights that work best for your model. \n",
    "\n",
    "## Section 4. Layer-wise Mixed Precision\n",
    "\n",
    "So far we have strictly quantized either in int8 or fp16. Now, we will show how to conduct layerwise mixed precision using the same `vgg7` model. In this case we will show how for instance, layer 0 and 1 can be set to fp16, while the remaining layers can be int8 quantized. \n",
    "\n",
    "For this, we set:\n",
    "- The `by` parameter to `name`\n",
    "- The `precision` to 'int8' for `passes.tensorrt.default.config`\n",
    "- The `precision` to 'fp16' for `passes.tensorrt.feature_layers_0.config and passes.tensorrt.feature_layers_1.config`\n",
    "- The `precision` to 'int8' for `passes.tensorrt.feature_layers_2.config and passes.tensorrt.feature_layers_3.config` (although this is not necessary since the default is already set to 'int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-28 23:25:51,157] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0328 23:25:54.303634 140449214740288 seed.py:54] Seed set to 0\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |        Default         | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |     \u001b[38;5;8mclassification\u001b[0m     |     cls      |                          |           cls            |\n",
      "| load_name               |          \u001b[38;5;8mNone\u001b[0m          |              | /root/mase/mase_output/v | /root/mase/mase_output/v |\n",
      "|                         |                        |              |  gg7-pre-trained/test-   |  gg7-pre-trained/test-   |\n",
      "|                         |                        |              |     accu-0.9332.ckpt     |     accu-0.9332.ckpt     |\n",
      "| load_type               |           \u001b[38;5;8mmz\u001b[0m           |              |            pl            |            pl            |\n",
      "| batch_size              |          \u001b[38;5;8m128\u001b[0m           |      64      |                          |            64            |\n",
      "| to_debug                |         False          |              |                          |          False           |\n",
      "| log_level               |          info          |              |                          |           info           |\n",
      "| report_to               |      tensorboard       |              |                          |       tensorboard        |\n",
      "| seed                    |           0            |              |                          |            0             |\n",
      "| quant_config            |          None          |              |                          |           None           |\n",
      "| training_optimizer      |          adam          |              |                          |           adam           |\n",
      "| trainer_precision       |        16-mixed        |              |                          |         16-mixed         |\n",
      "| learning_rate           |         \u001b[38;5;8m1e-05\u001b[0m          |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |           0            |              |                          |            0             |\n",
      "| max_epochs              |           \u001b[38;5;8m20\u001b[0m           |      10      |                          |            10            |\n",
      "| max_steps               |           -1           |              |                          |            -1            |\n",
      "| accumulate_grad_batches |           1            |              |                          |            1             |\n",
      "| log_every_n_steps       |           50           |              |                          |            50            |\n",
      "| num_workers             |           28           |              |                          |            28            |\n",
      "| num_devices             |           1            |              |                          |            1             |\n",
      "| num_nodes               |           1            |              |                          |            1             |\n",
      "| accelerator             |          \u001b[38;5;8mauto\u001b[0m          |     gpu      |                          |           gpu            |\n",
      "| strategy                |          auto          |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |         False          |              |                          |          False           |\n",
      "| github_ci               |         False          |              |                          |          False           |\n",
      "| disable_dataset_cache   |         False          |              |                          |          False           |\n",
      "| target                  |  xcu250-figd2104-2L-e  |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |          100           |              |                          |           100            |\n",
      "| is_pretrained           |         False          |              |                          |          False           |\n",
      "| max_token_len           |          512           |              |                          |           512            |\n",
      "| project_dir             | /root/mase/mase_output |              |                          |  /root/mase/mase_output  |\n",
      "| project                 |          None          |              |                          |           None           |\n",
      "| model                   |          \u001b[38;5;8mNone\u001b[0m          |     vgg7     |                          |           vgg7           |\n",
      "| dataset                 |          \u001b[38;5;8mNone\u001b[0m          |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |           20           |              |                          |            20            |\n",
      "| eta_min                 |         1e-06          |              |                          |          1e-06           |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7'...\u001b[0m\n",
      "I0328 23:25:54.313626 140449214740288 cli.py:846] Initialising model 'vgg7'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "I0328 23:25:54.417618 140449214740288 cli.py:874] Initialising dataset 'cifar10'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-28\u001b[0m\n",
      "I0328 23:25:54.418019 140449214740288 cli.py:910] Project will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-28\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'vgg7'...\u001b[0m\n",
      "I0328 23:25:54.535444 140449214740288 cli.py:370] Transforming model 'vgg7'...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\u001b[0m\n",
      "I0328 23:26:00.655963 140449214740288 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\u001b[0m\n",
      "I0328 23:26:00.777872 140449214740288 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0328 23:26:12.570783 140449214740288 utils.py:314] Applying fake quantization to PyTorch model...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0328 23:26:12.921518 140449214740288 utils.py:339] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "I0328 23:26:12.940881 140449214740288 summary.py:84] Quantized graph histogram:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm2d     | batch_norm2d |       6 |         0 |           6 |\n",
      "| Conv2d          | conv2d       |       6 |         5 |           1 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| MaxPool2d       | max_pool2d   |       3 |         0 |           3 |\n",
      "| ReLU            | relu         |       8 |         0 |           8 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| view            | view         |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
      "I0328 23:26:12.941653 140449214740288 summary.py:85] \n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm2d     | batch_norm2d |       6 |         0 |           6 |\n",
      "| Conv2d          | conv2d       |       6 |         5 |           1 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| MaxPool2d       | max_pool2d   |       3 |         0 |           3 |\n",
      "| ReLU            | relu         |       8 |         0 |           8 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| view            | view         |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0328 23:26:12.942434 140449214740288 calibrate.py:143] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.952852 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953087 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953220 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953323 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953427 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953537 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953635 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953727 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953824 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953913 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954009 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954101 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954192 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954295 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954387 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954478 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.535559 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.535989 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536076 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536209 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536275 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536394 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536448 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536548 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536605 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536704 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536756 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536853 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536910 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537009 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537059 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537172 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537243 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537340 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537401 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537500 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537567 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537664 140449214740288 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537712 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537806 140449214740288 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537872 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537964 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.538011 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.538102 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.538153 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.538257 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.538305 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.538399 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0328 23:26:18.546847 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0328 23:26:18.546956 140449214740288 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2937 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.547093 140449214740288 calibrate.py:131] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2937 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.547413 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2366 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.547523 140449214740288 calibrate.py:131] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2366 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.547886 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=1.8330 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.548011 140449214740288 calibrate.py:131] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=1.8330 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.548304 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2296 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.548407 140449214740288 calibrate.py:131] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2296 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.548746 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.4681 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.548850 140449214740288 calibrate.py:131] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.4681 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.549174 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2080 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.549281 140449214740288 calibrate.py:131] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2080 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.549598 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.9284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.549701 140449214740288 calibrate.py:131] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.9284 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.550004 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.550112 140449214740288 calibrate.py:131] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2013 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.550404 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.6127 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.550505 140449214740288 calibrate.py:131] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.6127 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.550795 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.1879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.550894 140449214740288 calibrate.py:131] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.1879 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.551028 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.551139 140449214740288 calibrate.py:131] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.551264 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([1024, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.551505 140449214740288 calibrate.py:131] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.551799 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=9.7654 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.551900 140449214740288 calibrate.py:131] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=9.7654 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.552192 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.0590 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.552289 140449214740288 calibrate.py:131] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.0590 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.552608 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=7.4475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.552708 140449214740288 calibrate.py:131] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=7.4475 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.552994 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1019 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.553097 140449214740288 calibrate.py:131] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1019 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "I0328 23:26:18.554053 140449214740288 calibrate.py:105] Performing post calibration analysis for calibrator percentile_99.0...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on vgg7\u001b[0m\n",
      "I0328 23:26:18.554283 140449214740288 runtime_analysis.py:357] Starting transformation analysis on vgg7\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.87663   |\n",
      "|      Average Precision       |   0.87531   |\n",
      "|        Average Recall        |   0.87386   |\n",
      "|       Average F1 Score       |   0.87335   |\n",
      "|         Average Loss         |   0.65133   |\n",
      "|       Average Latency        |  17.901 ms  |\n",
      "|   Average GPU Power Usage    |  57.532 W   |\n",
      "| Inference Energy Consumption | 0.28607 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0328 23:26:29.263397 140449214740288 runtime_analysis.py:521] \n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.87663   |\n",
      "|      Average Precision       |   0.87531   |\n",
      "|        Average Recall        |   0.87386   |\n",
      "|       Average F1 Score       |   0.87335   |\n",
      "|         Average Loss         |   0.65133   |\n",
      "|       Average Latency        |  17.901 ms  |\n",
      "|   Average GPU Power Usage    |  57.532 W   |\n",
      "| Inference Energy Consumption | 0.28607 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_50/model.json\u001b[0m\n",
      "I0328 23:26:29.264865 140449214740288 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_50/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0328 23:26:29.265057 140449214740288 calibrate.py:118] Post calibration analysis complete.\n",
      "W0328 23:26:29.265783 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.9458 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.266022 140449214740288 calibrate.py:131] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.9458 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.266428 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3704 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.266630 140449214740288 calibrate.py:131] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3704 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.267089 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2568 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.267289 140449214740288 calibrate.py:131] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2568 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.267678 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.267869 140449214740288 calibrate.py:131] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3621 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.268326 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.4123 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.268521 140449214740288 calibrate.py:131] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.4123 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.268913 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2821 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.269103 140449214740288 calibrate.py:131] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2821 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.269515 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.9841 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.269702 140449214740288 calibrate.py:131] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.9841 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.270093 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2734 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.270280 140449214740288 calibrate.py:131] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2734 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.270668 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.7013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.270851 140449214740288 calibrate.py:131] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.7013 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.271238 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2519 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.271429 140449214740288 calibrate.py:131] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2519 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.271625 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.271768 140449214740288 calibrate.py:131] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.271936 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([1024, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.272221 140449214740288 calibrate.py:131] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.272616 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=38.3167 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.272804 140449214740288 calibrate.py:131] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=38.3167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.273202 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1175 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.273392 140449214740288 calibrate.py:131] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1175 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.273818 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=19.2420 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.274000 140449214740288 calibrate.py:131] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=19.2420 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.274388 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1626 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.274579 140449214740288 calibrate.py:131] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1626 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "I0328 23:26:29.275199 140449214740288 calibrate.py:105] Performing post calibration analysis for calibrator percentile_99.9...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on vgg7\u001b[0m\n",
      "I0328 23:26:29.275390 140449214740288 runtime_analysis.py:357] Starting transformation analysis on vgg7\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.92097   |\n",
      "|      Average Precision       |   0.91959   |\n",
      "|        Average Recall        |   0.91991   |\n",
      "|       Average F1 Score       |   0.91957   |\n",
      "|         Average Loss         |   0.23991   |\n",
      "|       Average Latency        |  18.132 ms  |\n",
      "|   Average GPU Power Usage    |  57.867 W   |\n",
      "| Inference Energy Consumption | 0.29145 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0328 23:26:40.146152 140449214740288 runtime_analysis.py:521] \n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.92097   |\n",
      "|      Average Precision       |   0.91959   |\n",
      "|        Average Recall        |   0.91991   |\n",
      "|       Average F1 Score       |   0.91957   |\n",
      "|         Average Loss         |   0.23991   |\n",
      "|       Average Latency        |  18.132 ms  |\n",
      "|   Average GPU Power Usage    |  57.867 W   |\n",
      "| Inference Energy Consumption | 0.29145 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_51/model.json\u001b[0m\n",
      "I0328 23:26:40.148627 140449214740288 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_51/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0328 23:26:40.148960 140449214740288 calibrate.py:118] Post calibration analysis complete.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0328 23:26:40.149318 140449214740288 calibrate.py:213] Succeeded in calibrating the model in PyTorch!\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mFine tuning is disabled in the config. Skipping QAT fine tuning.\u001b[0m\n",
      "W0328 23:26:40.155524 140449214740288 fine_tune.py:92] Fine tuning is disabled in the config. Skipping QAT fine tuning.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0328 23:26:40.159881 140449214740288 quantize.py:209] Converting PyTorch model to ONNX...\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_43/model.onnx\u001b[0m\n",
      "I0328 23:26:48.207860 140449214740288 quantize.py:239] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_43/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0328 23:26:48.210374 140449214740288 quantize.py:102] Converting PyTorch model to TensorRT...\n",
      "[03/28/2024-23:26:55] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_40/model.trt\u001b[0m\n",
      "I0328 23:06:32.223787 139939454809920 quantize.py:202] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_40/model.trt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_41/model.json\u001b[0m\n",
      "I0328 23:06:32.581682 139939454809920 quantize.py:259] TensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_41/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_46/model.json\u001b[0m\n",
      "I0328 23:06:47.114224 139939454809920 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_46/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on vgg7-trt_quantized\u001b[0m\n",
      "I0328 23:06:47.208054 139939454809920 runtime_analysis.py:357] Starting transformation analysis on vgg7-trt_quantized\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results vgg7-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.91823   |\n",
      "|      Average Precision       |   0.9121    |\n",
      "|        Average Recall        |   0.90467   |\n",
      "|       Average F1 Score       |   0.92419   |\n",
      "|         Average Loss         |   0.24202   |\n",
      "|       Average Latency        |  8.2307 ms  |\n",
      "|   Average GPU Power Usage    |  55.687 W   |\n",
      "| Inference Energy Consumption | 0.12102 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0328 23:07:00.676242 139939454809920 runtime_analysis.py:521] \n",
      "Results vgg7-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.91823   |\n",
      "|      Average Precision       |   0.9121    |\n",
      "|        Average Recall        |   0.90467   |\n",
      "|       Average F1 Score       |   0.92419   |\n",
      "|         Average Loss         |   0.24202   |\n",
      "|       Average Latency        |  8.2307 ms  |\n",
      "|   Average GPU Power Usage    |  55.687 W   |\n",
      "| Inference Energy Consumption |  0.12102 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/tensorrt/version_8/model.json\u001b[0m\n",
      "I0328 23:07:00.677799 139939454809920 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/tensorrt/version_8/model.json\n"
     ]
    }
   ],
   "source": [
    "VGG_LAYERWISE_TOML = \"../../../machop/configs/tensorrt/vgg7_layerwise_mixed_precision.toml\"\n",
    "\n",
    "!python /root/local/mase-gp/src/ch transform --config {VGG_LAYERWISE_TOML} --load {VGG_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see through the quantized summary that one convolutional layer (feature_layers_1) has not been quantized as its precision will be configured to 'fp16' in the tensorrt engine conversion stage whilst the remaining convolutional and linear layers have been quantized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
