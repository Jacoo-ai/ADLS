{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADLS Proj: TensorRT with MASE for Multiple Precision Inference\n",
    "\n",
    "This notebook demonstrates the integration of TensorRT passes into MASE as part of the MASERT framework.\n",
    "\n",
    "Currently, our experiments are conducted on RTX 4060 and RTX 3070 GPUs, as our request for A100 access is still pending.\n",
    "\n",
    "### Objective\n",
    "Our goal is to plot trade-off curves that analyze the relationship between different variables, including:\n",
    "- **GPU Type** (e.g., RTX 4060, RTX 3070, and A100 when available)\n",
    "- **Dataset** (e.g., CIFAR-10)\n",
    "- **Model Type** (e.g., ResNet18, ResNet50, VGG, AlexNet ...)\n",
    "- **Precision vs. Runtime Trade-off** (FP32, FP16, INT8)\n",
    "\n",
    "At this stage, we have successfully implemented inference using multiple models, such as **ResNet18 and ResNet50**, on the **CIFAR-10 dataset**. Further experiments will explore the precision-runtime trade-off across different GPU architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model for Quantization Experiments\n",
    "\n",
    "In this section, we train an original model of a target model type. The trained model will later serve as a baseline for different precision quantization experiments, including FP32, FP16, and INT8. This process helps in evaluating the trade-offs between model accuracy and runtime efficiency across different GPU architectures.\n",
    "\n",
    "#### Running the Training Script\n",
    "\n",
    "To train the model, execute the following command:\n",
    "\n",
    "```bash\n",
    "!python3 ./ch train --config /workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_INT8_quantization_by_type.toml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0311 00:15:51.512130 140276942296128 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File | Manual Override |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                 |           cls            |\n",
      "| load_name               |           None           |              |                 |           None           |\n",
      "| load_type               |            mz            |              |                 |            mz            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                 |            64            |\n",
      "| to_debug                |          False           |              |                 |          False           |\n",
      "| log_level               |           info           |              |                 |           info           |\n",
      "| report_to               |       tensorboard        |              |                 |       tensorboard        |\n",
      "| seed                    |            0             |              |                 |            0             |\n",
      "| quant_config            |           None           |              |                 |           None           |\n",
      "| training_optimizer      |           adam           |              |                 |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                 |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                 |          0.001           |\n",
      "| weight_decay            |            0             |              |                 |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                 |            10            |\n",
      "| max_steps               |            -1            |              |                 |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                 |            1             |\n",
      "| log_every_n_steps       |            50            |              |                 |            50            |\n",
      "| num_workers             |            20            |              |                 |            20            |\n",
      "| num_devices             |            1             |              |                 |            1             |\n",
      "| num_nodes               |            1             |              |                 |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                 |           gpu            |\n",
      "| strategy                |           auto           |              |                 |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                 |          False           |\n",
      "| github_ci               |          False           |              |                 |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                 |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                 |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                 |           100            |\n",
      "| is_pretrained           |          False           |              |                 |          False           |\n",
      "| max_token_len           |           512            |              |                 |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                 | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                 |         e_output         |\n",
      "| project                 |           None           |              |                 |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                 |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                 |         cifar10          |\n",
      "| t_max                   |            20            |              |                 |            20            |\n",
      "| eta_min                 |          1e-06           |              |                 |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-11\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m##### WEIGHT DECAY ##### 0\u001b[0m\n",
      "I0311 00:15:51.711618 140276942296128 rank_zero.py:63] Using 16bit Automatic Mixed Precision (AMP)\n",
      "I0311 00:15:51.718478 140276942296128 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0311 00:15:51.718928 140276942296128 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0311 00:15:51.718978 140276942296128 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0311 00:15:54.418565 140276942296128 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0311 00:15:54.708575 140276942296128 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0:  53%|▌| 411/782 [00:11<00:10, 35.12it/s, v_num=0, train_acc_step=0.234]I0311 00:16:07.596064 140276942296128 rank_zero.py:63] \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 ./ch train --config /workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_INT8_quantization_by_type.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INT8 Quantization with TensorRT\n",
    "\n",
    "This section explains the process of **INT8 quantization** using TensorRT within the MASE framework. The key steps include **fake quantization, calibration, fine-tuning, and generating a TensorRT engine**.\n",
    "\n",
    "### Code Execution Flow\n",
    "\n",
    "1. **Apply TensorRT Passes**\n",
    "   - **Fake Quantization**: Inserts quantization simulation operations.\n",
    "   - **Summarization**: Displays which layers were quantized.\n",
    "   - **Calibration**: Uses calibration algorithms (e.g., histogram-based) to determine optimal quantization parameters.\n",
    "   - **Fine-Tuning**: Adjusts parameters to recover accuracy loss after quantization.\n",
    "\n",
    "2. **Generate the TensorRT Engine**\n",
    "   - Calls `tensorrt_engine_interface_pass` to convert the optimized graph into a **TensorRT engine**.\n",
    "\n",
    "3. **Benchmarking & Performance Analysis**\n",
    "   - Runs inference tests with warm-up and batch evaluation to measure efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 23:33:16.033470 139949971272768 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBefore analyzing original graph: pass_args = {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}, 'task': 'cls', 'dataset': 'cifar10', 'batch_size': 64, 'model': 'resnet18', 'data_module': <chop.dataset.MaseDataModule object at 0x7f46783a6790>, 'accelerator': 'cuda', 'num_GPU_warmup_batches': 0, 'num_batches': 1, 'test': True}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.68146    |\n",
      "|      Average Precision       |   0.72305    |\n",
      "|        Average Recall        |   0.70312    |\n",
      "|       Average F1 Score       |    0.7063    |\n",
      "|         Average Loss         |   0.88661    |\n",
      "|       Average Latency        |  9.1563 ms   |\n",
      "|   Average GPU Power Usage    |   12.336 W   |\n",
      "| Inference Energy Consumption | 0.031376 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_92/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.374635 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.374905 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375022 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375117 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375214 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375304 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375401 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375502 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375596 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375684 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375777 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375866 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.375959 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376046 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376138 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376226 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376351 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376449 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376544 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376631 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376723 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.376934 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377039 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377126 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377218 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377311 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377403 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377496 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377590 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377676 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377769 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377855 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.377939 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378027 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378117 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378202 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378294 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378367 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378456 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378540 139949971272768 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378637 139949971272768 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:33:28.378722 139949971272768 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0316 23:33:28.378963 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 23:33:28.379018 139949971272768 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.379268 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.379453 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2058 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.379612 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.379772 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1079 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.379925 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.380090 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.380250 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.380419 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1361 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.380581 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.380751 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4339 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.380904 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.381063 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.381209 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.381362 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4339 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.381511 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.381663 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5862 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.381811 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.382134 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2681 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.382292 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.382451 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5355 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.382599 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.382751 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.382896 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.383047 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5355 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.383193 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.383353 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3890 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.383574 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.383742 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384056 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384229 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2509 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384377 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384526 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5516 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384670 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384847 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2509 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.384998 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.385150 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.385288 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.385429 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.8474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.385566 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.385680 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:28.386364 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72647    |\n",
      "|      Average Precision       |   0.71544    |\n",
      "|        Average Recall        |   0.71927    |\n",
      "|       Average F1 Score       |   0.71627    |\n",
      "|         Average Loss         |   0.81653    |\n",
      "|       Average Latency        |   28.48 ms   |\n",
      "|   Average GPU Power Usage    |   9.9762 W   |\n",
      "| Inference Energy Consumption | 0.078923 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_93/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 23:33:34.669170 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6391 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.670367 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.670709 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.670995 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.671260 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2295 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.671504 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.671758 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.672040 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.672278 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1502 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.672672 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.672982 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.673335 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.673606 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3220 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.673857 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.674115 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.674345 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.674587 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2313 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.674825 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.675070 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.675660 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.675922 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.676162 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.676598 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0989 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.676830 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.677070 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.8090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.677295 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.677526 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6025 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.677749 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.677980 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5455 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.678203 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.678431 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.678693 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.678922 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.4767 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.679169 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.679399 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.6085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.679628 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.679853 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5514 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.680091 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.680317 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3538 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.680538 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.680737 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:34.680893 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72817    |\n",
      "|      Average Precision       |   0.71891    |\n",
      "|        Average Recall        |   0.72206    |\n",
      "|       Average F1 Score       |   0.71969    |\n",
      "|         Average Loss         |   0.80688    |\n",
      "|       Average Latency        |  26.695 ms   |\n",
      "|   Average GPU Power Usage    |   10.231 W   |\n",
      "| Inference Energy Consumption | 0.075864 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_94/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 23:33:40.369793 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.370489 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.370844 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8670 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.371139 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.371416 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3450 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.371681 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.371962 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6981 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.372231 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.372495 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1751 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.372755 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.373031 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7036 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.373361 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.373635 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3397 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.373938 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.374217 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7036 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.374497 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.374927 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7919 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.375241 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.375535 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.376043 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.376321 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0064 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.376582 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.377084 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.3984 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.377329 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.377593 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.0064 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.377834 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.378083 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.378326 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.378573 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.2523 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.378813 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.379063 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8157 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.379321 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.379645 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2320 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.379895 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.380143 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.8157 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.380383 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.380625 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0683 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.380862 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.381102 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7768 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.381345 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.381557 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:40.381721 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+------------+\n",
      "|      Metric (Per Batch)      |   Value    |\n",
      "+------------------------------+------------+\n",
      "| Average Validation Accuracy  |  0.72906   |\n",
      "|      Average Precision       |  0.71952   |\n",
      "|        Average Recall        |  0.72268   |\n",
      "|       Average F1 Score       |  0.72018   |\n",
      "|         Average Loss         |  0.80881   |\n",
      "|       Average Latency        | 27.193 ms  |\n",
      "|   Average GPU Power Usage    |  10.353 W  |\n",
      "| Inference Energy Consumption | 0.0782 mWh |\n",
      "+------------------------------+------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_95/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 23:33:46.393481 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3198 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:46.580791 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:46.936910 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9787 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:47.134923 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:47.413661 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9539 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:47.602616 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:48.001583 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.4356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:48.184937 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:48.371558 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.7954 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:48.561864 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:48.963184 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.3566 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:49.148711 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:49.459297 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:49.645491 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:50.035724 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.3566 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:50.218654 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:50.492040 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7086 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:50.674982 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:50.896987 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8137 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:51.079945 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:51.348553 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2809 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:51.537195 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:51.816132 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0214 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:51.993501 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:52.250335 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.2809 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:52.425881 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:52.690710 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:52.871062 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:53.051720 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.0620 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:53.233784 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:53.476575 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.3893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:53.663780 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:53.841041 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:54.017572 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:54.275478 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.3893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:54.458733 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:54.642318 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.5748 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:54.818391 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:54.992552 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7390 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:55.179094 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:55.181164 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:33:55.181576 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72856    |\n",
      "|      Average Precision       |   0.71857    |\n",
      "|        Average Recall        |   0.72175    |\n",
      "|       Average F1 Score       |   0.71928    |\n",
      "|         Average Loss         |   0.80959    |\n",
      "|       Average Latency        |  29.637 ms   |\n",
      "|   Average GPU Power Usage    |   9.9135 W   |\n",
      "| Inference Energy Consumption | 0.081613 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_96/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 23:34:03.560781 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:04.966312 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:07.634674 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.3725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:09.054171 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:10.878047 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:12.280713 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:15.369678 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1397 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:16.802258 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:18.203141 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.7466 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:19.627512 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:22.753909 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:24.159440 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:26.375501 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.9187 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:27.817661 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:30.957582 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:32.330909 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:34.137400 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0212 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:35.560478 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:37.129853 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5567 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:38.575501 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:40.396651 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.8660 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:41.819386 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:43.817842 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.9484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:45.208031 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:47.035292 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.8660 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:48.417859 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:50.161555 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7637 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:51.559497 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:52.952321 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6150 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:54.282879 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:55.785040 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.4419 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:57.181826 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:34:58.630017 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:00.010381 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:01.497818 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.4419 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:02.940324 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:04.362363 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0067 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:05.797974 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:07.183573 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9238 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:08.614521 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:08.615902 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:35:08.616193 139949971272768 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72818    |\n",
      "|      Average Precision       |   0.71889    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |   0.71974    |\n",
      "|         Average Loss         |   0.81131    |\n",
      "|       Average Latency        |  23.414 ms   |\n",
      "|   Average GPU Power Usage    |   13.143 W   |\n",
      "| Inference Energy Consumption | 0.085483 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_97/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0316 23:35:13.945412 139949971272768 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0316 23:35:13.945756 139949971272768 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0316 23:35:13.945809 139949971272768 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0316 23:35:16.615782 139949971272768 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 23:35:16.628233 139949971272768 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:28<00:00, 27.84it/s, v_num=21, train_acc_step=0.500\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 44.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 41.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 35.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 39.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 42.11it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 42.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 41.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 41.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 41.64it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 41.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 41.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 42.24it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 41.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 42.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 42.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 42.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 43.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 43.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 43.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 43.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 44.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 44.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 44.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 44.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 44.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 44.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 44.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 44.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 44.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 45.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 45.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 45.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 45.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 45.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 45.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 45.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 45.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 45.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 45.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 45.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 45.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:02, 45.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:02, 45.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:02, 45.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 45.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 45.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 45.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 44.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 44.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 44.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 44.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 44.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 44.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 44.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 44.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 44.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 44.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 44.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 44.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 44.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 44.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 44.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 44.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 44.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 44.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 44.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 44.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 44.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 44.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 44.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 44.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 44.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 44.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 44.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 44.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 44.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 43.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 43.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 43.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 43.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 43.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 43.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 43.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 43.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 43.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 43.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 43.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 43.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 43.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 43.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 43.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 43.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 43.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 43.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 43.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 43.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 43.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 43.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 43.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 43.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 43.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 43.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 43.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 43.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:00, 43.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:00, 43.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 43.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 43.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 43.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 43.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 43.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 43.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 43.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 43.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 43.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 43.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 43.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 43.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 43.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 43.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 43.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 43.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 44.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 44.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 44.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 43.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 44.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 43.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 43.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 43.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 43.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 43.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 43.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 43.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 43.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 43.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 43.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 43.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 43.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 43.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 43.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 43.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:03<00:00, 43.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:03<00:00, 43.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:03<00:00, 43.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:03<00:00, 43.96it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:30<00:00, 25.29it/s, v_num=21, train_acc_step=0.625\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:04, 35.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 39.49it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 43.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 44.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 44.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 44.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 44.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 45.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 44.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 44.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 44.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 44.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 44.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 44.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 43.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 44.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 43.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 43.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 41.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 41.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 41.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 41.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 41.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 41.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 41.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 41.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 41.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 41.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 41.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 41.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 41.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 40.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 40.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 40.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 40.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 40.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:02, 40.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:02, 40.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:02, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:02, 40.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 40.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 40.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 40.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 39.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 39.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 39.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 39.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 39.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 39.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 39.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 39.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 39.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 39.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 39.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 39.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 39.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 39.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 39.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 39.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 39.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 39.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 39.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 39.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:02, 39.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:02, 39.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:02, 39.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:02, 39.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:02, 39.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:02, 39.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:02, 39.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:02, 39.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:02, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:02, 39.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:02<00:01, 39.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:01, 39.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:01, 39.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:01, 39.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:01, 39.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:01, 39.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:01, 39.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 39.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 39.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 39.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 39.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 39.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 39.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 39.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 39.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 38.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 38.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 38.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 38.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 38.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 38.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 38.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 38.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 38.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 38.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 38.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 38.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 38.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 38.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 38.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 38.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 38.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 38.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 38.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 38.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:01, 38.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 38.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:03<00:01, 38.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:03<00:00, 38.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:03<00:00, 38.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:03<00:00, 38.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:03<00:00, 38.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 38.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 38.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 38.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 38.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 38.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 38.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 38.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 38.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 38.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 38.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 38.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 38.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 38.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 38.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 38.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 38.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 38.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 37.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 37.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 37.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 37.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 37.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 37.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 37.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 37.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 37.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 37.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 37.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:04<00:00, 37.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:04<00:00, 37.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:04<00:00, 37.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:04<00:00, 37.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:04<00:00, 37.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:04<00:00, 37.76it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:04<00:00, 37.79it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:35<00:00, 21.96it/s, v_num=21, train_acc_step=0.625\u001b[AI0316 23:36:26.460790 139949971272768 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:35<00:00, 21.73it/s, v_num=21, train_acc_step=0.625\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_26/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "default_precision: int8\n",
      "Failed finding int8 cache!\n",
      "Failed finding int8 cache!\n",
      "Succeed saving int8 cache!\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_28/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_29/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/16/2025-23:44:02] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+---------------+\n",
      "|      Metric (Per Batch)      |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73739    |\n",
      "|      Average Precision       |    0.73949    |\n",
      "|        Average Recall        |    0.74069    |\n",
      "|       Average F1 Score       |    0.73859    |\n",
      "|         Average Loss         |    0.7431     |\n",
      "|       Average Latency        |   2.2448 ms   |\n",
      "|   Average GPU Power Usage    |   15.399 W    |\n",
      "| Inference Energy Consumption | 0.0096021 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/tensorrt/version_8/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet18_INT8_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 19:14:45.704316 139759675335744 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBefore analyzing original graph: pass_args = {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}, 'task': 'cls', 'dataset': 'cifar10', 'batch_size': 64, 'model': 'resnet18', 'data_module': <chop.dataset.MaseDataModule object at 0x7f1a29c053d0>, 'accelerator': 'cuda', 'num_GPU_warmup_batches': 0, 'num_batches': 1, 'test': True}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.68146    |\n",
      "|      Average Precision       |   0.72305    |\n",
      "|        Average Recall        |   0.70312    |\n",
      "|       Average F1 Score       |    0.7063    |\n",
      "|         Average Loss         |   0.88661    |\n",
      "|       Average Latency        |  9.9334 ms   |\n",
      "|   Average GPU Power Usage    |   12.331 W   |\n",
      "| Inference Energy Consumption | 0.034025 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_86/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.160380 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.160572 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.160691 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.160786 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.160882 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.160973 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161070 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161169 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161264 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161351 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161444 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161532 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161623 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161710 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161803 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161890 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.161984 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162072 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162163 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162249 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162341 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162429 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162520 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162607 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162697 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162809 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.162917 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163016 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163114 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163199 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163291 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163376 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163465 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163552 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163642 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163738 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163842 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.163933 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.164024 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.164110 139759675335744 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.164204 139759675335744 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 19:14:57.164289 139759675335744 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0316 19:14:57.164522 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 19:14:57.164584 139759675335744 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.164860 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.165052 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2058 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.165209 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.165364 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1079 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.165513 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.165668 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.165842 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166006 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1361 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166155 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166321 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4339 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166469 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166618 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166760 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.166912 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4339 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167052 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167197 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5862 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167335 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167480 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2681 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167624 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167770 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5355 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.167914 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168062 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168215 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168361 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5355 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168501 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168647 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3890 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168784 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.168927 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169068 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169209 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2509 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169345 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169497 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5516 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169637 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169781 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2509 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.169917 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.170054 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.170187 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.170326 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.8474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.170459 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.170569 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:14:57.170832 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72647    |\n",
      "|      Average Precision       |   0.71544    |\n",
      "|        Average Recall        |   0.71927    |\n",
      "|       Average F1 Score       |   0.71627    |\n",
      "|         Average Loss         |   0.81653    |\n",
      "|       Average Latency        |  30.006 ms   |\n",
      "|   Average GPU Power Usage    |   10.142 W   |\n",
      "| Inference Energy Consumption | 0.084532 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_87/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 19:15:03.485491 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6391 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.486624 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.487071 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.487331 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.487574 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2295 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.487848 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.488095 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.488316 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.488540 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1502 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.488761 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.488986 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.489199 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.489419 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3220 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.489635 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.489865 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.490086 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.490309 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2313 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.490522 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.490740 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.490963 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.491185 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.491390 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.491936 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0989 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.492667 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.493211 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.8090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.493828 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.494487 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6025 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.495307 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.495651 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5455 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.496009 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.496329 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.496630 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.496933 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.4767 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.497228 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.497522 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.6085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.497824 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.498121 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5514 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.498419 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.498716 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3538 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.499007 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.499273 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:03.499459 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72817    |\n",
      "|      Average Precision       |   0.71891    |\n",
      "|        Average Recall        |   0.72206    |\n",
      "|       Average F1 Score       |   0.71969    |\n",
      "|         Average Loss         |   0.80688    |\n",
      "|       Average Latency        |  30.167 ms   |\n",
      "|   Average GPU Power Usage    |   10.119 W   |\n",
      "| Inference Energy Consumption | 0.084791 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_88/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 19:15:09.671502 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.672293 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.672858 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8670 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.673127 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.673724 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3450 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.673965 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.674210 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6981 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.674436 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.674662 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1751 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.674887 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.675118 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7036 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.675342 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.675570 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3397 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.675842 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.676083 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7036 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.676307 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.676531 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7919 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.676746 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.676966 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.677233 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.677459 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0064 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.677679 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.677900 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.3984 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.678123 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.678348 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.0064 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.678584 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.678810 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.679038 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.679261 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.2523 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.679470 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.679688 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8157 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.679900 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.680170 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2320 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.680377 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.680589 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.8157 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.680850 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.681064 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0683 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.681274 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.681530 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7768 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.681739 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.681933 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:09.682089 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72906    |\n",
      "|      Average Precision       |   0.71952    |\n",
      "|        Average Recall        |   0.72268    |\n",
      "|       Average F1 Score       |   0.72018    |\n",
      "|         Average Loss         |   0.80881    |\n",
      "|       Average Latency        |  30.407 ms   |\n",
      "|   Average GPU Power Usage    |   10.187 W   |\n",
      "| Inference Energy Consumption | 0.086039 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_89/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 19:15:16.123918 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3198 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:16.305811 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:16.619118 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9787 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:16.790971 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:17.043077 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9539 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:17.223336 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:17.576191 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.4356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:17.749911 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:17.919706 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.7954 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:18.085993 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:18.429068 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.3566 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:18.602037 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:18.871437 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:19.041880 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:19.407259 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.3566 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:19.580163 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:19.830562 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7086 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:20.002387 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:20.229384 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8137 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:20.400985 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:20.635998 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2809 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:20.811215 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:21.070531 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0214 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:21.244603 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:21.485077 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.2809 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:21.661268 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:21.901888 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:22.078322 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:22.263101 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.0620 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:22.439616 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:22.679619 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.3893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:22.848660 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:23.020117 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:23.189470 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:23.408177 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.3893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:23.579344 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:23.752161 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.5748 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:23.920157 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:24.094310 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7390 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:24.270331 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:24.271594 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:24.272025 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72856    |\n",
      "|      Average Precision       |   0.71857    |\n",
      "|        Average Recall        |   0.72175    |\n",
      "|       Average F1 Score       |   0.71928    |\n",
      "|         Average Loss         |   0.80959    |\n",
      "|       Average Latency        |  27.908 ms   |\n",
      "|   Average GPU Power Usage    |   10.063 W   |\n",
      "| Inference Energy Consumption | 0.078014 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_90/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 19:15:32.060863 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:33.454139 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:36.121356 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.3725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:37.499657 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:39.295904 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:40.682690 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:43.754514 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1397 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:45.198249 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:46.614595 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.7466 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:48.038631 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:51.166811 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:52.585875 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:54.786376 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.9187 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:56.241037 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:15:59.382497 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0239 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:00.774323 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:02.608540 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0212 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:04.033918 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:05.613139 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5567 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:07.038225 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:08.859338 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.8660 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:10.289275 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:12.265085 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.9484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:13.658717 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:15.504078 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.8660 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:16.872942 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:18.640203 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7637 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:20.065639 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:21.481805 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6150 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:22.861025 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:24.378704 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.4419 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:25.798675 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:27.259727 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:28.689476 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:30.222054 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.4419 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:31.663090 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:33.104988 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0067 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:34.584895 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:36.022940 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9238 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:37.479589 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:37.480759 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.7439 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 19:16:37.481009 139759675335744 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72818    |\n",
      "|      Average Precision       |   0.71889    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |   0.71974    |\n",
      "|         Average Loss         |   0.81131    |\n",
      "|       Average Latency        |  28.175 ms   |\n",
      "|   Average GPU Power Usage    |   10.146 W   |\n",
      "| Inference Energy Consumption | 0.079408 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_91/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0316 19:16:43.552521 139759675335744 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0316 19:16:43.552801 139759675335744 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0316 19:16:43.552855 139759675335744 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0316 19:16:46.154589 139759675335744 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 19:16:46.181025 139759675335744 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:27<00:00, 28.61it/s, v_num=18, train_acc_step=0.562\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 41.91it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:04, 34.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 34.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 37.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 39.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 40.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 40.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 38.64it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:04, 36.63it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:04, 34.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:04, 35.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 35.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 36.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 37.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 37.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 37.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 37.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 38.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 38.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 39.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 38.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 39.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 39.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 39.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 39.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 39.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 40.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 40.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 40.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 40.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:03, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:03, 40.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:03, 40.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 40.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 41.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 41.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 41.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 41.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:02, 41.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:02, 41.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:02, 41.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 41.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 41.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 41.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 41.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 41.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 41.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 41.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 41.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 41.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 42.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 42.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 41.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 42.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 42.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 42.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 42.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 42.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 42.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 42.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 42.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 42.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 42.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 41.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:02, 41.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:02, 41.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:02, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:02, 42.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 42.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 42.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 42.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 42.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 42.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 42.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 42.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 42.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 42.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 42.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 42.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:01, 42.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 42.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 42.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 42.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 42.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 42.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 42.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 42.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 42.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 42.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 42.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 42.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 42.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 42.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 42.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 42.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 42.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 42.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 42.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 42.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 42.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 42.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 42.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 42.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 42.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 42.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:00, 42.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 42.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 42.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 42.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 42.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 42.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 42.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 42.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 42.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 42.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 42.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 42.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 42.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 42.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 42.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 42.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 42.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 42.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 42.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 42.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 42.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 42.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 42.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 42.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 42.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 42.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 42.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 42.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 42.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 42.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 42.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 42.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 42.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 42.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 42.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 42.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:03<00:00, 42.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:03<00:00, 42.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:03<00:00, 42.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:03<00:00, 42.51it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:27<00:00, 28.16it/s, v_num=18, train_acc_step=0.625\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 40.27it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:04, 37.68it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 41.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 44.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 44.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 45.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 41.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 39.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 40.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 40.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 41.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 41.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 41.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 40.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 41.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 41.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 41.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 41.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 42.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 42.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 41.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 42.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 41.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 41.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 40.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 41.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 41.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 41.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:03, 41.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 41.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 41.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 41.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 41.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 41.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 41.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 40.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:02, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:02, 40.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:02, 40.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:02, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 40.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 40.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 40.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 40.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 40.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 40.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 40.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 40.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 40.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 40.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 40.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 40.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 40.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 40.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 40.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 40.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 40.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 40.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:02, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:02, 40.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:02, 40.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:02, 40.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:02, 40.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:02, 40.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:02, 40.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:02, 40.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 40.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 40.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 40.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 40.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:01, 40.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:01, 40.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:01, 40.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:01, 40.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:01, 40.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 40.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 40.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 40.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 40.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 40.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 40.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 40.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 40.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 40.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 40.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 40.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 40.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 40.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 40.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 40.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 40.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 40.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 40.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 40.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 40.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 40.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 40.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:01, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:01, 40.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 40.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 40.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 40.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 40.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:03<00:00, 40.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 40.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 40.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 40.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 40.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 40.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 40.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 40.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 40.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 40.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 40.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 40.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 40.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 40.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 40.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 40.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 40.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 40.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 40.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 40.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 40.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 40.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 40.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 40.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:03<00:00, 40.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:03<00:00, 40.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:03<00:00, 40.19it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:03<00:00, 40.24it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:32<00:00, 24.43it/s, v_num=18, train_acc_step=0.625\u001b[AI0316 19:17:51.149088 139759675335744 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:32<00:00, 24.16it/s, v_num=18, train_acc_step=0.625\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_22/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_23/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_24/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/16/2025-19:18:59] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |    0.7409    |\n",
      "|      Average Precision       |   0.74209    |\n",
      "|        Average Recall        |   0.74357    |\n",
      "|       Average F1 Score       |   0.74158    |\n",
      "|         Average Loss         |   0.74117    |\n",
      "|       Average Latency        |  4.4892 ms   |\n",
      "|   Average GPU Power Usage    |   17.026 W   |\n",
      "| Inference Energy Consumption | 0.021232 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/tensorrt/version_7/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet18_INT8_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 18:51:41.243087 140028686267456 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038181 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038356 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038463 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038551 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038640 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038726 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038814 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.038920 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039004 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039066 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039137 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039218 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039302 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039381 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039465 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039546 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039630 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039707 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039790 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039870 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.039949 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040024 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040102 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040177 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040256 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040321 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040400 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040479 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040560 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040635 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040715 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040791 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040870 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.040945 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041025 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041091 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041171 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041246 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041325 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041399 140028686267456 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041483 140028686267456 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 18:51:52.041559 140028686267456 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0316 18:51:52.041788 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 18:51:52.041850 140028686267456 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.042076 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.042302 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.042496 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.042657 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.042800 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.042952 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8358 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.043092 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.043239 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1359 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.043371 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.043518 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.043669 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.043882 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2610 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044018 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044161 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044302 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044437 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5872 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044570 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044710 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2720 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044836 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.044977 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045109 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045244 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6489 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045373 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045508 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045635 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045772 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4016 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.045896 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046028 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046155 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046287 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046418 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046552 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046676 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046804 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.046932 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.047062 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.9013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.047185 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.047309 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.047438 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.047541 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:52.047793 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72608    |\n",
      "|      Average Precision       |   0.71486    |\n",
      "|        Average Recall        |   0.71875    |\n",
      "|       Average F1 Score       |   0.71577    |\n",
      "|         Average Loss         |   0.81619    |\n",
      "|       Average Latency        |  28.244 ms   |\n",
      "|   Average GPU Power Usage    |   10.273 W   |\n",
      "| Inference Energy Consumption | 0.080596 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_80/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 18:51:57.987610 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.988270 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.988616 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4700 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.988888 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.989168 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.989428 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.989676 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.989906 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.990143 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.990371 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.990602 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.990821 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.991052 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.991273 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.991493 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.991705 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.991925 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1995 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.992156 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.992394 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.992612 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.993002 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.993342 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.993591 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.994117 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.994651 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.995157 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.995421 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6513 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.995711 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.995940 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1650 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.996162 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.996394 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.996624 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.996856 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.997079 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.997304 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.997525 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.997751 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.997969 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.998196 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.998413 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.998611 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:51:57.998760 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+------------+\n",
      "|      Metric (Per Batch)      |   Value    |\n",
      "+------------------------------+------------+\n",
      "| Average Validation Accuracy  |  0.72868   |\n",
      "|      Average Precision       |  0.71952   |\n",
      "|        Average Recall        |  0.72268   |\n",
      "|       Average F1 Score       |   0.7203   |\n",
      "|         Average Loss         |  0.80705   |\n",
      "|       Average Latency        | 29.545 ms  |\n",
      "|   Average GPU Power Usage    |  10.296 W  |\n",
      "| Inference Energy Consumption | 0.0845 mWh |\n",
      "+------------------------------+------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_81/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 18:52:03.939076 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0570 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.939765 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.940217 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8774 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.940539 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.940852 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4204 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.941156 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.941505 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.941797 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.942178 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.942486 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.943221 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.943728 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.944051 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.944357 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.944670 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.945052 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.945410 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.945685 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.945941 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.946180 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.946419 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.946646 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.946872 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.947093 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.947315 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.947533 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.947880 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.948105 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.948328 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7914 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.948553 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.948781 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.949016 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.949371 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1205 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.950031 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.950552 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.950824 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.951076 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7121 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.951310 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.951602 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.951831 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.952040 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:03.952197 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72872   |\n",
      "|      Average Precision       |   0.71967   |\n",
      "|        Average Recall        |   0.72279   |\n",
      "|       Average F1 Score       |   0.72034   |\n",
      "|         Average Loss         |   0.80904   |\n",
      "|       Average Latency        |  27.095 ms  |\n",
      "|   Average GPU Power Usage    |  10.567 W   |\n",
      "| Inference Energy Consumption | 0.07953 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_82/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 18:52:09.825933 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:09.999175 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:10.307472 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:10.479570 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:10.743774 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1439 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:10.921827 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:11.261753 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:11.437798 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:11.686861 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9089 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:11.863479 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:12.045222 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:12.226019 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:12.473916 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:12.646410 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:12.830920 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:13.012352 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:13.190180 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.6229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:13.363533 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:13.582956 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0186 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:13.759749 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:14.078080 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:14.270169 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:14.540811 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:14.723818 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:15.029841 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:15.215953 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:15.470390 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:15.644059 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:15.829029 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7037 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:16.006204 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:16.187105 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:16.380417 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:16.557248 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:16.920341 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:17.105025 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:17.290315 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:17.476316 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:17.665826 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:17.847975 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:18.024927 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:18.025868 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:18.026083 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72848    |\n",
      "|      Average Precision       |   0.71898    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |    0.7197    |\n",
      "|         Average Loss         |   0.80978    |\n",
      "|       Average Latency        |  27.259 ms   |\n",
      "|   Average GPU Power Usage    |   10.021 W   |\n",
      "| Inference Energy Consumption | 0.075878 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_83/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 18:52:25.837457 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:27.261026 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:29.712695 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4599 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:31.143352 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:32.982515 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.8759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:34.413547 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:37.173169 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1156 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:38.644041 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:40.417178 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5661 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:41.918060 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:43.391621 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:44.858359 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:46.828739 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:48.300991 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:49.765295 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:51.204580 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:52.693551 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:54.165482 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:55.678881 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.6632 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:57.157869 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:52:59.618752 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:01.104233 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:03.026506 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.0554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:04.495626 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:06.954960 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:08.398156 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:10.562084 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:12.082008 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:13.548589 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:14.997963 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:16.453018 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:17.883119 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:19.333714 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4003 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:20.759881 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:22.235291 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:23.721967 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:25.210216 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4923 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:26.691809 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:28.146419 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8677 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:29.639132 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:29.646161 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 18:53:29.646652 140028686267456 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7293    |\n",
      "|      Average Precision       |   0.71953    |\n",
      "|        Average Recall        |   0.72299    |\n",
      "|       Average F1 Score       |   0.72043    |\n",
      "|         Average Loss         |   0.81129    |\n",
      "|       Average Latency        |  26.245 ms   |\n",
      "|   Average GPU Power Usage    |   10.354 W   |\n",
      "| Inference Energy Consumption | 0.075484 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_84/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0316 18:53:35.332019 140028686267456 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0316 18:53:35.332374 140028686267456 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0316 18:53:35.332425 140028686267456 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0316 18:53:37.970855 140028686267456 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 18:53:37.991276 140028686267456 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:30<00:00, 25.94it/s, v_num=17, train_acc_step=0.688\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:06, 23.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:05, 27.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 27.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:06, 22.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:06, 23.65it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:06, 24.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:05, 25.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:05, 27.24it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:05, 28.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:05, 29.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:05, 29.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 30.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:04, 30.72it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:04, 31.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:04, 31.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:04, 32.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:04, 32.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:04, 33.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:04, 34.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 34.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 34.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 35.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 35.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 35.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 36.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 35.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 35.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 36.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 36.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 36.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 36.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 36.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:03, 37.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:03, 37.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:03, 37.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:03, 37.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:03, 37.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:03, 37.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:01<00:03, 38.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:01<00:03, 38.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:03, 38.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:03, 38.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:02, 38.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:03, 37.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 37.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 37.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 37.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 37.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 37.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 37.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 37.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 37.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 37.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 37.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 37.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 37.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 37.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 37.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 37.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 37.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 37.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 37.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 37.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 37.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 37.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 37.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 37.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 37.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:02, 37.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:02, 37.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:02, 37.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:02, 37.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:02, 37.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:02, 37.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:02<00:02, 37.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:02<00:02, 37.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:02<00:02, 37.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:02<00:02, 37.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:02<00:02, 37.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:02, 37.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:02, 37.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:01, 37.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:01, 37.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:01, 37.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:01, 37.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 37.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 37.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 37.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 37.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 37.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 37.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 37.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 37.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 37.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 37.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 37.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 37.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 37.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 37.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 37.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 37.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 37.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 37.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 37.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 37.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 37.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 37.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 37.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 37.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 37.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 37.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 37.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 37.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:03<00:01, 37.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:03<00:01, 37.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 37.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 37.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:03<00:01, 37.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:03<00:01, 37.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:03<00:00, 38.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:03<00:00, 38.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:03<00:00, 38.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 38.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 38.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 38.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 38.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 38.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 38.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 38.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 38.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 38.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 38.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 38.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 38.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 38.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 38.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 38.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 38.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 38.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 38.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 38.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 38.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 38.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 38.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 38.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 38.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 38.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:03<00:00, 38.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:04<00:00, 38.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:04<00:00, 38.67it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:04<00:00, 38.69it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:34<00:00, 22.95it/s, v_num=17, train_acc_step=0.125\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:04, 33.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:04, 34.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 28.29it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:05, 30.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:05, 30.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 30.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:04, 31.87it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:04, 32.75it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:04, 33.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:04, 34.22it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:04, 34.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 34.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:04, 32.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:04, 33.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:04, 33.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:04, 33.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:04, 32.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:04, 32.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:04, 33.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:04, 33.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:04, 33.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:04, 33.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:04, 33.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 33.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 33.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 33.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 34.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 34.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 34.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 34.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 34.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 34.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:03, 34.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:01<00:03, 33.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:03, 33.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:03, 32.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:03, 32.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:03, 32.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:01<00:04, 29.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:01<00:04, 28.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:04, 28.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:04, 28.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:04, 28.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:04, 28.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:03, 28.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:04, 26.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:04, 26.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:04, 26.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:04, 26.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:04, 25.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:04, 25.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:02<00:04, 25.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:02<00:04, 24.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:02<00:04, 24.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:02<00:04, 24.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:02<00:04, 24.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:02<00:04, 24.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:02<00:04, 24.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:02<00:04, 23.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:02<00:04, 23.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:02<00:04, 23.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:02<00:04, 23.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:02<00:04, 23.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:02<00:03, 23.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:02<00:03, 23.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:02<00:03, 22.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:03<00:04, 22.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:03<00:03, 22.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:03<00:03, 22.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:03<00:03, 22.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:03<00:03, 22.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:03<00:03, 21.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:03<00:03, 21.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:03<00:03, 21.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:03<00:03, 21.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:03<00:03, 21.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:03<00:03, 21.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:03<00:03, 20.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:03<00:03, 20.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:03<00:03, 20.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:03<00:03, 20.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:03<00:03, 20.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:03<00:03, 20.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:04<00:03, 20.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:04<00:03, 20.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:04<00:03, 20.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:04<00:03, 20.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:04<00:03, 20.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:04<00:03, 20.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:04<00:03, 19.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:04<00:03, 19.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:04<00:03, 19.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:04<00:03, 19.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:04<00:03, 19.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:04<00:03, 19.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:04<00:03, 19.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:04<00:03, 19.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:05<00:03, 19.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:05<00:02, 19.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:05<00:02, 19.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:05<00:02, 19.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:05<00:02, 18.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:05<00:02, 18.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:05<00:02, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:05<00:02, 18.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:05<00:02, 19.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:05<00:02, 18.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:05<00:02, 18.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:05<00:02, 18.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:05<00:02, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:05<00:02, 18.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:05<00:02, 18.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:05<00:02, 18.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:06<00:02, 18.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:06<00:02, 18.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:06<00:02, 18.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:06<00:02, 18.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:06<00:02, 18.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:06<00:02, 18.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:06<00:02, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:06<00:01, 18.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:06<00:01, 18.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:06<00:01, 18.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:06<00:01, 18.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:06<00:01, 18.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:06<00:01, 18.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:07<00:01, 18.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:07<00:01, 18.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:07<00:01, 18.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:07<00:01, 18.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:07<00:01, 18.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:07<00:01, 18.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:07<00:01, 17.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:07<00:01, 17.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:07<00:01, 18.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:07<00:01, 18.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:07<00:01, 18.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:07<00:01, 17.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:07<00:01, 17.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:07<00:00, 17.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:07<00:00, 17.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:07<00:00, 17.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:07<00:00, 17.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:08<00:00, 17.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:08<00:00, 17.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:08<00:00, 17.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:08<00:00, 17.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:08<00:00, 17.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:08<00:00, 17.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:08<00:00, 17.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:08<00:00, 17.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:08<00:00, 17.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:08<00:00, 17.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:08<00:00, 17.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:08<00:00, 17.35it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:09<00:00, 17.39it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:43<00:00, 17.99it/s, v_num=17, train_acc_step=0.125\u001b[AI0316 18:54:57.239567 140028686267456 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:43<00:00, 17.90it/s, v_num=17, train_acc_step=0.125\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_18/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_19/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_20/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBefore analyzing original graph: pass_args = {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': None}, 'weight': {'calibrator': 'histogram', 'quantize_axis': None}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}, 'task': 'cls', 'dataset': 'cifar10', 'batch_size': 64, 'model': 'resnet18', 'data_module': <chop.dataset.MaseDataModule object at 0x7f58cc4ae310>, 'accelerator': 'cuda', 'test': True, 'num_batches': 1, 'num_GPU_warmup_batches': 0}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.68146   |\n",
      "|      Average Precision       |   0.7399    |\n",
      "|        Average Recall        |   0.70312   |\n",
      "|       Average F1 Score       |   0.70984   |\n",
      "|         Average Loss         |   0.89538   |\n",
      "|       Average Latency        |  62.155 ms  |\n",
      "|   Average GPU Power Usage    |  13.255 W   |\n",
      "| Inference Energy Consumption | 0.22886 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_85/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/16/2025-18:56:20] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73938    |\n",
      "|      Average Precision       |   0.73979    |\n",
      "|        Average Recall        |   0.74207    |\n",
      "|       Average F1 Score       |   0.73999    |\n",
      "|         Average Loss         |   0.73969    |\n",
      "|       Average Latency        |  18.146 ms   |\n",
      "|   Average GPU Power Usage    |   16.787 W   |\n",
      "| Inference Energy Consumption | 0.084616 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/tensorrt/version_6/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet18_INT8_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 02:21:21.056977 140494406755392 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.087997 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088167 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088266 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088348 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088433 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088517 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088604 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088695 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088779 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088858 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.088949 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089029 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089111 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089190 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089273 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089351 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089435 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089513 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089596 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089676 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089760 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089838 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089920 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.089999 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090083 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090162 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090245 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090325 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090409 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090490 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090573 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090651 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090734 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090822 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090906 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.090986 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091068 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091146 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091232 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091306 140494406755392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091391 140494406755392 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 02:21:32.091465 140494406755392 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0315 02:21:32.091680 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0315 02:21:32.091738 140494406755392 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.091941 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092101 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092239 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092375 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092510 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092655 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8358 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092797 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.092946 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1359 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093083 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093218 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093343 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093477 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2610 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093605 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093736 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.093876 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094006 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5872 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094131 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094259 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2720 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094383 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094517 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094642 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094769 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6489 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.094891 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095019 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095145 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095278 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4016 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095404 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095533 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095656 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095781 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.095990 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096137 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096266 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096399 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096525 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096652 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.9013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096777 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.096904 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.097026 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.097127 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:32.097379 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72608    |\n",
      "|      Average Precision       |   0.71486    |\n",
      "|        Average Recall        |   0.71875    |\n",
      "|       Average F1 Score       |   0.71577    |\n",
      "|         Average Loss         |   0.81619    |\n",
      "|       Average Latency        |  29.604 ms   |\n",
      "|   Average GPU Power Usage    |   9.368 W    |\n",
      "| Inference Energy Consumption | 0.077035 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_10/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:21:38.361274 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.362315 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.362808 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4700 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363099 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363385 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363618 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.363879 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364109 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364330 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364542 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.364771 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365005 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365243 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365498 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.365726 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366071 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366341 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1995 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366575 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.366806 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367028 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367249 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367468 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.367899 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.368262 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.368669 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369010 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369326 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6513 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369634 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.369944 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1650 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.370243 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.370656 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.370944 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.371239 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.371532 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.371831 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.372119 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.372408 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.372699 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373008 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373312 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373595 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:38.373790 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72868    |\n",
      "|      Average Precision       |   0.71952    |\n",
      "|        Average Recall        |   0.72268    |\n",
      "|       Average F1 Score       |    0.7203    |\n",
      "|         Average Loss         |   0.80705    |\n",
      "|       Average Latency        |  27.485 ms   |\n",
      "|   Average GPU Power Usage    |   10.18 W    |\n",
      "| Inference Energy Consumption | 0.077725 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_11/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:21:44.078727 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0570 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.079337 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.079652 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8774 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.079928 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080175 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4204 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080457 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080696 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.080967 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081194 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081415 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081642 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.081875 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082105 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082331 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082558 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.082784 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083009 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083226 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083447 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083670 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.083903 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.084133 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.084362 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.084606 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.085184 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.085549 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.085789 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086019 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086479 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7914 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086699 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.086930 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087149 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087424 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1205 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087640 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.087871 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.088093 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.088314 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7121 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.088780 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089005 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089281 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089517 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:44.089663 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72872    |\n",
      "|      Average Precision       |   0.71967    |\n",
      "|        Average Recall        |   0.72279    |\n",
      "|       Average F1 Score       |   0.72034    |\n",
      "|         Average Loss         |   0.80904    |\n",
      "|       Average Latency        |  27.262 ms   |\n",
      "|   Average GPU Power Usage    |   10.725 W   |\n",
      "| Inference Energy Consumption | 0.081218 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_12/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:21:50.013751 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.193699 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.532325 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.707737 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:50.983930 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1439 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.152282 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.475319 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.654181 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:51.912652 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9089 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.092157 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.271842 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.456134 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.721934 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:52.898514 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.075565 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.263721 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.444887 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.6229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.630287 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:53.871474 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0186 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.046728 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.381313 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.556874 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:54.814447 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.001569 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.321210 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.501133 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.778420 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:55.960818 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.138005 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7037 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.309161 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.489939 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.672837 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:56.844730 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.204922 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.382050 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.563570 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.743386 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:57.922971 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.099598 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.280104 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.281308 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:21:58.281645 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72848    |\n",
      "|      Average Precision       |   0.71898    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |    0.7197    |\n",
      "|         Average Loss         |   0.80978    |\n",
      "|       Average Latency        |  28.014 ms   |\n",
      "|   Average GPU Power Usage    |   10.197 W   |\n",
      "| Inference Energy Consumption | 0.079347 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_13/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 02:22:06.211271 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:07.648026 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:10.083604 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4599 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:11.497441 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:13.300668 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.8759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:14.737156 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:17.501459 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1156 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:18.972856 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:20.775787 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5661 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:22.233590 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:23.680495 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:25.133930 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:27.128596 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:28.610648 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:30.076291 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:31.524113 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:32.976151 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:34.432886 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:35.951512 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.6632 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:37.443416 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:39.848795 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:41.291870 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:43.188360 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.0554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:44.606407 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:46.981904 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:48.373767 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:50.386144 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:51.804554 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:53.229105 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:54.592639 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:56.062018 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:57.493883 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:22:58.974630 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4003 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:00.391778 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:01.820893 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:03.290762 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:04.740563 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4923 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:06.225100 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:07.687951 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8677 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:09.174981 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:09.175845 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 02:23:09.176056 140494406755392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7293    |\n",
      "|      Average Precision       |   0.71953    |\n",
      "|        Average Recall        |   0.72299    |\n",
      "|       Average F1 Score       |   0.72043    |\n",
      "|         Average Loss         |   0.81129    |\n",
      "|       Average Latency        |  23.632 ms   |\n",
      "|   Average GPU Power Usage    |   13.855 W   |\n",
      "| Inference Energy Consumption | 0.090947 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_14/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0315 02:23:14.374530 140494406755392 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0315 02:23:14.374890 140494406755392 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0315 02:23:14.374947 140494406755392 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0315 02:23:16.972962 140494406755392 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0315 02:23:16.979569 140494406755392 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:29<00:00, 26.35it/s, v_num=13, train_acc_step=0.688\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 39.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 42.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 44.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 44.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 45.64it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 42.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 41.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 41.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 42.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 42.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 42.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 42.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 42.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 43.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 43.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 43.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 43.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 43.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 43.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 43.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 43.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 43.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 43.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 44.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 44.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 44.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 44.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 44.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 44.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 44.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 44.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 44.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 44.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 44.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 44.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 44.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:02, 44.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:02, 44.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 44.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 44.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 44.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 44.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 44.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 44.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 44.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 44.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 44.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 44.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 44.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 44.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 44.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 44.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 44.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 44.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 44.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 44.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 44.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 44.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 44.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 44.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 43.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 43.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 43.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 43.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 43.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 43.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 43.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 43.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 42.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 42.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 42.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 42.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 42.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 42.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 42.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 42.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 42.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 42.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 41.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 42.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 42.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 42.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 41.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 41.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 41.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 41.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 41.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 41.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 41.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 41.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 41.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 41.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 41.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 41.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 41.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 41.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 41.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 41.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 41.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:01, 41.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 41.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 41.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 41.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 41.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 40.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 40.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 40.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 40.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 40.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 40.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 40.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 40.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 40.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 40.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 40.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 40.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 40.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 40.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:03<00:00, 40.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:03<00:00, 40.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:03<00:00, 40.58it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:03<00:00, 40.59it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:31<00:00, 25.16it/s, v_num=13, train_acc_step=0.125\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 41.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 45.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 47.20it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 44.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 45.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 43.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 44.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 44.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 43.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 43.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 43.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 44.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 44.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 43.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 43.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 43.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:03, 43.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:03, 43.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:03, 43.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:03, 43.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:03, 43.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:03, 43.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 44.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 43.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 44.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 44.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 44.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 43.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 43.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 42.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 42.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 41.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 41.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 42.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 42.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 42.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 42.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 42.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 42.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 42.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 42.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:02, 41.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:02, 41.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:02, 41.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:02, 41.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:02, 41.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:02, 41.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:02, 41.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:02, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:02, 41.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:02, 41.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:02, 41.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:02, 41.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:02, 41.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:02, 41.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 41.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 41.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 41.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 41.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 41.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 40.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 40.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 40.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:02, 40.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:02, 40.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:02, 40.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:02, 40.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:02, 40.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:02, 40.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:02, 40.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:02, 40.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:02, 40.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:02, 40.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:02, 40.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 40.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 40.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 40.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:01, 39.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:01, 39.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:01, 39.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:01, 39.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:01, 39.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:01, 39.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:01, 39.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:01, 39.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:01, 39.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:01, 39.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:01, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:01, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:01, 39.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:02<00:01, 39.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:02<00:01, 39.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:02<00:01, 39.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:02<00:01, 39.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:02<00:01, 39.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:02<00:01, 39.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:02<00:01, 39.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:02<00:01, 39.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:02<00:01, 39.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:02<00:01, 39.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:02<00:01, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:02<00:01, 39.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:02<00:01, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:02<00:01, 38.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:02<00:01, 38.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:02<00:01, 38.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:02<00:01, 38.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:02<00:01, 38.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:02<00:01, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:02<00:01, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:02<00:01, 38.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:01, 38.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:01, 38.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 38.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 38.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:03<00:01, 38.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:03<00:00, 38.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:03<00:00, 38.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:03<00:00, 38.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:03<00:00, 38.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:03<00:00, 38.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:03<00:00, 38.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:03<00:00, 38.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:03<00:00, 38.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:03<00:00, 38.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:03<00:00, 38.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:03<00:00, 38.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:03<00:00, 38.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:03<00:00, 38.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:03<00:00, 38.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:03<00:00, 38.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:03<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:03<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:03<00:00, 38.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:03<00:00, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:03<00:00, 38.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:03<00:00, 38.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:03<00:00, 38.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:03<00:00, 38.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:03<00:00, 38.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:03<00:00, 38.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:03<00:00, 38.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:03<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:04<00:00, 38.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:04<00:00, 38.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:04<00:00, 38.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:04<00:00, 38.34it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:35<00:00, 22.01it/s, v_num=13, train_acc_step=0.125\u001b[AI0315 02:24:27.561993 140494406755392 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:35<00:00, 21.88it/s, v_num=13, train_acc_step=0.125\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_17/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_18/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_19/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.68146   |\n",
      "|      Average Precision       |   0.7399    |\n",
      "|        Average Recall        |   0.70312   |\n",
      "|       Average F1 Score       |   0.70984   |\n",
      "|         Average Loss         |    0.895    |\n",
      "|       Average Latency        |  48.939 ms  |\n",
      "|   Average GPU Power Usage    |  8.1492 W   |\n",
      "| Inference Energy Consumption | 0.11078 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_15/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-02:27:22] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73975    |\n",
      "|      Average Precision       |   0.74038    |\n",
      "|        Average Recall        |   0.74242    |\n",
      "|       Average F1 Score       |   0.74044    |\n",
      "|         Average Loss         |   0.73944    |\n",
      "|       Average Latency        |   3.601 ms   |\n",
      "|   Average GPU Power Usage    |   13.729 W   |\n",
      "| Inference Energy Consumption | 0.013733 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet18_INT8_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 01:23:45.774668 139703610053696 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922260 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922491 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922657 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922785 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.922956 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923104 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923228 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923357 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923478 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923589 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923706 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923818 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.923936 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924046 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924172 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924284 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924398 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924507 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924622 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924733 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924848 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.924958 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925071 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925192 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925306 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925416 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925532 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925656 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925778 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.925884 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926000 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926108 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926219 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926326 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926456 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926565 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926676 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926784 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.926894 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.927001 139703610053696 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.927117 139703610053696 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0315 01:23:58.927211 139703610053696 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0315 01:23:58.927518 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0315 01:23:58.927609 139703610053696 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.927929 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1400 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928180 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928388 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928645 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.928863 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929070 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8358 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929276 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1957 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929511 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1359 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929744 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1970 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.929947 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930136 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930328 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2610 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930509 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930694 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4627 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.930871 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931056 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5872 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931236 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931422 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2720 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931605 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931797 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.931978 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932162 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6489 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932338 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932523 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932701 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.932884 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4016 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933084 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933270 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933448 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0944 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933627 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933794 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0710 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.933966 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.5874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934165 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934376 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.2608 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934560 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934740 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.9013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.934911 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935086 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935261 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935403 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:23:58.935742 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72608   |\n",
      "|      Average Precision       |   0.71486   |\n",
      "|        Average Recall        |   0.71875   |\n",
      "|       Average F1 Score       |   0.71577   |\n",
      "|         Average Loss         |   0.81619   |\n",
      "|       Average Latency        |  34.633 ms  |\n",
      "|   Average GPU Power Usage    |  13.806 W   |\n",
      "| Inference Energy Consumption | 0.13282 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_0/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:06.038734 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.039483 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1815 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.039825 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4700 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040095 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2904 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040351 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040592 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.040854 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041092 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041334 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041567 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.041802 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042031 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042269 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042513 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2041 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.042753 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=7.9494 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043049 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043301 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1995 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043533 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2007 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.043810 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3943 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.044165 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.044557 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.044898 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.045269 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.045599 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.045931 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.7997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.046232 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3283 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.046531 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.6513 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.046845 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047185 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1650 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047479 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047739 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.047980 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.048222 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.048454 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.048927 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.4910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.049339 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.049886 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.050177 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.050515 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.050834 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0719 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.051083 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:06.051247 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72868    |\n",
      "|      Average Precision       |   0.71952    |\n",
      "|        Average Recall        |   0.72268    |\n",
      "|       Average F1 Score       |    0.7203    |\n",
      "|         Average Loss         |   0.80705    |\n",
      "|       Average Latency        |  28.964 ms   |\n",
      "|   Average GPU Power Usage    |   10.53 W    |\n",
      "| Inference Energy Consumption | 0.084719 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_1/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:11.926549 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0570 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.927209 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2071 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.927555 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8774 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.927806 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928054 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4204 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928285 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2896 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928526 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.6484 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.928769 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929061 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929307 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2948 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929539 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.929760 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930126 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930378 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930610 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.5421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.930837 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.931249 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.931474 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2315 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932123 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932453 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2258 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932724 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.932990 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933232 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933460 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2141 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933697 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.933957 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934198 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7517 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934422 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934650 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7914 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.934872 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935144 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935390 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935625 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1205 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.935845 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936084 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.0269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936304 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2988 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936535 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7121 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.936811 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937063 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937299 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0867 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937504 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:11.937657 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.72872    |\n",
      "|      Average Precision       |   0.71967    |\n",
      "|        Average Recall        |   0.72279    |\n",
      "|       Average F1 Score       |   0.72034    |\n",
      "|         Average Loss         |   0.80904    |\n",
      "|       Average Latency        |  26.359 ms   |\n",
      "|   Average GPU Power Usage    |   10.624 W   |\n",
      "| Inference Energy Consumption | 0.077789 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_2/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:17.712724 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:17.894579 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.251268 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.431818 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.694564 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1439 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:18.881787 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.219688 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.399970 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2902 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.650830 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9089 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:19.819225 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.001526 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.172828 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.438764 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.627349 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:20.827448 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.9078 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.013975 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.211982 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.6229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.391825 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2328 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.612207 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0186 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:21.784920 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.150280 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.329423 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2018 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.599805 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:22.787929 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.105024 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.295989 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4561 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.579686 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.766986 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:23.944687 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7037 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.128643 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.316481 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.499539 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1536 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:24.684113 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1098 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.054862 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.232592 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.412049 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3027 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.592864 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2634 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.771907 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0888 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:25.950548 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:26.129124 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:26.130245 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:26.130683 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72848   |\n",
      "|      Average Precision       |   0.71898   |\n",
      "|        Average Recall        |   0.72227   |\n",
      "|       Average F1 Score       |   0.7197    |\n",
      "|         Average Loss         |   0.80978   |\n",
      "|       Average Latency        |  33.931 ms  |\n",
      "|   Average GPU Power Usage    |  9.4373 W   |\n",
      "| Inference Energy Consumption | 0.08895 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0315 01:24:35.326884 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:36.899141 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1926 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:39.525205 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4599 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:41.033430 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3665 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:42.986601 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.8759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:44.495841 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:47.389332 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1156 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:48.954459 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:50.742857 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5661 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:52.182403 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:53.631896 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:55.090106 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2664 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:57.066362 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:24:58.536308 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2533 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:00.002814 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0887 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:01.434386 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:02.874977 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:04.346352 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:05.848518 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.6632 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:07.320424 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:09.730585 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:11.231554 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:13.152189 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.0554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:14.590133 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:17.024239 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.1341 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:18.439450 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:20.490301 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:21.951633 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:23.403716 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:24.795568 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:26.234439 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:27.662348 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1693 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:29.139847 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4003 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:30.580707 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:32.044255 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.0340 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:33.526308 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:34.972633 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4923 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:36.432395 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:37.897637 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8677 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:39.397825 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1051 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:39.398750 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=19.6309 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0315 01:25:39.399023 139703610053696 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0832, 0.1520](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7293    |\n",
      "|      Average Precision       |   0.71953    |\n",
      "|        Average Recall        |   0.72299    |\n",
      "|       Average F1 Score       |   0.72043    |\n",
      "|         Average Loss         |   0.81129    |\n",
      "|       Average Latency        |  26.884 ms   |\n",
      "|   Average GPU Power Usage    |   10.213 W   |\n",
      "| Inference Energy Consumption | 0.076263 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_4/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0315 01:25:45.103971 139703610053696 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0315 01:25:45.104289 139703610053696 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0315 01:25:45.104348 139703610053696 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0315 01:25:47.751724 139703610053696 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0315 01:25:47.763391 139703610053696 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:31<00:00, 24.52it/s, v_num=12, train_acc_step=0.688\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:04, 31.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:04, 32.68it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 34.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 34.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 32.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 33.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:04, 33.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:04, 32.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:04, 32.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:04, 32.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:04, 32.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 31.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:04, 31.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:04, 31.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:04, 31.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:04, 31.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:04, 31.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:04, 31.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:04, 30.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:04, 30.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:04, 30.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:04, 31.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:04, 30.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:04, 30.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:04, 30.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:04, 30.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:04, 29.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:04, 29.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:04, 29.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:01<00:04, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:01<00:04, 29.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:01<00:04, 29.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:01<00:04, 29.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:01<00:04, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:04, 29.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:04, 29.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:04, 28.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:04, 28.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:01<00:04, 28.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:01<00:04, 28.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:04, 28.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:04, 28.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:03, 28.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:03, 28.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:03, 28.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:03, 28.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:03, 28.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:03, 28.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:03, 28.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:03, 28.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:03, 28.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:03, 28.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:03, 29.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:03, 29.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:03, 29.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:03, 29.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:03, 29.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:03, 29.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:02<00:03, 29.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:02<00:03, 29.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:02<00:03, 29.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:02<00:03, 29.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:02<00:03, 29.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:02<00:03, 28.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:02<00:03, 28.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:02<00:03, 28.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:02<00:03, 28.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:02<00:03, 28.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:02<00:03, 28.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:02<00:02, 29.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:02<00:02, 29.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:02<00:02, 29.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:02<00:02, 29.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:02<00:02, 29.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:02<00:02, 29.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:02<00:02, 29.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:02<00:02, 29.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:02<00:02, 29.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:02<00:02, 29.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:02, 29.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:02, 29.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:02, 29.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:02, 29.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:02, 29.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:02, 29.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:02, 29.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:03<00:02, 29.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:03<00:02, 29.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:03<00:02, 29.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:03<00:02, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:03<00:02, 29.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:03<00:02, 29.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:03<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:03<00:02, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:03<00:02, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:03<00:02, 29.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:03<00:01, 29.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:03<00:01, 29.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:03<00:01, 29.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:03<00:01, 29.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:03<00:01, 29.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:03<00:01, 29.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:03<00:01, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:03<00:01, 29.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:03<00:01, 29.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:03<00:01, 29.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:03<00:01, 29.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:03<00:01, 29.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:03<00:01, 29.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:03<00:01, 29.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:03<00:01, 29.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:03<00:01, 29.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:03<00:01, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 29.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 29.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:04<00:01, 29.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:04<00:01, 29.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:04<00:01, 29.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:04<00:01, 29.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:04<00:01, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:04<00:01, 29.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:04<00:01, 29.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:04<00:01, 29.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:04<00:01, 29.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:04<00:01, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:04<00:00, 29.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:04<00:00, 29.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:04<00:00, 29.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:04<00:00, 29.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:04<00:00, 29.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:04<00:00, 29.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:04<00:00, 29.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:04<00:00, 29.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:04<00:00, 29.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:04<00:00, 29.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:04<00:00, 29.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:04<00:00, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:04<00:00, 29.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:04<00:00, 29.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:04<00:00, 29.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:04<00:00, 29.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:04<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:05<00:00, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:05<00:00, 29.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:05<00:00, 29.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:05<00:00, 29.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:05<00:00, 29.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:05<00:00, 29.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:05<00:00, 29.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:05<00:00, 29.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:05<00:00, 29.59it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:05<00:00, 29.63it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:37<00:00, 20.61it/s, v_num=12, train_acc_step=0.125\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:04, 34.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:05, 27.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 28.20it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 31.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 32.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 32.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:04, 33.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:04, 34.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:04, 34.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:04, 33.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:04, 33.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:04, 33.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:04, 33.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:04, 33.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:04, 32.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:04, 32.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:04, 32.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:04, 32.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:04, 33.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:04, 33.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:04, 33.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:04, 33.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:04, 33.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:03, 33.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:03, 33.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:03, 33.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:03, 33.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:03, 33.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:03, 33.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:03, 33.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:03, 33.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:03, 33.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:03, 34.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:03, 34.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:03, 34.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:03, 33.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:03, 33.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:03, 34.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:01<00:03, 34.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:01<00:03, 33.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:01<00:03, 34.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:01<00:03, 34.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:01<00:03, 34.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:01<00:03, 34.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:01<00:03, 34.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:01<00:03, 34.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:01<00:03, 34.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:01<00:03, 34.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:01<00:03, 33.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:01<00:03, 33.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:01<00:03, 33.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:01<00:03, 33.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:01<00:03, 33.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:03, 33.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:03, 33.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:03, 33.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:02, 33.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:02, 33.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:02, 33.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:02, 32.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:02, 32.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:02, 32.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:02, 32.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:02, 32.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:02<00:02, 32.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:02<00:02, 32.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:02<00:02, 32.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:02<00:02, 32.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:02<00:02, 32.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:02<00:02, 32.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:02<00:02, 32.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:02<00:02, 31.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:02<00:02, 31.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:02<00:02, 31.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:02<00:02, 31.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:02<00:02, 31.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:02<00:02, 31.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:02<00:02, 31.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:02<00:02, 31.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:02<00:02, 31.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:02<00:02, 31.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:02<00:02, 31.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:02<00:02, 31.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:02<00:02, 31.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:02<00:02, 31.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:02<00:02, 31.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:02<00:02, 31.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:02<00:02, 31.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:02<00:02, 31.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:02<00:02, 31.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:02<00:02, 31.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:02<00:02, 30.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:03<00:02, 30.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:03<00:02, 31.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:03<00:02, 30.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:03<00:01, 30.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:03<00:01, 30.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:03<00:01, 30.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:03<00:01, 30.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:03<00:01, 30.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:03<00:01, 30.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:03<00:01, 30.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:03<00:01, 30.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:03<00:01, 30.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:03<00:01, 30.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:03<00:01, 30.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:03<00:01, 30.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:03<00:01, 30.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:03<00:01, 30.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:03<00:01, 30.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:03<00:01, 30.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:03<00:01, 30.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:03<00:01, 30.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:03<00:01, 29.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:03<00:01, 29.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:03<00:01, 29.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:03<00:01, 29.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:03<00:01, 29.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:03<00:01, 30.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:04<00:01, 29.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:04<00:01, 29.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:04<00:01, 29.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:04<00:01, 29.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:04<00:01, 29.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:04<00:01, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:04<00:01, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:04<00:01, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:04<00:00, 29.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:04<00:00, 29.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:04<00:00, 29.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:04<00:00, 29.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:04<00:00, 29.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:04<00:00, 29.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:04<00:00, 29.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:04<00:00, 29.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:04<00:00, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:04<00:00, 29.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:04<00:00, 29.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:04<00:00, 29.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:04<00:00, 29.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:04<00:00, 29.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:04<00:00, 29.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:04<00:00, 29.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:04<00:00, 29.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:05<00:00, 29.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:05<00:00, 29.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:05<00:00, 29.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:05<00:00, 29.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:05<00:00, 29.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:05<00:00, 29.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:05<00:00, 29.91it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:05<00:00, 29.89it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:43<00:00, 17.95it/s, v_num=12, train_acc_step=0.125\u001b[AI0315 01:27:10.410279 139703610053696 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [00:43<00:00, 17.84it/s, v_num=12, train_acc_step=0.125\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_1/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_2/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73031   |\n",
      "|      Average Precision       |   0.72099   |\n",
      "|        Average Recall        |   0.72423   |\n",
      "|       Average F1 Score       |   0.7218    |\n",
      "|         Average Loss         |   0.81107   |\n",
      "|       Average Latency        |  42.757 ms  |\n",
      "|   Average GPU Power Usage    |  9.3992 W   |\n",
      "| Inference Energy Consumption | 0.11163 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-01:30:18] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.74089    |\n",
      "|      Average Precision       |   0.74216    |\n",
      "|        Average Recall        |   0.74431    |\n",
      "|       Average F1 Score       |   0.74236    |\n",
      "|         Average Loss         |    0.7364    |\n",
      "|       Average Latency        |  4.4764 ms   |\n",
      "|   Average GPU Power Usage    |   12.884 W   |\n",
      "| Inference Energy Consumption | 0.016021 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_0/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_INT8_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 Conversion with TensorRT\n",
    "\n",
    "#### Overview\n",
    "This section describes the process of converting a model to **FP16 precision** using TensorRT. Unlike **INT8 quantization**, **FP16 does not require calibration, fake quantization, or fine-tuning**. The conversion process is simpler and primarily focuses on **speeding up inference while maintaining high precision**.\n",
    "\n",
    "### Code Execution Flow\n",
    "\n",
    "1. **Apply TensorRT FP16 Pass**\n",
    "   - **No Fake Quantization**: Since FP16 does not require quantization-aware training, the `quantize` option is set to `false`.\n",
    "   - **No Calibration**: Unlike INT8, FP16 does not need calibration data, so `num_calibration_batches` is set to `0`.\n",
    "   - **No Fine-Tuning**: Additional training is unnecessary in FP16 mode.\n",
    "\n",
    "2. **Generate the TensorRT Engine**\n",
    "   - Calls `tensorrt_engine_interface_pass` to convert the model to a **TensorRT FP16 engine**.\n",
    "\n",
    "3. **Benchmarking & Performance Analysis**\n",
    "   - Runs inference tests with warm-up and batch evaluation to measure efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 18:46:58.871801 140478726022208 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mprecision=fp16, skipping int8 calibration/fine-tune...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_14/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_15/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/2025-03-16/version_16/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBefore analyzing original graph: pass_args = {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}, 'task': 'cls', 'dataset': 'cifar10', 'batch_size': 64, 'model': 'resnet18', 'data_module': <chop.dataset.MaseDataModule object at 0x7fc1952c9c90>, 'accelerator': 'cuda', 'num_GPU_warmup_batches': 0, 'num_batches': 1, 'test': True}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.68146    |\n",
      "|      Average Precision       |   0.72305    |\n",
      "|        Average Recall        |   0.70312    |\n",
      "|       Average F1 Score       |    0.7063    |\n",
      "|         Average Loss         |   0.88661    |\n",
      "|       Average Latency        |  13.428 ms   |\n",
      "|   Average GPU Power Usage    |   8.974 W    |\n",
      "| Inference Energy Consumption | 0.033473 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/mase_graph/version_79/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/16/2025-18:48:06] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+---------------+\n",
      "|      Metric (Per Batch)      |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.74016    |\n",
      "|      Average Precision       |    0.73992    |\n",
      "|        Average Recall        |    0.74276    |\n",
      "|       Average F1 Score       |    0.74024    |\n",
      "|         Average Loss         |    0.73782    |\n",
      "|       Average Latency        |   2.6209 ms   |\n",
      "|   Average GPU Power Usage    |   7.9719 W    |\n",
      "| Inference Energy Consumption | 0.0058037 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-16/tensorrt/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_FP16_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_FP16_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_FP16_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 Conversion with TensorRT\n",
    "\n",
    "The process for converting a model to **FP32 precision** using TensorRT is quite similar to the **FP16 conversion**, but with even fewer modifications. Since FP32 is the default precision for deep learning models, the main goal here is to **leverage TensorRT optimizations** without changing the numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 01:43:15.535419 140335826768960 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet18_cls_ci | e_output/resnet18_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-08/softwar | far10_2025-03-08/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp32'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mprecision=fp32, skipping int8 calibration/fine-tune...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_13/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_14/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/2025-03-15/version_15/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72852    |\n",
      "|      Average Precision       |   0.71886    |\n",
      "|        Average Recall        |   0.72227    |\n",
      "|       Average F1 Score       |   0.71961    |\n",
      "|         Average Loss         |   0.80975    |\n",
      "|       Average Latency        |  8.3068 ms   |\n",
      "|   Average GPU Power Usage    |   17.661 W   |\n",
      "| Inference Energy Consumption | 0.040752 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/mase_graph/version_9/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/15/2025-01:43:32] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+---------------+\n",
      "|      Metric (Per Batch)      |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.74162    |\n",
      "|      Average Precision       |    0.74235    |\n",
      "|        Average Recall        |    0.74514    |\n",
      "|       Average F1 Score       |    0.7427     |\n",
      "|         Average Loss         |    0.73414    |\n",
      "|       Average Latency        |   2.706 ms    |\n",
      "|   Average GPU Power Usage    |   11.678 W    |\n",
      "| Inference Energy Consumption | 0.0087779 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-15/tensorrt/version_4/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-15/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_FP32_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/tensorrt/resnet18_FP32_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_FP32_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on RTX 4060 with ResNet18 and CIFAR-10\n",
    "\n",
    "The following results were obtained while running **ResNet18 on CIFAR-10** using **TensorRT** on an **RTX 4060 GPU**. \n",
    "\n",
    "#### **Accuracy Comparison**\n",
    "| Model Version   | Accuracy |\n",
    "|----------------|----------|\n",
    "| **Original (FP32)**  | **0.73**  |\n",
    "| **After Quantization**  | **0.74**  |\n",
    "\n",
    "- The slight accuracy **increase** after quantization is attributed to **QAT**.\n",
    "\n",
    "#### **Latency Reduction**\n",
    "| Precision Mode | Initial Latency | Optimized Latency |\n",
    "|---------------|----------------|------------------|\n",
    "| **FP32**      | 8.3ms            | **2.7ms**        |\n",
    "| **FP16**      | 8.1ms            | **1.0ms**        |\n",
    "| **INT8**      | **42.8ms**       | **4.5ms**        |\n",
    "\n",
    "- **FP32 to FP16** significantly reduces latency, bringing it down to **1.0ms**.\n",
    "- **INT8 inference** achieves **4.5ms latency**, but the initial unoptimized latency was **42.8ms**, 这很奇怪，怀疑是fake quantize操作.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
