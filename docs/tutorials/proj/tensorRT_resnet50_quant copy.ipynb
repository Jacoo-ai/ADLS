{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Quantization Tutorial\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE as part of the MASERT framework. The following demonstrations were run on a NVIDIA RTX A2000 GPU with a Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz CPU.\n",
    "\n",
    "## Section 1. Show Configuration\n",
    "Firstly, we will show you how to do a int8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each using their own individual pass, and are explained in depth at each subsection:\n",
    "\n",
    "1. [Fake quantization](#section-11-fake-quantization): `tensorrt_fake_quantize_transform_pass`\n",
    "2. [Calibration](#section-12-calibration): `tensorrt_calibrate_transform_pass`\n",
    "3. [Quantized Aware Training](#section-13-quantized-aware-training-qat): `tensorrt_fine_tune_transform_pass`\n",
    "4. [Quantization](#section-14-tensorrt-quantization): `tensorrt_engine_interface_pass`\n",
    "5. [Analysis](#section-15-performance-analysis): `tensorrt_analysis_pass`\n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"src\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.utils import to_numpy_if_tensor\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.tools import get_cf_args, get_dummy_input\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model, get_tokenizer\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
    "from chop.passes.graph import (\n",
    "    summarize_quantization_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    runtime_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dependency (the dependent package \"cuda\" refers to \"cuda-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mExtension: All dependencies for TensorRT pass are available.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chop.tools.check_dependency import check_deps_tensorRT_pass\n",
    "check_deps_tensorRT_pass(silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}\n",
      "{'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "# Path to your TOML file\n",
    "# JSC_TOML_PATH = 'toy_INT8_quantization_by_type.toml'\n",
    "JSC_TOML_PATH = 'resnet50_INT8_quant.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(JSC_TOML_PATH, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt' section and its children\n",
    "tensorrt_config = pass_args.get('passes', {}).get('tensorrt', {})\n",
    "print(tensorrt_config)\n",
    "# Extract the 'passes.runtime_analysis' section and its children\n",
    "runtime_analysis_config = pass_args.get('passes', {}).get('tensorrt', {}).get('runtime_analysis', {})\n",
    "print(runtime_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a model and training it using the toml configuration model arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n"
     ]
    }
   ],
   "source": [
    "from chop.dataset import MaseDataModule\n",
    "from chop.models import get_model_info\n",
    "from chop.models import get_model\n",
    "from chop.tools.get_input import InputGenerator\n",
    "\n",
    "# Load the basics in\n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Add the data_module and other necessary information to the configs\n",
    "configs = [tensorrt_config, runtime_analysis_config]\n",
    "for config in configs:\n",
    "    config['task'] = pass_args['task']\n",
    "    config['dataset'] = pass_args['dataset']\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    # task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 18:20:15.102040 140174726280256 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File | Manual Override |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                 |           cls            |\n",
      "| load_name               |           None           |              |                 |           None           |\n",
      "| load_type               |            mz            |              |                 |            mz            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                 |            64            |\n",
      "| to_debug                |          False           |              |                 |          False           |\n",
      "| log_level               |           info           |              |                 |           info           |\n",
      "| report_to               |       tensorboard        |              |                 |       tensorboard        |\n",
      "| seed                    |            0             |              |                 |            0             |\n",
      "| quant_config            |           None           |              |                 |           None           |\n",
      "| training_optimizer      |           adam           |              |                 |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                 |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                 |          0.001           |\n",
      "| weight_decay            |            0             |              |                 |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                 |            10            |\n",
      "| max_steps               |            -1            |              |                 |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                 |            1             |\n",
      "| log_every_n_steps       |            50            |              |                 |            50            |\n",
      "| num_workers             |            20            |              |                 |            20            |\n",
      "| num_devices             |            1             |              |                 |            1             |\n",
      "| num_nodes               |            1             |              |                 |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                 |           gpu            |\n",
      "| strategy                |           auto           |              |                 |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                 |          False           |\n",
      "| github_ci               |          False           |              |                 |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                 |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                 |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                 |           100            |\n",
      "| is_pretrained           |          False           |              |                 |          False           |\n",
      "| max_token_len           |           512            |              |                 |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                 | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                 |         e_output         |\n",
      "| project                 |           None           |              |                 |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet50   |                 |         resnet50         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                 |         cifar10          |\n",
      "| t_max                   |            20            |              |                 |            20            |\n",
      "| eta_min                 |          1e-06           |              |                 |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+-----------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet50'...\u001b[0m\n",
      "self.args.model is resnet50\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining model 'resnet50'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m##### WEIGHT DECAY ##### 0\u001b[0m\n",
      "I0315 18:20:15.352615 140174726280256 rank_zero.py:63] Using 16bit Automatic Mixed Precision (AMP)\n",
      "I0315 18:20:15.362067 140174726280256 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0315 18:20:15.362511 140174726280256 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0315 18:20:15.362582 140174726280256 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0315 18:20:18.464544 140174726280256 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0315 18:20:18.885035 140174726280256 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 23.5 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [00:33<00:00, 23.02it/s, v_num=0, train_acc_step=0.312]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 55.55it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 65.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 67.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 73.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 58.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 58.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 62.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 65.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 64.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 58.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 61.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 61.68it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 62.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 63.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 63.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 61.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 58.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 58.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 59.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 59.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 60.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 61.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 62.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 62.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 63.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 63.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 64.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 64.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 63.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 64.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 64.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 63.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 63.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 63.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 63.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 64.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 64.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 65.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 65.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 66.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 66.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 65.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 65.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 66.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 66.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 66.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 67.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 67.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 67.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 67.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 67.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 67.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 67.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 67.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 67.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 67.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 67.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 66.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 66.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 66.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 66.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 66.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 65.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 64.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 64.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 64.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 64.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 64.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 64.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 63.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 63.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 63.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 63.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 63.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 63.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 63.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 63.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 63.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 63.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 63.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 63.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 63.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 63.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 63.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 63.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 63.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 63.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 63.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 63.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 63.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 63.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 63.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 63.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 63.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 62.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 63.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 63.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 63.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 62.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 62.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 62.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 62.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 62.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 61.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 61.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 61.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 61.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 61.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 61.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 60.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 60.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 60.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 60.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 60.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 60.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 60.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 60.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 60.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 60.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 60.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 60.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 60.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 60.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 60.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 60.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 60.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 60.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 60.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 60.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 60.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 60.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 61.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 61.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 61.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 61.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 61.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 61.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 60.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 60.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 60.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 60.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 60.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 61.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 61.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 61.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 60.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 61.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 60.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 60.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 61.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 61.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 61.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 61.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 61.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 61.23it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 61.36it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:36<00:00, 21.48it/s, v_num=0, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 51.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 65.87it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 54.37it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 61.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 66.73it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 58.87it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 62.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 60.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 63.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 65.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 67.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 68.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 69.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 70.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:01, 71.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:01, 71.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 73.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 71.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 70.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 67.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 67.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 68.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 69.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 69.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 69.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 69.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 69.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 70.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 70.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 71.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 71.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 72.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 73.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 73.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 73.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 73.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 74.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 74.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 73.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 73.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 74.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 74.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 74.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 74.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 73.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 72.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 71.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 71.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 71.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 70.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 69.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 70.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 70.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 70.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 70.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 69.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 69.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 70.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 69.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 69.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 69.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 69.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 69.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 70.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 70.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 70.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 69.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 70.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 70.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 70.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 70.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 70.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 70.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 70.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 70.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 70.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 70.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 69.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 69.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 69.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 69.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 69.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 69.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 69.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 69.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 69.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 68.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 69.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 68.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 68.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 68.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 69.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 69.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 68.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 67.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 67.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 67.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 67.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 67.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 67.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 67.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 67.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 67.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 67.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 67.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 67.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 67.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 67.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 67.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 67.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 67.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 67.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 67.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 67.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 67.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 66.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 66.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 66.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 66.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 67.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 66.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 66.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 67.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 67.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 67.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 67.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 67.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 67.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 67.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 66.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 66.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 66.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 66.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 66.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 66.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 66.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 66.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 66.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 66.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 66.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 66.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 66.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 66.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 66.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 66.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 66.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 66.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 67.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 67.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 67.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 67.25it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 67.16it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 782/782 [00:36<00:00, 21.55it/s, v_num=0, train_acc_step=0.125,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 46.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 59.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 64.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 65.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 62.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 65.22it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 67.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 62.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 64.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 65.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 66.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 67.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 68.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 69.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 68.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 69.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 70.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 69.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 69.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 65.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 65.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 65.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 66.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 66.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 65.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 66.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 66.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 66.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 66.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 64.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 63.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 63.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 63.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 63.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 63.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 63.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 64.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 64.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 64.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 65.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 65.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 65.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 65.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 66.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 66.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 65.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 65.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 66.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 65.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 65.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 66.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 66.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 66.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 66.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 65.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 65.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 66.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 66.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 65.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 65.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 65.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 65.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 65.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 65.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 65.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 65.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 65.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 65.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 65.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 65.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 65.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 65.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 65.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 65.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 65.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 65.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 65.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 65.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 65.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 66.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 66.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 66.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 66.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 66.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 66.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 66.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 65.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 65.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 66.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 66.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 66.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 66.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 66.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 66.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 66.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 66.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 66.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 66.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 67.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 67.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 67.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 67.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 67.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 67.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 67.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 67.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 67.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 67.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 67.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 68.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 67.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 68.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 68.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 68.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 68.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 68.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 68.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 68.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 68.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 68.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 68.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 68.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 68.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 68.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 68.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 68.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 68.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 68.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 68.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 68.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 68.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 68.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 68.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 68.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 68.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 68.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 68.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 68.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 67.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 67.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 67.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 68.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 67.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 67.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 67.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 67.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 67.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 68.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 68.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 68.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 68.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 68.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 68.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 68.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 68.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 68.08it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 67.90it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 782/782 [00:36<00:00, 21.16it/s, v_num=0, train_acc_step=0.125,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 59.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 40.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 48.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 55.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 60.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 64.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 65.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 66.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 68.62it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 70.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 72.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:01, 73.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:01, 74.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:01, 75.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:01, 74.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:01, 72.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 72.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 73.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 73.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 72.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 71.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 70.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 70.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 71.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 71.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 71.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 71.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 72.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 72.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 72.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 73.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 73.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 73.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 73.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 73.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 74.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 74.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 73.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 74.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 74.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 74.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 74.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 74.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 74.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 74.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 74.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 74.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 74.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 74.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 74.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 74.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 74.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 74.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 74.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 74.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 74.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 74.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 73.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 73.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 73.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 74.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 73.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 73.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 73.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 73.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 73.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 73.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 73.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 72.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 72.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 73.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 72.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 72.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 71.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 71.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 72.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 72.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 72.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 72.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 71.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 71.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 72.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 71.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 71.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 71.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 71.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 72.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 72.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 72.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 72.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 72.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 72.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 72.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 71.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 71.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 70.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 70.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 70.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 70.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 70.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 70.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 70.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 70.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 70.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 70.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 70.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 70.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 70.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 70.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 70.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 70.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 70.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 70.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 70.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 70.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 70.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 70.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 70.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 70.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 70.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 70.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 70.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 70.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 70.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 70.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 70.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 70.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 70.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 70.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 70.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 70.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 70.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 70.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 70.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 70.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 70.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 70.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 70.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 70.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 70.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 70.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 70.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 70.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 70.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 70.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 70.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 70.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 71.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 71.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 71.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 71.11it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 782/782 [00:37<00:00, 21.06it/s, v_num=0, train_acc_step=0.0625\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 54.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 47.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 55.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 59.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 61.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 63.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 61.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 64.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 66.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 67.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 69.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 68.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 69.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 64.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 62.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 60.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 61.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 62.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 62.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 60.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 60.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 60.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 61.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 61.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 61.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 62.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 62.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 63.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 62.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 63.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 63.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 63.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 63.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 63.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 63.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 63.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 63.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 63.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 64.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 64.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 64.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 64.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 64.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 64.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 64.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 64.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 65.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 65.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 65.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 65.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 64.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 65.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 65.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 65.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 65.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 65.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 65.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 65.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 65.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 65.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 65.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 64.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 64.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 64.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 64.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 64.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 65.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 64.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 65.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 64.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 64.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 64.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 63.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 63.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 63.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 63.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 63.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 63.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 63.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 63.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 63.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 63.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 63.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 63.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 63.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 63.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 63.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 63.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 63.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 63.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 64.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 63.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 63.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 63.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 62.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 62.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 62.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 62.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 62.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 62.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 62.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 62.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 62.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 62.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 62.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 62.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 62.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 62.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 62.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 62.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 62.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 62.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 62.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 62.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 62.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 62.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 62.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 62.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 62.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 62.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 63.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 63.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 63.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 63.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 63.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 63.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 63.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 63.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 62.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 62.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 62.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 62.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 62.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 62.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 62.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 62.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 62.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 63.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 63.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 63.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 63.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 63.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 63.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 63.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 63.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 63.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 63.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 63.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 63.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 63.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 63.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 64.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 63.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 64.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 64.16it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 64.26it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 782/782 [00:39<00:00, 19.98it/s, v_num=0, train_acc_step=0.438,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 43.24it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 45.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 53.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 57.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 52.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 47.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 48.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 51.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 53.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 56.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 57.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 57.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 56.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 55.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 56.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 58.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 59.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 59.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 60.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 60.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 61.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 62.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 62.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 61.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 61.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 60.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 60.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 60.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 61.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 61.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 61.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 61.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 61.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 61.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 61.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 62.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 62.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 62.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 61.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 62.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 61.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 61.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 62.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 62.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 62.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 62.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 62.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 62.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 61.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 61.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 61.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 61.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 61.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 61.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 61.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 61.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 62.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 61.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 62.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:01, 61.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:01, 62.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:01, 62.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 61.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 62.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 62.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 62.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 62.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 61.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 61.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 61.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 61.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 61.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 61.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 61.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 61.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 61.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 61.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 61.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 61.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 61.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 61.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 61.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 61.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 61.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 61.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 61.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 61.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 61.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 61.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 61.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 61.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:01, 61.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:01, 60.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:01, 60.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 60.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 60.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 60.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 60.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 59.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 59.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 60.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 60.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 60.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 60.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 59.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 59.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 59.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 59.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 59.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 59.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 59.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 59.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 59.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 59.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 59.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 59.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 59.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 59.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 59.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 59.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 59.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 59.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 60.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 59.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 59.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 59.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 59.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 59.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 59.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 59.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 59.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 59.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 59.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 59.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 59.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 59.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 59.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 59.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 59.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 60.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 60.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 60.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 60.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 60.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 60.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 60.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 60.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 60.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 60.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 60.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 60.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 60.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 60.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 60.37it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 60.43it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 782/782 [00:39<00:00, 20.02it/s, v_num=0, train_acc_step=0.312,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 45.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 60.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 51.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 37.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 31.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 33.62it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:04, 34.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:04, 37.20it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 38.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 40.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 42.31it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 43.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 43.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 44.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 45.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 45.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 46.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 46.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 47.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 46.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 47.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 48.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 48.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 49.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 50.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 51.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 51.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 52.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 52.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 53.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 53.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 54.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 54.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 53.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 53.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 54.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 53.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 54.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 54.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 54.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 54.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 54.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:02, 54.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:02, 54.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:02, 55.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 55.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 55.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 55.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 56.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 56.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 55.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 56.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 56.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 56.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 56.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 56.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:01, 56.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:01, 56.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:01, 56.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:01, 56.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:01, 56.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:01, 56.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:01, 56.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:01, 56.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 57.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 57.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 57.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 57.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 57.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 57.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 57.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 57.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 57.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 57.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 57.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 57.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 57.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 57.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 58.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 58.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 58.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 58.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 58.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 58.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 57.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 57.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 57.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 57.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 57.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 57.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 57.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 57.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:01, 57.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:01, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:01, 57.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:01, 57.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:01, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:01, 57.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 57.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 57.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 57.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 57.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 57.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 57.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 57.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 57.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 57.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 57.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 57.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 57.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 57.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 57.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 57.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 57.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 57.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 57.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 57.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 58.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 57.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 57.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 57.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 58.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 57.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 57.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 57.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 57.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 57.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 57.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 57.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 57.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 57.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 57.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 57.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 57.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 57.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 57.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 57.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 57.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 57.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 57.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 57.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 57.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 57.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 57.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 57.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 57.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 57.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 57.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 57.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 57.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 57.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 57.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 57.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 57.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 57.59it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 782/782 [00:39<00:00, 19.79it/s, v_num=0, train_acc_step=0.312,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 48.43it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 55.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 54.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 57.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 52.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 56.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 58.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 60.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 60.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 62.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 62.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 60.27it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.22it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 58.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 59.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 60.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 60.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 60.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 61.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 62.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 62.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 63.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 63.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 64.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 62.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 63.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 64.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 64.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 63.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 62.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 61.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 60.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 60.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 60.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 60.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 60.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 61.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 61.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 62.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 62.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 62.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 62.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 62.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 63.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 63.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 62.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 61.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 61.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 61.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 61.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 61.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 61.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 62.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 62.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 62.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 62.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 62.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 61.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 61.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 61.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:01, 61.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:01, 61.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:01, 61.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 61.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 61.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 61.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 61.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 61.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 60.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 60.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 60.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 60.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 60.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 60.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 60.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 60.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 60.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 60.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 60.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 60.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 60.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 60.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 60.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 60.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 60.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 60.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 60.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 60.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 60.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 60.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 60.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 60.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:01, 60.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:01, 60.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:01, 60.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 60.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 60.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 60.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 60.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 60.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 60.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 60.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 60.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 60.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 60.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 60.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 61.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 61.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 61.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 61.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 61.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 60.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 60.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 60.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 60.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 60.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 60.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 60.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 61.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 61.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 61.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 61.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 60.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 60.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 60.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 61.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 61.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 61.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 61.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 61.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 61.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 61.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 61.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 61.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 61.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 61.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 61.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 61.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 61.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 61.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 61.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 61.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 61.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 62.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 62.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 62.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 62.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 62.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 62.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 62.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 62.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 62.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 62.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 62.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 62.56it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 62.55it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 782/782 [00:41<00:00, 19.06it/s, v_num=0, train_acc_step=0.375,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 45.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:04, 32.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 30.34it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 33.63it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 38.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 47.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 47.75it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 49.19it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 49.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 49.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 50.29it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 50.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 51.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 52.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 52.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 51.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 50.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 50.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 50.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 51.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 51.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 52.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 52.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 53.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 54.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 54.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 53.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 54.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 53.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 53.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 53.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 53.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 54.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 54.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 54.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 54.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 54.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 55.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 55.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 55.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:02, 55.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:02, 56.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 56.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 56.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 55.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 56.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 56.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 56.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 56.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 56.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 57.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 57.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 57.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 57.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 57.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:01, 57.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:01, 57.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:01, 57.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:01, 57.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:01, 57.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:01, 57.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:01, 57.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 57.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 57.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 57.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 57.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 57.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 57.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 57.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 57.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 57.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 57.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 57.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 57.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 57.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 57.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 57.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 58.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 57.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 57.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 57.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 57.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 57.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 57.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 57.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 57.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 57.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 57.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 57.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 58.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:01, 57.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:01, 57.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:01, 57.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:01, 58.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:01, 57.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:01, 57.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 57.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 57.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 57.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 58.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 57.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 57.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 57.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 57.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 57.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 57.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 57.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 57.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 57.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 57.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 57.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 57.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 57.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 57.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 57.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 57.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 58.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 58.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 58.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 58.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 58.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 58.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 58.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 58.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 58.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 58.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 57.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 58.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 58.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 58.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 58.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 58.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 58.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 58.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 58.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 58.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 58.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 58.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 58.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 58.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 58.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 58.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 58.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 58.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 59.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 59.13it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 59.20it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 782/782 [00:40<00:00, 19.40it/s, v_num=0, train_acc_step=0.500,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:03, 50.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 55.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 58.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 47.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 36.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:04, 36.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 39.72it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 41.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 39.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:03, 41.49it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:03, 43.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:03, 44.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:03, 44.75it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:03, 43.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:03, 45.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:03, 45.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:03, 46.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 46.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 46.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 47.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 47.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 47.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 48.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 49.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:02, 48.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:02, 48.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:02, 49.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:02, 49.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:02, 50.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:02, 50.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:02, 51.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:02, 51.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:02, 51.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:02, 52.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:02, 52.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:02, 52.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:02, 53.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:02, 53.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:02, 53.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:02, 54.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:02, 54.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:02, 54.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:02, 54.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:02, 53.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:02, 52.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:02, 52.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:02, 52.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:02, 53.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:02, 53.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:02, 53.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 53.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 53.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 53.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:01<00:01, 53.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:01<00:01, 54.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:01<00:01, 54.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:01<00:01, 54.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:01<00:01, 54.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:01<00:01, 55.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:01<00:01, 55.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:01<00:01, 55.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:01<00:01, 55.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:01<00:01, 54.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:01<00:01, 55.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:01<00:01, 55.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:01<00:01, 55.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:01<00:01, 55.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:01<00:01, 55.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:01<00:01, 55.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:01<00:01, 55.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:01<00:01, 55.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:01<00:01, 55.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:01<00:01, 55.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:01<00:01, 55.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 55.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 55.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 55.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 55.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 55.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 55.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 55.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 56.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:01, 56.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:01, 56.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:01, 56.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:01, 56.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:01, 56.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:01, 56.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:01, 56.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:01, 56.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:01, 56.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:01, 56.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:01, 56.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:01, 57.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:01, 57.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:01, 57.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:01, 57.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:01, 57.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:01, 57.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 57.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 57.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 57.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 57.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 57.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 57.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 57.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 57.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 57.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 56.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 56.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 56.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 56.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:02<00:00, 56.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:02<00:00, 56.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:02<00:00, 56.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:02<00:00, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:02<00:00, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:02<00:00, 57.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:02<00:00, 57.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:02<00:00, 57.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:02<00:00, 57.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:02<00:00, 57.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:02<00:00, 57.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:02<00:00, 57.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:02<00:00, 57.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:02<00:00, 57.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:02<00:00, 57.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:02<00:00, 57.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:02<00:00, 57.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:02<00:00, 57.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:02<00:00, 57.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:02<00:00, 57.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:02<00:00, 57.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:02<00:00, 57.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:02<00:00, 57.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:02<00:00, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:02<00:00, 56.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:02<00:00, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:02<00:00, 56.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:02<00:00, 56.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:02<00:00, 56.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:02<00:00, 56.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:02<00:00, 56.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:02<00:00, 57.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:02<00:00, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:02<00:00, 57.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:02<00:00, 57.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:02<00:00, 57.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:02<00:00, 57.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:02<00:00, 57.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 57.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 57.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 57.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 57.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 57.49it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 57.50it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 782/782 [00:43<00:00, 17.95it/s, v_num=0, train_acc_step=0.500,\u001b[AI0315 18:27:17.058184 140174726280256 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Epoch 9: 100%|█| 782/782 [00:44<00:00, 17.71it/s, v_num=0, train_acc_step=0.500,\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 ./ch train --config /workspace/ADLS_Proj/docs/tutorials/proj/resnet50_INT8_quant.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load in the checkpoint. You will have to adjust this according to where it has been stored in the mase_output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done!\n",
      "dummy in done\n",
      "_ done\n",
      "init_metadata_analysis_pass done\n",
      "add_common_metadata_analysis_pass done\n",
      "add_software_metadata_analysis_pass done\n",
      "metadata_value_type_cast_transform_pass done\n",
      "using safe deepcopy\n",
      "deep copy done\n"
     ]
    }
   ],
   "source": [
    "# Load in the trained checkpoint - change this accordingly\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet18_cls_cifar10_2025-03-08/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "\n",
    "model = load_model(load_name=RES_CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "print(\"load model done!\")\n",
    "\n",
    "# Initiate metadata\n",
    "dummy_in = next(iter(input_generator))\n",
    "print(\"dummy in done\")\n",
    "\n",
    "_ = model(**dummy_in)\n",
    "print(\"_ done\")\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "print(\"init_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "print(\"add_common_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "print(\"add_software_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})\n",
    "print(\"metadata_value_type_cast_transform_pass done\")\n",
    "\n",
    "# Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
    "mg_original = deepcopy_mase_graph(mg)\n",
    "print(\"deep copy done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Resnet: INT8/FP16/FP32 Quantization Comparison\n",
    "\n",
    "We will now load in a new toml configuration that uses fp16 instead of int8, whilst keeping the other settings the exact same for a fair comparison. This time however, we will use chop from the terminal which runs all the passes showcased in [Section 1](#section-1---int8-quantization).\n",
    "\n",
    "Since float quantization does not require calibration, nor is it supported by `pytorch-quantization`, the model will not undergo fake quantization; for the time being this unfortunately means QAT is unavailable and only undergoes Post Training Quantization (PTQ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 23:05:43.177328 139704359384128 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet50_cls_ci | e_output/resnet50_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-15/softwar | far10_2025-03-15/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet50   |                          |         resnet50         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet50'...\u001b[0m\n",
      "self.args.model is resnet50\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet50'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet50', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['mse'], 'percentiles': [99.0, 99.5], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBefore analyzing original graph: pass_args = {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['mse'], 'percentiles': [99.0, 99.5], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}, 'task': 'cls', 'dataset': 'cifar10', 'batch_size': 64, 'model': 'resnet50', 'data_module': <chop.dataset.MaseDataModule object at 0x7f0d494aa7d0>, 'accelerator': 'cuda', 'num_GPU_warmup_batches': 0, 'num_batches': 1, 'test': True}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.51486   |\n",
      "|      Average Precision       |   0.55863   |\n",
      "|        Average Recall        |   0.51562   |\n",
      "|       Average F1 Score       |   0.52506   |\n",
      "|         Average Loss         |    1.342    |\n",
      "|       Average Latency        |  36.239 ms  |\n",
      "|   Average GPU Power Usage    |   16.17 W   |\n",
      "| Inference Energy Consumption | 0.16277 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_12/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      53 |         0 |          53 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      53 |         0 |          53 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      49 |         0 |          49 |\n",
      "| add               | add                 |      16 |         0 |          16 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.896738 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.896901 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897003 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897096 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897189 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897279 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897369 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897469 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897562 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897648 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897736 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897819 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897906 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.897989 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898074 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898156 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898243 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898326 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898413 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898495 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898580 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898662 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898750 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898832 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.898918 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899005 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899090 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899175 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899264 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899346 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899436 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899519 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899605 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899688 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899774 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899858 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.899944 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900026 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900111 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900192 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900276 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900362 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900448 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900529 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900615 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900696 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900781 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900862 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.900949 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901031 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901116 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901224 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901316 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901408 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901495 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901577 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901663 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901746 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901831 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901912 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.901995 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902077 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902161 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902243 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902308 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902373 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902467 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902562 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902649 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902733 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902818 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902900 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.902985 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903066 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903141 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903222 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903307 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903379 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903463 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903545 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903630 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903712 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903796 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903878 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.903962 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904043 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904130 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904191 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904274 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904356 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904429 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904511 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904597 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904678 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904765 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904848 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.904933 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905014 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905098 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905181 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905266 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905348 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905433 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905494 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905563 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905646 139704359384128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905736 139704359384128 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 23:06:05.905818 139704359384128 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0316 23:06:06.179313 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 23:06:06.179410 139704359384128 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3206 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:06.351093 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:06.611588 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5933 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:06.794480 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7290 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:07.061049 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9987 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:07.241077 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2524 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:07.502488 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5019 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:07.686676 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3817 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:07.948346 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.5933 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:08.125993 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:08.431378 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.8485 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:08.608196 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6649 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:08.935211 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9716 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:09.116999 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2461 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:09.422802 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.3319 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:09.601585 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3588 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:09.887729 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=17.9334 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:10.246746 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:10.551910 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:10.730402 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2527 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:11.003267 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.5120 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:11.195144 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3412 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:11.502820 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=26.3615 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:11.686788 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4854 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:11.958530 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:12.134483 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:12.375149 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8393 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:12.546498 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:12.903264 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=26.3615 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:13.088573 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2760 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:13.462130 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=27.0863 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:13.645764 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:13.979719 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.5131 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:14.162681 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1964 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:14.442550 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.2049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:14.618660 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:14.944281 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=34.3949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:15.125180 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5068 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:15.437346 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.3580 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:15.614578 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:15.920317 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8841 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:16.176619 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2850 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:16.546292 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=36.0607 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:16.740857 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4743 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:17.089868 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.2986 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:17.267562 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1998 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:17.569813 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8788 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:17.749605 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2653 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:18.059121 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=39.0160 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:18.235242 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:18.617894 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.6177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:18.804883 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:19.138399 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8912 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:19.329781 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2110 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:19.654813 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=39.0160 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:19.838143 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:20.128025 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=20.8949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:20.366109 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:20.698667 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0316 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:20.963153 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1672 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:21.379292 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.7718 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:21.592229 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:21.965547 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=23.3042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:22.208551 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3707 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:22.540111 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1911 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:22.733666 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1578 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:23.053665 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.7694 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:23.231384 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1978 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:23.552819 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=27.3679 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:23.728908 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3611 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:24.008339 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.6012 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:24.189627 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:24.462923 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4330 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:24.662718 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1972 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:24.966996 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=32.0123 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:25.181287 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3573 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:25.478532 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0414 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:25.678558 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1516 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:25.942413 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9769 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:26.165978 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1878 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:26.483774 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=35.6297 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:26.674469 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:26.951733 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0107 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:27.133493 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1526 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:27.377392 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0871 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:27.562729 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1912 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:27.859772 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=41.1631 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:28.048881 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2639 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:28.310765 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2942 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:28.511963 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:28.771915 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5858 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:28.960625 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:29.243081 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=41.1631 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:29.430350 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1986 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:29.676847 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.2726 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:29.865949 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:30.154985 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4193 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:30.355524 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:30.608526 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7858 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:30.801521 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1521 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:31.037883 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=18.7568 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:31.219318 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2501 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:31.461815 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1833 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:31.638042 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0833 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:31.887621 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2684 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:32.097555 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1311 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:32.098019 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1760 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 23:06:32.098528 139704359384128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.50227   |\n",
      "|      Average Precision       |   0.48806   |\n",
      "|        Average Recall        |   0.49793   |\n",
      "|       Average F1 Score       |   0.48839   |\n",
      "|         Average Loss         |   1.4049    |\n",
      "|       Average Latency        |  118.73 ms  |\n",
      "|   Average GPU Power Usage    |  12.876 W   |\n",
      "| Inference Energy Consumption | 0.42469 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_13/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0316 23:06:52.659622 139704359384128 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0316 23:06:52.659957 139704359384128 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0316 23:06:52.660012 139704359384128 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0316 23:06:55.311145 139704359384128 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 23:06:55.327903 139704359384128 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 23.5 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "263       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [01:06<00:00, 11.71it/s, v_num=20, train_acc_step=0.438\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:10, 14.24it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:10, 15.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:09, 15.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:09, 16.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:08, 17.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:08, 17.73it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:08, 18.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:08, 18.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:07, 18.73it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:07, 18.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:07, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:07, 19.10it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:07, 19.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:07, 19.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:07, 19.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:07, 19.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:07, 19.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:07, 19.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:07, 19.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:01<00:06, 19.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:01<00:06, 19.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:01<00:06, 19.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:01<00:06, 19.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:01<00:06, 19.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:01<00:06, 19.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:01<00:06, 19.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:01<00:06, 19.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:01<00:06, 19.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:01<00:06, 19.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:01<00:06, 19.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:01<00:06, 19.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:01<00:06, 19.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:01<00:06, 19.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:01<00:06, 19.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:06, 19.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:06, 19.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:06, 19.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:01<00:06, 19.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:02<00:06, 19.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:02<00:06, 19.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:02<00:06, 19.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:02<00:06, 19.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:02<00:05, 19.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:02<00:05, 19.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:02<00:05, 19.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:02<00:05, 19.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:02<00:05, 19.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:02<00:05, 19.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:02<00:05, 19.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:02<00:05, 19.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:02<00:05, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:02<00:05, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:02<00:05, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:02<00:05, 18.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:02<00:05, 19.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:02<00:05, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:03<00:05, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:03<00:05, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:03<00:05, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:03<00:05, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:03<00:05, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:03<00:05, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:03<00:04, 18.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:03<00:04, 18.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:03<00:04, 18.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:03<00:04, 18.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:03<00:04, 18.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:03<00:04, 18.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:03<00:04, 18.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:03<00:04, 18.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:03<00:04, 18.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:03<00:04, 18.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:03<00:04, 18.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:03<00:04, 18.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:03<00:04, 18.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:04<00:04, 18.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:04<00:04, 18.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:04<00:04, 18.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:04<00:04, 18.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:04<00:04, 18.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:04<00:04, 18.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:04<00:03, 18.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:04<00:03, 18.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:04<00:03, 18.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:04<00:03, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:04<00:03, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:04<00:03, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:04<00:03, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:04<00:03, 18.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:04<00:03, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:04<00:03, 18.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:04<00:03, 18.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:04<00:03, 18.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:05<00:03, 18.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:05<00:03, 18.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:05<00:03, 18.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:05<00:03, 18.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:05<00:03, 18.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:05<00:03, 18.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:05<00:03, 18.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:05<00:03, 18.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:05<00:02, 18.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:05<00:02, 18.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:05<00:02, 18.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:05<00:02, 18.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:05<00:02, 18.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:05<00:02, 18.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:05<00:02, 18.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:05<00:02, 18.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:05<00:02, 18.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:06<00:02, 18.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:06<00:02, 18.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:06<00:02, 18.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:06<00:02, 18.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:06<00:02, 18.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:06<00:02, 18.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:06<00:02, 18.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:06<00:02, 18.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:06<00:02, 18.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:06<00:02, 18.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:06<00:01, 18.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:06<00:01, 18.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:06<00:01, 18.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:06<00:01, 18.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:06<00:01, 18.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:06<00:01, 18.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:06<00:01, 18.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:06<00:01, 18.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:07<00:01, 18.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:07<00:01, 18.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:07<00:01, 18.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:07<00:01, 18.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:07<00:01, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:07<00:01, 18.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:07<00:01, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:07<00:01, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:07<00:01, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:07<00:01, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:07<00:00, 18.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:07<00:00, 18.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:07<00:00, 18.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:07<00:00, 18.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:07<00:00, 18.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:07<00:00, 18.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:07<00:00, 18.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:08<00:00, 18.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:08<00:00, 18.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:08<00:00, 18.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:08<00:00, 18.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:08<00:00, 18.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:08<00:00, 18.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:08<00:00, 18.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:08<00:00, 18.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:08<00:00, 18.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:08<00:00, 18.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:08<00:00, 18.06it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:08<00:00, 18.03it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [01:02<00:00, 12.58it/s, v_num=20, train_acc_step=0.562\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:09, 15.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:08, 18.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:08, 19.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:08, 18.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:08, 17.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:08, 18.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:08, 18.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:07, 18.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:07, 19.11it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:07, 18.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:07, 19.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:07, 19.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:07, 18.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:07, 18.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:07, 18.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:07, 18.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:07, 18.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:07, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:01<00:07, 18.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:01<00:07, 18.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:01<00:07, 18.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:01<00:07, 18.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:01<00:07, 18.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:01<00:07, 18.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:01<00:07, 18.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:01<00:07, 18.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:01<00:07, 18.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:01<00:07, 18.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:01<00:06, 18.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:01<00:06, 18.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:01<00:06, 18.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:01<00:06, 18.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:01<00:06, 18.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:01<00:06, 18.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:01<00:06, 18.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:01<00:06, 18.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:01<00:06, 18.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:02<00:06, 18.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:02<00:06, 18.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:02<00:06, 18.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:02<00:06, 18.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:02<00:06, 18.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:02<00:06, 18.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:02<00:06, 18.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:02<00:06, 18.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:02<00:06, 18.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:02<00:06, 18.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:02<00:06, 18.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:02<00:05, 18.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:02<00:05, 18.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:02<00:05, 18.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:02<00:05, 18.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:02<00:05, 18.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:02<00:05, 18.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:03<00:05, 18.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:03<00:05, 18.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:03<00:05, 18.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:03<00:05, 18.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:03<00:05, 18.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:03<00:05, 18.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:03<00:05, 18.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:03<00:05, 18.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:03<00:05, 18.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:03<00:05, 18.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:03<00:05, 18.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:03<00:05, 18.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:03<00:04, 18.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:03<00:04, 18.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:03<00:04, 18.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:03<00:04, 18.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:03<00:04, 18.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:03<00:04, 18.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:04<00:04, 18.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:04<00:04, 18.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:04<00:04, 18.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:04<00:04, 18.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:04<00:04, 18.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:04<00:04, 18.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:04<00:04, 17.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:04<00:04, 17.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:04<00:04, 17.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:04<00:04, 17.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:04<00:04, 17.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:04<00:04, 17.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:04<00:04, 17.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:04<00:03, 17.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:04<00:03, 17.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:04<00:03, 17.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:04<00:03, 17.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:05<00:03, 17.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:05<00:03, 17.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:05<00:03, 17.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:05<00:03, 17.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:05<00:03, 17.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:05<00:03, 17.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:05<00:03, 17.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:05<00:03, 17.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:05<00:03, 17.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:05<00:03, 17.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:05<00:03, 17.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:05<00:03, 17.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:05<00:03, 17.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:05<00:03, 17.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:05<00:02, 17.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:05<00:02, 17.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:05<00:02, 17.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:06<00:02, 17.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:06<00:02, 17.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:06<00:02, 17.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:06<00:02, 17.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:06<00:02, 17.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:06<00:02, 17.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:06<00:02, 17.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:06<00:02, 17.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:06<00:02, 17.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:06<00:02, 17.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:06<00:02, 17.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:06<00:02, 17.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:06<00:02, 17.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:06<00:02, 17.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:06<00:02, 17.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:06<00:01, 17.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:06<00:01, 17.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:07<00:01, 17.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:07<00:01, 17.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:07<00:01, 17.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:07<00:01, 17.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:07<00:01, 17.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:07<00:01, 17.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:07<00:01, 17.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:07<00:01, 17.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:07<00:01, 17.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:07<00:01, 17.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:07<00:01, 17.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:07<00:01, 17.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:07<00:01, 17.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:07<00:01, 17.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:07<00:01, 17.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:07<00:01, 17.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:08<00:00, 17.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:08<00:00, 17.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:08<00:00, 17.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:08<00:00, 17.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:08<00:00, 17.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:08<00:00, 17.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:08<00:00, 17.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:08<00:00, 17.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:08<00:00, 17.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:08<00:00, 17.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:08<00:00, 17.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:08<00:00, 17.34it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:09<00:00, 17.35it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [01:11<00:00, 10.92it/s, v_num=20, train_acc_step=0.562\u001b[AI0316 23:09:25.286116 139704359384128 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [01:12<00:00, 10.81it/s, v_num=20, train_acc_step=0.562\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_5/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "default_precision: int8\n",
      "Failed finding int8 cache!\n",
      "Failed finding int8 cache!\n",
      "Succeed saving int8 cache!\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_7/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_8/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50-trt_quantized\u001b[0m\n",
      "[03/16/2025-23:29:00] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.51279    |\n",
      "|      Average Precision       |   0.50882    |\n",
      "|        Average Recall        |    0.5162    |\n",
      "|       Average F1 Score       |   0.50441    |\n",
      "|         Average Loss         |    1.3589    |\n",
      "|       Average Latency        |  6.4791 ms   |\n",
      "|   Average GPU Power Usage    |   32.987 W   |\n",
      "| Inference Energy Consumption | 0.059368 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/tensorrt/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet50_INT8_quant_debug.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 20:10:49.459262 140012433056832 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet50_cls_ci | e_output/resnet50_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-15/softwar | far10_2025-03-15/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet50   |                          |         resnet50         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet50'...\u001b[0m\n",
      "self.args.model is resnet50\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet50'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet50', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['mse'], 'percentiles': [99.0, 99.5], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBefore analyzing original graph: pass_args = {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['mse'], 'percentiles': [99.0, 99.5], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}, 'task': 'cls', 'dataset': 'cifar10', 'batch_size': 64, 'model': 'resnet50', 'data_module': <chop.dataset.MaseDataModule object at 0x7f5503420610>, 'accelerator': 'cuda', 'num_GPU_warmup_batches': 0, 'num_batches': 1, 'test': True}\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.51486    |\n",
      "|      Average Precision       |   0.55863    |\n",
      "|        Average Recall        |   0.51562    |\n",
      "|       Average F1 Score       |   0.52506    |\n",
      "|         Average Loss         |    1.342     |\n",
      "|       Average Latency        |   21.48 ms   |\n",
      "|   Average GPU Power Usage    |   12.991 W   |\n",
      "| Inference Energy Consumption | 0.077512 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_10/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      53 |         0 |          53 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      53 |         0 |          53 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      49 |         0 |          49 |\n",
      "| add               | add                 |      16 |         0 |          16 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.545340 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.545527 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.545641 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.545733 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.545826 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.545914 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546006 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546104 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546198 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546283 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546373 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546459 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546547 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546640 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546729 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546813 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546901 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.546986 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547075 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547160 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547247 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547324 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547399 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547485 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547575 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547661 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547748 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547835 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.547924 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548012 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548104 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548188 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548267 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548352 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548438 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548522 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548609 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548693 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548782 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548866 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.548953 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549038 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549125 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549210 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549296 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549379 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549466 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549548 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549637 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549719 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549805 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549889 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.549974 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550073 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550151 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550237 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550325 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550445 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550536 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550620 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550707 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550791 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550880 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.550965 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551053 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551136 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551222 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551306 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551393 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551478 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551569 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551651 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551736 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551820 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551913 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.551997 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552083 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552165 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552250 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552330 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552416 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552498 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552583 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552664 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552747 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552828 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552915 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.552996 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553081 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553162 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553246 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553331 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553411 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553493 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553579 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553661 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553746 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553820 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553905 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.553987 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554073 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554156 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554241 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554323 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554408 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554490 140012433056832 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554581 140012433056832 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 20:11:12.554666 140012433056832 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0316 20:11:12.809484 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 20:11:12.809594 140012433056832 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3206 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:12.997720 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:13.257469 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5933 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:13.439609 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7290 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:13.679066 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9987 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:13.845265 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2524 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:14.121342 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5019 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:14.292062 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3817 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:14.540163 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.5933 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:14.718785 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:14.976784 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.8485 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:15.147632 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6649 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:15.531021 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9716 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:15.708998 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2461 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:15.987306 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.3319 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:16.159100 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3588 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:16.647680 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=17.9334 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:16.821424 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:17.124683 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:17.301449 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2527 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:17.580554 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.5120 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:17.757423 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3412 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:18.116881 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=26.3615 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:18.328495 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4854 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:18.650205 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:18.825752 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:19.141841 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8393 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:19.321999 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:19.632478 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=26.3615 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:19.851923 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2760 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:20.222049 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=27.0863 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:20.398890 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:20.748413 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.5131 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:20.938358 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1964 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:21.229487 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.2049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:21.411765 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:21.727486 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=34.3949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:21.908626 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5068 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:22.162966 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.3580 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:22.341159 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:22.622112 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8841 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:22.799859 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2850 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:23.141278 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=36.0607 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:23.316511 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4743 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:23.612173 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.2986 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:23.783991 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1998 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:24.042100 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8788 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:24.215898 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2653 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:24.520248 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=39.0160 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:24.697716 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:24.980736 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.6177 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:25.153956 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:25.415730 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8912 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:25.592574 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2110 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:25.895169 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=39.0160 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:26.064349 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:26.350700 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=20.8949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:26.522957 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:26.800722 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0316 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:26.969406 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1672 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:27.220896 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.7718 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:27.392278 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:27.699542 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=23.3042 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:27.880890 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3707 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:28.136577 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1911 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:28.312632 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1578 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:28.577350 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.7694 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:28.750412 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1978 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:29.018845 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=27.3679 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:29.189175 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3611 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:29.456026 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.6012 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:29.634968 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:29.919236 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4330 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:30.101176 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1972 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:30.369111 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=32.0123 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:30.548007 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3573 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:30.777107 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0414 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:30.954965 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1516 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:31.227038 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9769 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:31.399289 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1878 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:31.686510 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=35.6297 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:31.863688 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:32.131063 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0107 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:32.318840 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1526 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:32.540355 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0871 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:32.727107 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1912 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:32.980104 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=41.1631 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:33.165566 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2639 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:33.391777 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2942 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:33.572660 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:33.779647 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5858 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:33.973486 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:34.236755 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=41.1631 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:34.409557 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1986 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:34.661944 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.2726 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:34.846486 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:35.111780 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4193 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:35.293018 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:35.516913 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7858 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:35.696340 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1521 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:35.920689 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=18.7568 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.100064 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2501 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.294252 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1833 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.464809 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0833 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.696143 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2684 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.876161 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1311 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.876545 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1760 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 20:11:36.877170 140012433056832 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.50227   |\n",
      "|      Average Precision       |   0.48806   |\n",
      "|        Average Recall        |   0.49793   |\n",
      "|       Average F1 Score       |   0.48839   |\n",
      "|         Average Loss         |   1.4049    |\n",
      "|       Average Latency        |  58.52 ms   |\n",
      "|   Average GPU Power Usage    |  15.519 W   |\n",
      "| Inference Energy Consumption | 0.25227 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_11/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0316 20:11:47.904735 140012433056832 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0316 20:11:47.905069 140012433056832 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0316 20:11:47.905128 140012433056832 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0316 20:11:50.482949 140012433056832 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 20:11:50.506856 140012433056832 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 23.5 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "263       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [01:04<00:00, 12.10it/s, v_num=19, train_acc_step=0.438\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:10, 14.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:09, 16.29it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:09, 16.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:09, 15.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:09, 16.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:09, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:09, 16.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:08, 17.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:08, 17.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:08, 16.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:08, 17.22it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:08, 17.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:08, 17.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:08, 17.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:08, 17.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:07, 17.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:07, 17.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:01<00:07, 17.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:01<00:07, 17.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:01<00:07, 17.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:01<00:07, 17.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:01<00:07, 17.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:01<00:07, 17.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:01<00:07, 17.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:01<00:07, 17.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:01<00:07, 17.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:01<00:07, 17.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:01<00:07, 17.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:01<00:07, 17.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:01<00:07, 17.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:01<00:07, 17.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:01<00:07, 17.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:01<00:07, 17.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:01<00:07, 17.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:02<00:06, 17.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:02<00:06, 17.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:02<00:06, 17.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:02<00:06, 17.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:02<00:06, 17.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:02<00:06, 17.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:02<00:06, 17.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:02<00:06, 17.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:02<00:06, 17.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:02<00:06, 17.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:02<00:06, 17.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:02<00:06, 17.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:02<00:06, 17.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:02<00:06, 17.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:02<00:06, 17.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:02<00:06, 17.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:02<00:06, 17.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:03<00:06, 17.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:03<00:06, 17.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:03<00:06, 17.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:03<00:06, 16.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:03<00:05, 16.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:03<00:05, 16.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:03<00:05, 16.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:03<00:05, 16.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:03<00:05, 16.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:03<00:05, 16.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:03<00:05, 16.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:03<00:05, 16.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:03<00:05, 16.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:03<00:05, 16.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:03<00:05, 16.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:03<00:05, 16.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:04<00:05, 16.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:04<00:05, 16.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:04<00:05, 16.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:04<00:05, 16.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:04<00:05, 16.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:04<00:05, 16.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:04<00:04, 16.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:04<00:04, 16.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:04<00:04, 16.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:04<00:04, 16.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:04<00:04, 16.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:04<00:04, 16.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:04<00:04, 16.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:04<00:04, 16.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:04<00:04, 16.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:04<00:04, 16.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:05<00:04, 16.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:05<00:04, 16.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:05<00:04, 16.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:05<00:04, 16.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:05<00:04, 16.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:05<00:04, 16.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:05<00:04, 16.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:05<00:03, 16.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:05<00:03, 16.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:05<00:03, 16.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:05<00:03, 16.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:05<00:03, 16.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:05<00:03, 16.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:05<00:03, 16.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:05<00:03, 16.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:06<00:03, 16.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:06<00:03, 16.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:06<00:03, 16.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:06<00:03, 16.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:06<00:03, 16.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:06<00:03, 16.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:06<00:03, 16.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:06<00:03, 16.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:06<00:03, 16.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:06<00:02, 16.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:06<00:02, 16.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:06<00:02, 16.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:06<00:02, 16.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:06<00:02, 16.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:06<00:02, 16.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:06<00:02, 16.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:06<00:02, 16.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:07<00:02, 16.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:07<00:02, 16.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:07<00:02, 16.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:07<00:02, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:07<00:02, 16.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:07<00:02, 16.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:07<00:02, 16.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:07<00:02, 16.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:07<00:02, 16.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:07<00:01, 16.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:07<00:01, 16.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:07<00:01, 16.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:07<00:01, 16.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:07<00:01, 16.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:07<00:01, 16.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:08<00:01, 16.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:08<00:01, 16.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:08<00:01, 16.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:08<00:01, 16.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:08<00:01, 16.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:08<00:01, 16.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:08<00:01, 16.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:08<00:01, 16.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:08<00:01, 16.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:08<00:01, 16.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:08<00:00, 16.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:08<00:00, 16.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:08<00:00, 16.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:08<00:00, 16.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:08<00:00, 16.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:08<00:00, 16.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:09<00:00, 16.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:09<00:00, 16.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:09<00:00, 16.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:09<00:00, 16.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:09<00:00, 16.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:09<00:00, 16.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:09<00:00, 16.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:09<00:00, 16.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:09<00:00, 16.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:09<00:00, 16.13it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:09<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [02:11<00:00,  5.94it/s, v_num=19, train_acc_step=0.562\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:15, 10.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:15,  9.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:14, 10.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:13, 10.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:15,  9.64it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:15,  9.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:14, 10.27it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:14,  9.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:15,  9.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:01<00:15,  9.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:01<00:14,  9.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:01<00:15,  9.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:01<00:15,  9.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:01<00:14,  9.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:01<00:14,  9.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:01<00:15,  9.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:01<00:14,  9.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:01<00:14,  9.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:02<00:14,  9.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:02<00:14,  9.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:02<00:14,  9.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:02<00:14,  9.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:02<00:14,  9.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:02<00:14,  9.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:02<00:14,  9.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:02<00:13,  9.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:02<00:14,  9.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:02<00:13,  9.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:03<00:13,  9.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:03<00:13,  9.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:03<00:13,  9.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:03<00:13,  9.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:03<00:13,  9.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:03<00:13,  9.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:03<00:13,  9.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:03<00:12,  9.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:03<00:12,  9.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:04<00:12,  9.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:04<00:12,  9.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:04<00:12,  9.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:04<00:12,  9.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:04<00:12,  9.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:04<00:12,  9.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:04<00:12,  9.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:04<00:12,  9.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:05<00:12,  8.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:05<00:12,  8.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:05<00:12,  8.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:05<00:12,  8.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:05<00:12,  8.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:05<00:12,  8.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:05<00:12,  8.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:06<00:11,  8.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:06<00:11,  8.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:06<00:11,  8.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:06<00:11,  8.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:06<00:11,  8.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:06<00:11,  8.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:06<00:11,  8.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:07<00:11,  8.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:07<00:11,  8.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:07<00:11,  8.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:07<00:11,  8.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:07<00:11,  8.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:07<00:10,  8.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:07<00:10,  8.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:08<00:10,  8.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:08<00:10,  8.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:08<00:10,  8.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:08<00:10,  8.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:08<00:10,  8.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:08<00:10,  8.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:08<00:10,  8.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:09<00:10,  8.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:09<00:10,  8.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:09<00:09,  8.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:09<00:09,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:09<00:09,  8.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:09<00:09,  8.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:09<00:09,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:10<00:09,  8.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:10<00:09,  8.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:10<00:09,  8.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:10<00:09,  8.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:10<00:08,  8.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:10<00:08,  7.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:10<00:08,  8.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:11<00:08,  7.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:11<00:08,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:11<00:08,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:11<00:08,  7.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:11<00:08,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:11<00:08,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:12<00:08,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:12<00:07,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:12<00:07,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:12<00:07,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:12<00:07,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:12<00:07,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:12<00:07,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:12<00:07,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:13<00:07,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:13<00:06,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:13<00:06,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:13<00:06,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:13<00:06,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:13<00:06,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:13<00:06,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:14<00:06,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:14<00:06,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:14<00:05,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:14<00:05,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:14<00:05,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:14<00:05,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:14<00:05,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:14<00:05,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:14<00:05,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:15<00:05,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:15<00:04,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:15<00:04,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:15<00:04,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:15<00:04,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:15<00:04,  7.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:16<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:16<00:04,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:16<00:04,  7.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:16<00:03,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:16<00:03,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:16<00:03,  7.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:16<00:03,  7.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:16<00:03,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:17<00:03,  7.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:17<00:03,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:17<00:02,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:17<00:02,  7.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:17<00:02,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:17<00:02,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:17<00:02,  7.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:18<00:02,  7.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:18<00:02,  7.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:18<00:02,  7.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:18<00:01,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:18<00:01,  7.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:18<00:01,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:18<00:01,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:19<00:01,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:19<00:01,  7.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:19<00:01,  7.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:19<00:01,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:19<00:00,  7.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:19<00:00,  7.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:19<00:00,  7.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:20<00:00,  7.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:20<00:00,  7.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:20<00:00,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:20<00:00,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:20<00:00,  7.69it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [02:32<00:00,  5.13it/s, v_num=19, train_acc_step=0.562\u001b[AI0316 20:15:40.377620 140012433056832 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [02:33<00:00,  5.10it/s, v_num=19, train_acc_step=0.562\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_1/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_2/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50-trt_quantized\u001b[0m\n",
      "[03/16/2025-20:17:42] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.51947   |\n",
      "|      Average Precision       |   0.51377   |\n",
      "|        Average Recall        |   0.52206   |\n",
      "|       Average F1 Score       |   0.51048   |\n",
      "|         Average Loss         |   1.3463    |\n",
      "|       Average Latency        |  8.6635 ms  |\n",
      "|   Average GPU Power Usage    |  32.582 W   |\n",
      "| Inference Energy Consumption | 0.07841 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/tensorrt/version_4/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet50_INT8_quant_debug.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 00:18:52.977772 140198266307648 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet50_cls_ci | e_output/resnet50_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-15/softwar | far10_2025-03-15/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet50   |                          |         resnet50         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet50'...\u001b[0m\n",
      "self.args.model is resnet50\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet50'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet50', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n",
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      53 |         0 |          53 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      53 |         0 |          53 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      49 |         0 |          49 |\n",
      "| add               | add                 |      16 |         0 |          16 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665140 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665305 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665406 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665489 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665574 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665654 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665738 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665825 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665909 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.665987 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666068 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666146 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666223 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666299 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666369 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666444 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666523 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666599 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666678 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666753 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666830 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666908 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.666986 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667062 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667132 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667215 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667294 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667372 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667452 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667531 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667611 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667688 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667767 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667844 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667923 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.667999 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668059 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668122 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668277 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668366 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668462 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668547 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668617 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668679 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668757 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668837 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668922 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.668999 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669090 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669171 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669279 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669364 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669447 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669532 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669600 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669681 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669776 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669856 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.669940 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670012 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670100 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670179 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670260 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670338 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670424 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670500 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670870 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.670974 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671054 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671140 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671204 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671280 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671357 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671420 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671504 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671581 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671660 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671735 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671814 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671890 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.671968 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672043 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672121 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672194 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672272 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672326 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672405 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672480 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672558 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672633 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672710 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672783 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672862 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.672929 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673008 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673083 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673162 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673237 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673314 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673388 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673468 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673543 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673622 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673696 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673774 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673850 140198266307648 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.673924 140198266307648 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0316 00:19:14.674001 140198266307648 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0316 00:19:14.674220 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 00:19:14.674262 140198266307648 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6209 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.674448 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.0934 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.674609 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1794 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.674747 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4833 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.674884 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675018 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1691 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675162 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.8045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675302 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2352 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675450 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=3.1794 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675585 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2385 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675724 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.0422 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675852 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4615 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.675985 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7096 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676112 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1682 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676244 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.8622 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676373 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2427 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676517 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.1231 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676648 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4601 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676782 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7411 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.676907 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1678 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677036 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.0126 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677159 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2413 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677292 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.7556 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677417 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677551 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7155 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677673 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1226 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677803 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.0446 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.677930 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678061 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.7556 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678182 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678310 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678439 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3289 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678576 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7456 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678711 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1248 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678844 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.678972 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1785 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679106 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5405 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679232 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3326 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679376 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.4780 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679508 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1293 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679640 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.8947 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679766 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1801 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.679897 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0777 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680022 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3322 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680150 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.8823 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680270 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1289 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680396 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.0652 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680551 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1804 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680726 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.9981 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.680865 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2443 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681003 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.5146 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681129 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1009 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681259 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.4269 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681383 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1335 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681511 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.9981 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681635 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1342 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681765 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.8910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.681891 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2381 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682039 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2849 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682194 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0939 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682341 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3253 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682475 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1286 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682614 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0530 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682740 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2385 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.682879 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.6954 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683018 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683154 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.3491 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683284 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1263 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683438 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5433 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683564 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2359 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683695 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.0556 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683820 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0863 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.683972 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.8631 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684093 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1249 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684230 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0032 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684361 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2355 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684494 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.9876 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684623 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0872 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684755 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.1040 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.684884 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1240 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685019 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8971 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685151 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685279 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.8087 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685415 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0881 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685548 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.6410 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685663 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685793 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1991 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.685918 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1718 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686048 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.9129 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686175 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0636 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686306 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=0.5088 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686443 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0969 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686578 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.1991 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686709 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.0995 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686842 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.4940 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.686971 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1675 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687108 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=0.7271 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687234 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0555 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687366 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=0.3184 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687493 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0909 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687622 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.0343 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687745 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687871 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=0.3540 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.687994 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0543 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.688122 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=0.2089 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.688246 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0856 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.688353 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1987 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:14.688817 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.34804   |\n",
      "|      Average Precision       |   0.41501   |\n",
      "|        Average Recall        |   0.34354   |\n",
      "|       Average F1 Score       |   0.32155   |\n",
      "|         Average Loss         |   2.3194    |\n",
      "|       Average Latency        |  64.013 ms  |\n",
      "|   Average GPU Power Usage    |  12.397 W   |\n",
      "| Inference Energy Consumption | 0.22044 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_0/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 00:19:26.629032 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6391 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.629631 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.629992 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9023 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.630237 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6213 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.630468 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.3481 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.630689 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2205 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.630962 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.3702 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.631172 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2984 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.631422 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.9023 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.631629 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3101 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.631843 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.4445 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.632047 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5761 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.632255 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.0230 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.632460 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2135 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.632671 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.1933 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.632871 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3063 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.633077 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2132 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.633280 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5998 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.633488 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0035 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.633691 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2165 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.633897 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7984 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.634099 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2941 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.634355 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.6350 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.634556 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4276 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.634760 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.1628 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.634964 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1594 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.635206 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8146 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.635409 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2249 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.635623 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.6350 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.635830 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2259 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.636044 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.1796 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.636250 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4217 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.636461 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.1455 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.636662 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1613 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.636864 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.5592 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.637061 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.637268 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=16.9094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.637465 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4279 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.637671 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.9656 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.637876 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1673 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.638076 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6130 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.638276 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2335 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.638478 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=17.7526 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.638676 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4191 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.638877 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0167 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.639105 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.639311 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.3750 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.639506 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2314 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.639710 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=17.9112 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.639907 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3124 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.640108 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.640303 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1327 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.640502 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7427 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.640700 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1733 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.640904 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=17.9112 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.641100 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1730 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.641303 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9204 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.641499 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3057 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.641705 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.6015 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.641902 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1255 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.642104 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.6674 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.642302 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1663 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.642503 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.5962 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.642707 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.642915 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5641 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.643113 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1192 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.643315 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6446 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.643660 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1625 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.643865 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.9102 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.644300 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3020 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.644521 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2134 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.644720 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1129 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.644919 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8553 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.645229 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1601 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.645494 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=16.9251 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.645714 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.645935 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1942 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.646128 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1150 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.646333 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.4964 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.646537 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1590 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.646736 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=19.9370 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.646937 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3026 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.647136 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8101 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.647337 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1176 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.647535 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9354 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.647734 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.647933 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=25.3066 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.648129 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2206 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.648330 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6768 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.648526 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0884 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.648730 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.0799 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.648957 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1267 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.649153 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=25.3066 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.649516 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1367 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.649755 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.7350 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.650026 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2153 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.650259 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.8128 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.650478 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0734 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.650706 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5667 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.651052 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.651353 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7145 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.651587 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.651818 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1127 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.652148 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0697 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.652508 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1730 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.652805 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1097 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.653120 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1987 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:26.653312 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-----------+\n",
      "|      Metric (Per Batch)      |   Value   |\n",
      "+------------------------------+-----------+\n",
      "| Average Validation Accuracy  |  0.50169  |\n",
      "|      Average Precision       |  0.48721  |\n",
      "|        Average Recall        |  0.49638  |\n",
      "|       Average F1 Score       |  0.48727  |\n",
      "|         Average Loss         |  1.4071   |\n",
      "|       Average Latency        | 57.164 ms |\n",
      "|   Average GPU Power Usage    | 13.666 W  |\n",
      "| Inference Energy Consumption | 0.217 mWh |\n",
      "+------------------------------+-----------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_1/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 00:19:37.136347 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0729 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.137036 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1434 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.137420 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.137759 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7317 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.138002 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4712 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.138234 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.138478 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.1553 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.138713 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3381 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.139008 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.0782 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.139344 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.139602 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0966 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.139906 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6409 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.140321 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3413 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.140621 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2516 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.140953 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7884 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.141395 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.141669 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.8201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.142091 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7277 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.142339 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.0390 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.142590 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2495 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.142826 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.5031 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.143055 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3415 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.143297 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=22.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.143528 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4870 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.143752 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0845 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.143976 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1903 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.144207 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.6474 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.144444 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2643 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.144690 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=22.4531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.144927 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2702 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.145172 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=24.7418 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.145419 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.145662 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.2840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.145903 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1912 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.146158 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.3033 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.146429 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2678 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.146694 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=28.9079 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.146993 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5081 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.147262 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.147521 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1980 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.147778 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.6356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.148076 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2851 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.148358 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=29.8839 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.148628 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4763 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.148936 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8233 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.149212 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1966 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.149473 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.4463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.149756 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.150029 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=31.6764 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.150286 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3686 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.150559 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.6004 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.150813 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.151059 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0624 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.151297 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2034 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.151567 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=31.6764 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.151854 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2065 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.152179 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=19.1974 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.152419 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3638 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.152656 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.3430 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.152881 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1603 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.153099 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.5094 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.153305 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.153512 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=21.6076 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.153718 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3667 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.153933 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.7755 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.154138 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1507 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.154348 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.5313 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.154550 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1938 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.154757 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=24.8754 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.154957 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3551 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.155164 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.3014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.155363 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1371 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.155569 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9766 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.155767 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1937 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.155974 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=28.0455 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.156172 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.156379 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.0910 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.156583 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1403 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.156787 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.8549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.156985 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.157192 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=31.9172 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.157386 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3612 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.157592 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7549 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.157794 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1452 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.158005 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9792 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.158213 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1868 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.158426 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=38.3459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.158627 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2594 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.158838 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.2449 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.159640 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1145 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.159864 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7313 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.160098 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1560 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.160307 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=38.3459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.160510 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1805 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.160720 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9076 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.160928 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2580 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.161248 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5273 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.161534 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0931 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.161759 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8589 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.161973 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1434 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.162191 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.1145 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.162450 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2473 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.162683 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2348 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.162899 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0832 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.163109 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3000 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.163321 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1302 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.163508 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1987 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:37.163656 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.5033    |\n",
      "|      Average Precision       |   0.48979   |\n",
      "|        Average Recall        |   0.49979   |\n",
      "|       Average F1 Score       |   0.49025   |\n",
      "|         Average Loss         |   1.4046    |\n",
      "|       Average Latency        |  64.344 ms  |\n",
      "|   Average GPU Power Usage    |  14.008 W   |\n",
      "| Inference Energy Consumption | 0.25038 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_2/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 00:19:49.091264 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:49.260666 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:49.535495 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:49.704479 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7290 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:49.873801 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8278 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:50.048572 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2524 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:50.295318 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7123 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:50.474045 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3817 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:50.758733 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.3180 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:50.937910 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3635 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:51.165490 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1696 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:51.338655 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6649 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:51.625675 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3855 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:51.814132 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2461 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:52.145130 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.4508 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:52.331637 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3588 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:52.635814 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=17.1778 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:52.888553 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.7049 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:53.203155 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.3850 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:53.390253 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2527 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:53.666959 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.7655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:53.849574 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3412 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:54.137921 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=25.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:54.357242 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4854 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:54.669454 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:54.844018 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1893 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:55.186453 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.7741 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:55.357530 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:55.806314 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=25.2463 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:55.978840 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2760 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:56.353723 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=28.0125 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:56.527749 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:56.903701 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=16.8105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:57.079061 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1964 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:57.399263 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=16.0114 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:57.610242 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2655 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:58.005575 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=33.8818 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:58.184280 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5068 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:58.493783 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8344 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:58.676519 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2001 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:58.955172 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.0198 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:59.129067 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2850 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:59.425939 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=35.0847 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:59.609560 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4743 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:19:59.894570 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.9367 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:00.120737 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1998 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:00.491396 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.6592 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:00.699966 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2653 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:01.006755 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=36.8590 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:01.186798 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3579 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:01.479139 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8783 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:01.658253 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1759 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:01.962423 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.0121 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:02.150033 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2110 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:02.465028 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=36.8590 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:02.656095 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:03.003158 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=20.4300 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:03.215480 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:03.626549 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.4453 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:03.816702 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1672 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:04.153212 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.0076 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:04.327528 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2111 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:04.721445 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=23.5607 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:04.899619 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3707 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:05.270892 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4421 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:05.453729 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1578 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:05.806906 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.9307 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:06.012976 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1978 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:06.410634 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=28.0637 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:06.602494 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3611 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:07.022872 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.8311 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:07.196784 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:07.535671 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.5414 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:07.744922 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1972 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:08.243601 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=33.9495 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:08.465195 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3573 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:08.903701 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.3418 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:09.130017 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1516 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:09.524259 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1224 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:09.832487 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1878 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:10.233095 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=36.4884 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:10.433592 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3619 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:10.876182 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.1171 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:11.211017 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1526 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:11.742671 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1036 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:12.016819 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1912 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:12.509276 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=42.3829 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:12.731818 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2639 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:13.308774 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.3254 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:13.566233 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1234 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:13.967628 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5791 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:14.208919 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:14.629916 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=42.3829 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:14.864308 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1986 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:15.219466 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.3225 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:15.419003 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2633 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:15.683652 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4243 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:15.884415 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0994 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:16.144639 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7869 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:16.455759 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1521 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:17.035327 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=18.3133 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:17.325321 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2501 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:17.780769 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1514 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:18.182470 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0833 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:18.650334 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.2651 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:18.963210 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1311 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:18.965363 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1987 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:18.965651 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.50422   |\n",
      "|      Average Precision       |   0.48985   |\n",
      "|        Average Recall        |   0.4999    |\n",
      "|       Average F1 Score       |   0.49014   |\n",
      "|         Average Loss         |   1.4036    |\n",
      "|       Average Latency        |  135.06 ms  |\n",
      "|   Average GPU Power Usage    |  12.572 W   |\n",
      "| Inference Energy Consumption | 0.47165 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0316 00:20:43.785732 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1200 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:45.360037 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1373 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:47.641460 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3588 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:49.291201 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6216 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:51.048172 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.3557 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:52.677744 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2492 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:54.376814 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.4801 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:55.934077 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3377 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:58.208860 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.3588 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:20:59.847970 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:01.736824 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0114 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:03.439734 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6412 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:05.468186 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.7525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:07.203966 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2502 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:09.352989 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5548 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:11.049577 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3600 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:12.992644 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=19.2721 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:14.672056 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.6715 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:16.636445 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:18.344503 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2616 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:20.494035 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.4742 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:22.118396 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3318 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:24.141087 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=26.2236 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:25.777565 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:28.033117 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.2879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:29.741058 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2008 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:32.536902 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.2161 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:33.993993 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2654 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:35.843324 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=26.2236 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:37.537308 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:40.443487 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=27.9810 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:41.961833 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5268 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:44.798911 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=17.1537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:46.367068 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2190 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:48.735932 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.7210 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:50.321008 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2823 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:52.738744 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=23.5832 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:54.150056 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.5929 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:56.149568 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.9023 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:57.744093 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2108 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:21:59.309642 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8660 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:00.875789 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2827 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:02.599946 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=28.1062 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:04.351040 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4839 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:06.188158 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8401 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:07.744592 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2237 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:09.568808 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.4775 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:11.023987 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2838 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:12.761454 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=24.7662 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:14.226279 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3908 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:15.941386 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.8085 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:17.349763 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2099 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:19.322863 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8360 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:20.819419 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2247 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:22.636828 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=24.7662 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:24.176941 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2287 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:26.502146 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=21.6528 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:28.094572 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:31.342116 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.7770 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:32.951150 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1857 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:35.371653 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=15.1173 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:36.966300 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2580 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:40.078144 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=22.2460 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:42.063330 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4097 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:45.014184 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.2617 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:46.617169 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1726 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:49.115155 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8156 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:50.836025 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2086 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:53.758745 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=28.3730 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:55.307920 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3955 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:58.264817 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.5006 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:22:59.784086 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1799 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:02.076988 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.9349 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:03.664169 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.3.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2115 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:06.813440 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=31.2075 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:08.374948 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4123 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:10.929445 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.6301 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:12.465644 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1722 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:14.789113 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.8367 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:16.537878 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.4.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2090 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:18.955716 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=34.9347 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:20.646914 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3907 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:23.421628 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.3869 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:25.026482 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1826 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:27.222027 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.4715 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:28.770111 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.5.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:31.119921 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=41.1207 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:32.737955 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3037 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:34.964958 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7967 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:36.518210 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1487 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:38.164092 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7390 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:39.733088 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1865 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:42.394608 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=41.1207 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:44.078305 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:45.935914 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.0342 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:47.524752 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3091 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:48.987943 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.7500 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:50.569336 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1166 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:51.829872 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.9505 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:53.324780 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1882 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:55.053142 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.8521 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:56.598317 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2816 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:57.854403 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.1860 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:23:59.444580 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1004 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:24:00.626413 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=1.4707 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:24:02.206614 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.2.conv3._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1511 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:24:02.207518 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=25.1987 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0316 00:24:02.207819 140198266307648 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0533, 0.1169](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.50165   |\n",
      "|      Average Precision       |   0.48739   |\n",
      "|        Average Recall        |   0.49638   |\n",
      "|       Average F1 Score       |   0.48728   |\n",
      "|         Average Loss         |   1.4094    |\n",
      "|       Average Latency        |  132.53 ms  |\n",
      "|   Average GPU Power Usage    |  13.819 W   |\n",
      "| Inference Energy Consumption | 0.50871 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_4/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0316 00:24:24.870569 140198266307648 rank_zero.py:63] GPU available: True (cuda), used: True\n",
      "I0316 00:24:24.870985 140198266307648 rank_zero.py:63] TPU available: False, using: 0 TPU cores\n",
      "I0316 00:24:24.871046 140198266307648 rank_zero.py:63] HPU available: False, using: 0 HPUs\n",
      "I0316 00:24:27.655450 140198266307648 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 00:24:27.669380 140198266307648 model_summary.py:104] \n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | GraphModule        | 23.5 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | acc_train | MulticlassAccuracy | 0      | train\n",
      "3 | loss_val  | MeanMetric         | 0      | train\n",
      "4 | loss_test | MeanMetric         | 0      | train\n",
      "---------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "263       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0: 100%|█| 782/782 [02:37<00:00,  4.97it/s, v_num=0, train_acc_step=0.500]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<01:04,  2.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:41,  3.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:35,  4.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:31,  4.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:28,  5.37it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:01<00:28,  5.31it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:01<00:26,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:01<00:25,  5.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:01<00:25,  5.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:01<00:24,  5.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:01<00:23,  6.22it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:01<00:23,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:02<00:23,  6.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:02<00:22,  6.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:02<00:22,  6.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:02<00:21,  6.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:02<00:21,  6.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:02<00:21,  6.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:02<00:21,  6.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:03<00:21,  6.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:03<00:20,  6.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:03<00:20,  6.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:03<00:20,  6.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:03<00:20,  6.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:03<00:19,  6.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:03<00:19,  6.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:04<00:19,  6.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:04<00:19,  6.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:04<00:19,  6.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:04<00:19,  6.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:04<00:18,  6.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:04<00:18,  6.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:04<00:18,  6.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:05<00:18,  6.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:05<00:18,  6.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:05<00:17,  6.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:05<00:17,  6.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:05<00:17,  6.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:05<00:17,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:05<00:17,  6.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:06<00:17,  6.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:06<00:16,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:06<00:16,  6.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:06<00:16,  6.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:06<00:16,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:06<00:16,  6.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:06<00:16,  6.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:07<00:16,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:07<00:15,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:07<00:15,  6.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:07<00:15,  6.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:07<00:15,  6.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:07<00:15,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:07<00:15,  6.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:08<00:14,  6.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:08<00:14,  6.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:08<00:14,  6.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:08<00:14,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:08<00:14,  6.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:08<00:14,  6.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:08<00:13,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:09<00:13,  6.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:09<00:13,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:09<00:13,  6.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:09<00:13,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:09<00:13,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:09<00:13,  6.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:09<00:13,  6.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:10<00:12,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:10<00:12,  6.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:10<00:12,  6.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:10<00:12,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:10<00:12,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:10<00:12,  6.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:10<00:11,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:11<00:11,  6.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:11<00:11,  6.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:11<00:11,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:11<00:11,  6.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:11<00:11,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:11<00:11,  6.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:11<00:10,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:12<00:10,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:12<00:10,  6.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:12<00:10,  6.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:12<00:10,  6.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:12<00:10,  6.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:12<00:10,  6.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:12<00:09,  6.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:13<00:09,  6.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:13<00:09,  6.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:13<00:09,  6.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:13<00:09,  6.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:13<00:09,  6.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:13<00:08,  6.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:13<00:08,  6.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:14<00:08,  6.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:14<00:08,  6.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:14<00:08,  6.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:14<00:08,  6.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:14<00:08,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:14<00:07,  6.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:14<00:07,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:14<00:07,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:15<00:07,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:15<00:07,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:15<00:07,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:15<00:07,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:15<00:06,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:15<00:06,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:15<00:06,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:16<00:06,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:16<00:06,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:16<00:06,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:16<00:06,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:16<00:05,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:16<00:05,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:16<00:05,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:16<00:05,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:17<00:05,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:17<00:05,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:17<00:04,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:17<00:04,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:17<00:04,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:17<00:04,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:17<00:04,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:18<00:04,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:18<00:04,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:18<00:03,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:18<00:03,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:18<00:03,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:18<00:03,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:18<00:03,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:19<00:03,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:19<00:03,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:19<00:02,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:19<00:02,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:19<00:02,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:19<00:02,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:19<00:02,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:20<00:02,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:20<00:02,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:20<00:01,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:20<00:01,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:20<00:01,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:20<00:01,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:20<00:01,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:20<00:01,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:21<00:01,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:21<00:00,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:21<00:00,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:21<00:00,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:21<00:00,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:21<00:00,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:21<00:00,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:22<00:00,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:22<00:00,  7.09it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [02:36<00:00,  4.99it/s, v_num=0, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:39,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:27,  5.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:24,  6.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:26,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:23,  6.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:24,  6.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:01<00:23,  6.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:01<00:22,  6.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:01<00:22,  6.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:01<00:22,  6.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:01<00:21,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:01<00:21,  6.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:01<00:21,  6.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:02<00:20,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:02<00:21,  6.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:02<00:20,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:02<00:19,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:02<00:20,  6.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:02<00:19,  6.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:02<00:19,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:03<00:19,  6.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:03<00:19,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:03<00:19,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:03<00:19,  6.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:03<00:18,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:03<00:18,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:03<00:18,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:03<00:18,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:04<00:17,  7.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:04<00:18,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:04<00:17,  7.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:04<00:17,  7.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:04<00:17,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:04<00:17,  7.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:04<00:17,  7.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:05<00:17,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:05<00:16,  7.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:05<00:16,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:05<00:16,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:05<00:16,  7.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:05<00:16,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:05<00:16,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:06<00:15,  7.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:06<00:16,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:06<00:15,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:06<00:15,  7.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:06<00:15,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:06<00:15,  7.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:06<00:15,  7.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:07<00:15,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:07<00:14,  7.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:07<00:14,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:07<00:14,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:07<00:14,  7.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:07<00:14,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:07<00:14,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:08<00:14,  7.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:08<00:14,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:08<00:13,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:08<00:13,  7.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:08<00:13,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:08<00:13,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:08<00:13,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:09<00:13,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:09<00:13,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:09<00:13,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:09<00:12,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:09<00:12,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:09<00:12,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:10<00:12,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:10<00:12,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:10<00:12,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:10<00:12,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:10<00:11,  6.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:10<00:11,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:10<00:11,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:11<00:11,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:11<00:11,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:11<00:11,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:11<00:11,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:11<00:10,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:11<00:10,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:11<00:10,  6.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:12<00:10,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:12<00:10,  6.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:12<00:10,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:12<00:10,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:12<00:09,  6.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:12<00:09,  6.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:12<00:09,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:13<00:09,  6.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:13<00:09,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:13<00:09,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:13<00:09,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:13<00:08,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:13<00:08,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:13<00:08,  6.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:13<00:08,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:14<00:08,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:14<00:08,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:14<00:07,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:14<00:07,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:14<00:07,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:14<00:07,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:14<00:07,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:15<00:07,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:15<00:07,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:15<00:06,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:15<00:06,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:15<00:06,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:15<00:06,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:15<00:06,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:16<00:06,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:16<00:06,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:16<00:05,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:16<00:05,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:16<00:05,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:16<00:05,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:16<00:05,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:17<00:05,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:17<00:05,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:17<00:05,  7.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:17<00:04,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:17<00:04,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:17<00:04,  7.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:17<00:04,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:18<00:04,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:18<00:04,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:18<00:03,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:18<00:03,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:18<00:03,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:18<00:03,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:18<00:03,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:19<00:03,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:19<00:03,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:19<00:02,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:19<00:02,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:19<00:02,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:19<00:02,  7.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:19<00:02,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:20<00:02,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:20<00:02,  7.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:20<00:01,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:20<00:01,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:20<00:01,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:20<00:01,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:20<00:01,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:21<00:01,  7.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:21<00:01,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:21<00:00,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:21<00:00,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:21<00:00,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:21<00:00,  7.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:21<00:00,  7.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:21<00:00,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:22<00:00,  7.08it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:22<00:00,  7.08it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [02:59<00:00,  4.36it/s, v_num=0, train_acc_step=0.250,\u001b[AI0316 00:30:29.725321 140198266307648 rank_zero.py:63] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Epoch 1: 100%|█| 782/782 [03:00<00:00,  4.34it/s, v_num=0, train_acc_step=0.250,\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_1/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_2/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_3/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.53153   |\n",
      "|      Average Precision       |   0.57969   |\n",
      "|        Average Recall        |   0.53125   |\n",
      "|       Average F1 Score       |   0.54146   |\n",
      "|         Average Loss         |   1.3225    |\n",
      "|       Average Latency        |  310.7 ms   |\n",
      "|   Average GPU Power Usage    |  11.536 W   |\n",
      "| Inference Energy Consumption | 0.99562 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_5/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50-trt_quantized\u001b[0m\n",
      "[03/16/2025-00:40:02] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.51901   |\n",
      "|      Average Precision       |   0.51396   |\n",
      "|        Average Recall        |   0.52022   |\n",
      "|       Average F1 Score       |   0.51189   |\n",
      "|         Average Loss         |    1.344    |\n",
      "|       Average Latency        |  24.97 ms   |\n",
      "|   Average GPU Power Usage    |  20.918 W   |\n",
      "| Inference Energy Consumption | 0.14509 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/tensorrt/version_0/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_INT8_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet50_INT8_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_INT8_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 01:04:00.709789 140315264521280 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet50_cls_ci | e_output/resnet50_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-15/softwar | far10_2025-03-15/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet50   |                          |         resnet50         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet50'...\u001b[0m\n",
      "self.args.model is resnet50\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet50'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet50', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mprecision=fp16, skipping int8 calibration/fine-tune...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_4/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_5/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_6/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.51486    |\n",
      "|      Average Precision       |   0.55863    |\n",
      "|        Average Recall        |   0.51562    |\n",
      "|       Average F1 Score       |   0.52506    |\n",
      "|         Average Loss         |    1.342     |\n",
      "|       Average Latency        |  23.828 ms   |\n",
      "|   Average GPU Power Usage    |   14.699 W   |\n",
      "| Inference Energy Consumption | 0.097293 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_6/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50-trt_quantized\u001b[0m\n",
      "[03/16/2025-01:04:40] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50-trt_quantized:\n",
      "+------------------------------+---------------+\n",
      "|      Metric (Per Batch)      |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.52117    |\n",
      "|      Average Precision       |    0.51545    |\n",
      "|        Average Recall        |    0.52298    |\n",
      "|       Average F1 Score       |    0.51427    |\n",
      "|         Average Loss         |    1.5778     |\n",
      "|       Average Latency        |   1.5498 ms   |\n",
      "|   Average GPU Power Usage    |   11.435 W    |\n",
      "| Inference Energy Consumption | 0.0049227 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/tensorrt/version_1/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_FP16_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet50_FP16_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_FP16_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 01:09:12.493298 140488866358336 seed.py:57] Seed set to 0\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |     cls      |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           |              | /workspace/ADLS_Proj/mas | /workspace/ADLS_Proj/mas |\n",
      "|                         |                          |              | e_output/resnet50_cls_ci | e_output/resnet50_cls_ci |\n",
      "|                         |                          |              | far10_2025-03-15/softwar | far10_2025-03-15/softwar |\n",
      "|                         |                          |              | e/training_ckpts/best.ck | e/training_ckpts/best.ck |\n",
      "|                         |                          |              |            pt            |            pt            |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |              |            pl            |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |      64      |                          |            64            |\n",
      "| to_debug                |          False           |              |                          |          False           |\n",
      "| log_level               |           info           |              |                          |           info           |\n",
      "| report_to               |       tensorboard        |              |                          |       tensorboard        |\n",
      "| seed                    |            0             |              |                          |            0             |\n",
      "| quant_config            |           None           |              |                          |           None           |\n",
      "| training_optimizer      |           adam           |              |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |              |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |            0             |              |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |      10      |                          |            10            |\n",
      "| max_steps               |            -1            |              |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |              |                          |            1             |\n",
      "| log_every_n_steps       |            50            |              |                          |            50            |\n",
      "| num_workers             |            20            |              |                          |            20            |\n",
      "| num_devices             |            1             |              |                          |            1             |\n",
      "| num_nodes               |            1             |              |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |     gpu      |                          |           gpu            |\n",
      "| strategy                |           auto           |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |              |                          |          False           |\n",
      "| github_ci               |          False           |              |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |              |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |              |                          |           100            |\n",
      "| is_pretrained           |          False           |              |                          |          False           |\n",
      "| max_token_len           |           512            |              |                          |           512            |\n",
      "| project_dir             | /workspace/ADLS_Proj/mas |              |                          | /workspace/ADLS_Proj/mas |\n",
      "|                         |         e_output         |              |                          |         e_output         |\n",
      "| project                 |           None           |              |                          |           None           |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |   resnet50   |                          |         resnet50         |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |            20            |              |                          |            20            |\n",
      "| eta_min                 |          1e-06           |              |                          |          1e-06           |\n",
      "+-------------------------+--------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet50'...\u001b[0m\n",
      "self.args.model is resnet50\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet50'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet50', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 0, 'post_calibration_analysis': False, 'default': {'config': {'quantize': False, 'precision': 'fp32'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': False}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\u001b[0m\n",
      "using safe deepcopy\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mprecision=fp32, skipping int8 calibration/fine-tune...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_7/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_8/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /workspace/ADLS_Proj/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/2025-03-16/version_9/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRunning minimal runtime analysis on original graph (just 1 batch) ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.51486   |\n",
      "|      Average Precision       |   0.55863   |\n",
      "|        Average Recall        |   0.51562   |\n",
      "|       Average F1 Score       |   0.52506   |\n",
      "|         Average Loss         |    1.342    |\n",
      "|       Average Latency        |  30.144 ms  |\n",
      "|   Average GPU Power Usage    |  18.065 W   |\n",
      "| Inference Energy Consumption | 0.15127 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/mase_graph/version_7/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mMeasuring final TensorRT engine with warmup=20, test=500 ...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet50-trt_quantized\u001b[0m\n",
      "[03/16/2025-01:09:33] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet50-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.52034    |\n",
      "|      Average Precision       |   0.51452    |\n",
      "|        Average Recall        |   0.52217    |\n",
      "|       Average F1 Score       |   0.51342    |\n",
      "|         Average Loss         |    1.5777    |\n",
      "|       Average Latency        |  3.0873 ms   |\n",
      "|   Average GPU Power Usage    |   16.157 W   |\n",
      "| Inference Energy Consumption | 0.013856 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /workspace/mase_output/tensorrt/quantization/resnet50_cls_cifar10_2025-03-16/tensorrt/version_2/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-16/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RES_FP32_BY_TYPE_TOML = \"/workspace/ADLS_Proj/docs/tutorials/proj/resnet50_FP32_quant.toml\"\n",
    "RES_CHECKPOINT_PATH = \"/workspace/ADLS_Proj/mase_output/resnet50_cls_cifar10_2025-03-15/software/training_ckpts/best.ckpt\"\n",
    "!python ch transform --config {RES_FP32_BY_TYPE_TOML} --load {RES_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `fp16` acheives a slighty higher test accuracy but a slightly lower latency (~30%) from that of int8 quantization; it is still ~2.5x faster than the unquantized model. Now lets apply quantization to a more complicated model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
