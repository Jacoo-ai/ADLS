{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Quantization Tutorial\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE as part of the MASERT framework. The following demonstrations were run on a NVIDIA RTX A2000 GPU with a Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz CPU.\n",
    "\n",
    "## Section 1. INT8 Quantization\n",
    "Firstly, we will show you how to do a int8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each using their own individual pass, and are explained in depth at each subsection:\n",
    "\n",
    "1. [Fake quantization](#section-11-fake-quantization): `tensorrt_fake_quantize_transform_pass`\n",
    "2. [Calibration](#section-12-calibration): `tensorrt_calibrate_transform_pass`\n",
    "3. [Quantized Aware Training](#section-13-quantized-aware-training-qat): `tensorrt_fine_tune_transform_pass`\n",
    "4. [Quantization](#section-14-tensorrt-quantization): `tensorrt_engine_interface_pass`\n",
    "5. [Analysis](#section-15-performance-analysis): `tensorrt_analysis_pass`\n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"src\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.utils import to_numpy_if_tensor\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.tools import get_cf_args, get_dummy_input\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model, get_tokenizer\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
    "from chop.passes.graph import (\n",
    "    summarize_quantization_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    runtime_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dependency (the dependent package \"cuda\" refers to \"cuda-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mExtension: All dependencies for TensorRT pass are available.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chop.tools.check_dependency import check_deps_tensorRT_pass\n",
    "check_deps_tensorRT_pass(silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}\n",
      "{'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "# Path to your TOML file\n",
    "# JSC_TOML_PATH = 'toy_INT8_quantization_by_type.toml'\n",
    "JSC_TOML_PATH = 'resnet18_INT8_quantization_by_type.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(JSC_TOML_PATH, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt' section and its children\n",
    "tensorrt_config = pass_args.get('passes', {}).get('tensorrt', {})\n",
    "print(tensorrt_config)\n",
    "# Extract the 'passes.runtime_analysis' section and its children\n",
    "runtime_analysis_config = pass_args.get('passes', {}).get('tensorrt', {}).get('runtime_analysis', {})\n",
    "print(runtime_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a model and training it using the toml configuration model arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srcPkgs/miniconda3/envs/mase/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/srcPkgs/miniconda3/envs/mase/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/srcPkgs/miniconda3/envs/mase/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/srcPkgs/miniconda3/envs/mase/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "from chop.dataset import MaseDataModule\n",
    "from chop.models import get_model_info\n",
    "from chop.models import get_model\n",
    "from chop.tools.get_input import InputGenerator\n",
    "\n",
    "# Load the basics in\n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Add the data_module and other necessary information to the configs\n",
    "configs = [tensorrt_config, runtime_analysis_config]\n",
    "for config in configs:\n",
    "    config['task'] = pass_args['task']\n",
    "    config['dataset'] = pass_args['dataset']\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    # task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the `jsc-toy` model using the machop `train` action with the config from the toml file.\n",
    "\n",
    "Run the following command to train a jsc-toy.\n",
    "\n",
    "```bash\n",
    "cd machop\n",
    "./ch train --config ./configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load in the checkpoint. You will have to adjust this according to where it has been stored in the mase_output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05/software/training_ckpts/best.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done!\n",
      "dummy in done\n",
      "_ done\n",
      "init_metadata_analysis_pass done\n",
      "add_common_metadata_analysis_pass done\n",
      "add_software_metadata_analysis_pass done\n",
      "metadata_value_type_cast_transform_pass done\n",
      "deep copy done\n"
     ]
    }
   ],
   "source": [
    "# Load in the trained checkpoint - change this accordingly\n",
    "JSC_CHECKPOINT_PATH = \"/root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "\n",
    "model = load_model(load_name=JSC_CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "print(\"load model done!\")\n",
    "\n",
    "# Initiate metadata\n",
    "dummy_in = next(iter(input_generator))\n",
    "print(\"dummy in done\")\n",
    "\n",
    "_ = model(**dummy_in)\n",
    "print(\"_ done\")\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "print(\"init_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "print(\"add_common_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "print(\"add_software_metadata_analysis_pass done\")\n",
    "\n",
    "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})\n",
    "print(\"metadata_value_type_cast_transform_pass done\")\n",
    "\n",
    "# Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
    "mg_original = deepcopy_mase_graph(mg)\n",
    "print(\"deep copy done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1 Fake Quantization\n",
    "\n",
    "Firstly, we fake quantize the module in order to perform calibration and fine tuning before actually quantizing - this is only used if we have int8 calibration as other precisions are not currently supported within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library.\n",
    "\n",
    "This is acheived through the `tensorrt_fake_quantize_transform_pass` which goes through the model, either by type or by name, replaces each layer appropriately to a fake quantized form if the `quantize` parameter is set in the default config (`passes.tensorrt.default.config`) or on a per name or type basis. \n",
    "\n",
    "Currently the quantizable layers are:\n",
    "- Linear\n",
    "- Conv1d, Conv2d, Conv3d \n",
    "- ConvTranspose1d, ConvTranspose2d, ConvTranspose3d \n",
    "- MaxPool1d, MaxPool2d, MaxPool3d\n",
    "- AvgPool1d, AvgPool2d, AvgPool3d\n",
    "- LSTM, LSTMCell\n",
    "\n",
    "To create a custom quantized module, click [here](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#document-tutorials/creating_custom_quantized_modules).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op is placeholder\n",
      "placeholder not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is max_pool2d\n",
      "max_pool2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is conv2d\n",
      "node.op == call_module\n",
      "op is batch_norm2d\n",
      "batch_norm2d not in QUANTIZEABLE_OP\n",
      "op is add\n",
      "add not in QUANTIZEABLE_OP\n",
      "op is relu\n",
      "relu not in QUANTIZEABLE_OP\n",
      "op is adaptive_avg_pool2d\n",
      "adaptive_avg_pool2d not in QUANTIZEABLE_OP\n",
      "op is flatten\n",
      "flatten not in QUANTIZEABLE_OP\n",
      "op is linear\n",
      "node.op == call_module\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op is output\n",
      "output not in QUANTIZEABLE_OP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type     | OP                  |   Total |   Changed |   Unchanged |\n",
      "|-------------------+---------------------+---------+-----------+-------------|\n",
      "| AdaptiveAvgPool2d | adaptive_avg_pool2d |       1 |         0 |           1 |\n",
      "| BatchNorm2d       | batch_norm2d        |      20 |         0 |          20 |\n",
      "| MaxPool2d         | max_pool2d          |       1 |         0 |           1 |\n",
      "| QuantConv2d       | conv2d              |      20 |         0 |          20 |\n",
      "| QuantLinear       | linear              |       1 |         0 |           1 |\n",
      "| ReLU              | relu                |      17 |         0 |          17 |\n",
      "| add               | add                 |       8 |         0 |           8 |\n",
      "| flatten           | flatten             |       1 |         0 |           1 |\n",
      "| output            | output              |       1 |         0 |           1 |\n",
      "| x                 | placeholder         |       1 |         0 |           1 |\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<chop.ir.graph.mase_graph.MaseGraph at 0x7f9a47445b50>, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_config)\n",
    "# summarize_quantization_analysis_pass(mg_original, mg)\n",
    "summarize_quantization_analysis_pass(mg, pass_args={\"save_dir\": \"quantize_summary_res18\", \"original_graph\": mg_original})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have succesfully fake quantized all linear layers inside `jsc-toy`. This means that we will be able to simulate a quantized model in order to calibrate and fine tune it. This fake quantization was done on typewise i.e. for linear layers only. See [Section 4](#section-4-layer-wise-mixed-precision) for how to apply quantization layerwise - i.e. only first and second layers for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2 Calibration\n",
    "\n",
    "Next, we perform calibration using the `tensorrt_calibrate_transform_pass`. Calibration is achieved by passing data samples to the quantizer and deciding the best amax for activations. \n",
    "\n",
    "Calibrators can be added as a search space parameter to examine the best performing calibrator. The calibrators have been included in the toml as follows.\n",
    "For example: `calibrators = [\"percentile\", \"mse\", \"entropy\"]`\n",
    "\n",
    "Note: \n",
    "- To use `percentile` calibration, a list of percentiles must be given\n",
    "- To use `max` calibration, the `histogram` weight and input calibrators must be removed and replaced with `max`. This will use global maximum absolute value to calibrate the model.\n",
    "- If `post_calibration_analysis` is set true the `tensorrt_analysis_pass` will be run for each calibrator tested to evaluate the most suitable calibrator for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0305 18:50:17.693002 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.695286 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.697411 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.700781 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.703452 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.705822 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.707861 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.709778 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.713432 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.715162 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.717043 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.719617 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.722004 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.725242 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.726795 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.729634 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.732314 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.734586 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.737632 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.739554 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.741648 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.744894 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.747350 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.749489 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.751543 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.754321 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.756094 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.758993 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.761053 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.763150 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.764901 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.766889 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.768575 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.770701 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.773803 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.775705 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.777749 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.782793 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.785251 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.787841 140311821633344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.790901 140311821633344 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "W0305 18:50:17.793255 140311821633344 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0305 18:50:17.818785 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0305 18:50:17.820130 140311821633344 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6388 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.824271 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1424 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.831166 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.834392 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2066 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.838692 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.0555 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.842830 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2004 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.849139 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8624 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.853614 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1951 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.857262 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.0898 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.861439 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1975 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.865893 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4642 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.869488 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.874239 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2541 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.877927 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.881978 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.4642 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.885890 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3464 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.892669 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.6459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.896919 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.901035 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.2822 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.904641 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1509 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.909377 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.5816 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.913299 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1268 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.917605 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.7209 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.921640 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1127 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.925820 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=4.5816 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.932454 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2544 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.937406 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4493 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.941025 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0952 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.944940 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=2.4684 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.949112 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0938 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.952805 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.3791 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.957238 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0700 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.961829 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.7229 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.965539 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0594 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.971780 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=5.3791 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.975230 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.1924 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.979440 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.0820 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.984035 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0552 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.992106 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.7297 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.996938 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0559 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:17.999248 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=17.8715 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:18.002455 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0803, 0.1389](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72101   |\n",
      "|      Average Precision       |   0.7109    |\n",
      "|        Average Recall        |   0.71461   |\n",
      "|       Average F1 Score       |   0.71167   |\n",
      "|         Average Loss         |   0.82877   |\n",
      "|       Average Latency        |  81.558 ms  |\n",
      "|   Average GPU Power Usage    |  20.833 W   |\n",
      "| Inference Energy Consumption | 0.47198 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_6/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0305 18:50:34.943918 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.6388 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.947658 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.1835 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.951739 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.955371 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.959790 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.1452 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.963426 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2521 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.968325 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.2936 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.972647 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2550 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.977403 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.0966 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.982228 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2554 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.991856 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.0459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:34.999135 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2054 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.007616 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.2629 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.012866 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2048 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.019821 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.0459 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.024585 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4511 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.030073 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.2740 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.036216 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1998 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.040765 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=3.4147 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.045341 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1932 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.050301 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8758 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.054884 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1659 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.060402 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2817 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.064506 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1576 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.069237 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=6.8758 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.073606 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3275 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.078841 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7086 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.087902 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1261 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.092066 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4574 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.096219 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1270 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.101799 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.6725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.106164 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.110378 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5604 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.114321 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0825 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.118608 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=8.6725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.127397 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.2534 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.132843 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.5806 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.137514 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0715 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.142789 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.8894 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.150294 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0725 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.153597 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=17.8715 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:35.156471 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0803, 0.1389](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73017   |\n",
      "|      Average Precision       |   0.72055   |\n",
      "|        Average Recall        |   0.7232    |\n",
      "|       Average F1 Score       |   0.72112   |\n",
      "|         Average Loss         |   0.8176    |\n",
      "|       Average Latency        |  79.954 ms  |\n",
      "|   Average GPU Power Usage    |  21.085 W   |\n",
      "| Inference Energy Consumption | 0.46829 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_7/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0305 18:50:51.306876 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.0361 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.311694 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2210 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.315988 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8498 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.321718 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4138 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.327359 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2439 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.332719 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.342901 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.8831 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.350607 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3187 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.361726 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.1231 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.366797 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3286 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.372120 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.7363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.381216 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.386834 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.2525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.391684 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.400207 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.7363 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.405901 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5654 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.410970 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.8155 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.414990 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2409 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.420547 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.4843 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.424969 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2298 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.430110 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.2737 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.434860 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1996 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.439594 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.7940 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.444619 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2048 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.449617 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=9.2737 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.458280 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.463817 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8708 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.468837 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1605 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.473879 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.5791 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.478240 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.483472 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.3490 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.487654 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1343 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.492736 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3385 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.501243 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1082 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.506352 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.3490 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.511399 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3017 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.515722 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.1903 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.521168 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0873 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.525574 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.4024 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.530083 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0876 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.533111 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=17.8715 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:50:51.536264 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0803, 0.1389](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72992   |\n",
      "|      Average Precision       |   0.71925   |\n",
      "|        Average Recall        |   0.72154   |\n",
      "|       Average F1 Score       |   0.71972   |\n",
      "|         Average Loss         |   0.81841   |\n",
      "|       Average Latency        |  77.66 ms   |\n",
      "|   Average GPU Power Usage    |  21.331 W   |\n",
      "| Inference Energy Consumption | 0.46015 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_8/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0305 18:51:09.962239 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=3.3206 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:11.847253 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2136 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:14.751762 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.0056 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:16.573871 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4146 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:18.706563 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9940 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:20.600903 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3165 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:22.985134 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.4188 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:24.818100 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3198 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:26.861742 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.6788 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:28.663590 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3226 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:30.825725 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.2026 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:32.627603 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2509 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:35.367842 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.8584 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:37.174878 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2578 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:39.328263 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=12.2026 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:41.174994 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.5454 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:43.434257 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7010 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:45.253548 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2407 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:47.382011 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=4.9033 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:49.220044 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2311 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:51.187270 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.5348 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:53.007813 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2002 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:55.225055 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.3611 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:57.046745 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2105 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:51:58.999634 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=10.5348 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:00.871752 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3925 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:03.117228 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.5768 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:04.907701 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1714 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:07.633189 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7145 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:09.471536 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1709 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:12.090240 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.4713 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:13.904300 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:16.017936 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.3469 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:17.823424 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1143 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:20.495272 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.4713 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:22.289734 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3019 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:24.373146 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=10.4165 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:26.238782 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0884 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:28.328020 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.3383 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:30.157404 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.0891 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:30.162035 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=17.8715 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:30.165398 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0803, 0.1389](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72893   |\n",
      "|      Average Precision       |   0.71898   |\n",
      "|        Average Recall        |   0.72154   |\n",
      "|       Average F1 Score       |   0.71955   |\n",
      "|         Average Loss         |   0.81791   |\n",
      "|       Average Latency        |  75.582 ms  |\n",
      "|   Average GPU Power Usage    |  21.569 W   |\n",
      "| Inference Energy Consumption | 0.45283 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_9/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "W0305 18:52:51.096322 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.1331 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:52:55.230983 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mconv1._weight_quantizer                 : TensorQuantizer(8bit fake per-tensor amax=0.2050 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:03.476927 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.2240 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:07.699579 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.4028 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:12.963064 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.8195 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:17.157367 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2949 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:23.294537 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=12.1992 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:27.439353 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3236 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:32.359207 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.5654 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:36.450145 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer1.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.3288 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:41.881873 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=14.3958 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:46.044312 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2614 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:53.505056 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=5.4070 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:53:57.625881 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2852 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:02.897230 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=14.3958 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:07.033403 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.4878 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:13.817927 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.8680 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:18.084695 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2587 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:23.552574 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.0086 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:27.652969 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer2.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2507 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:32.250851 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.4574 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:36.397003 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2297 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:42.081111 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=6.7569 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:46.255969 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.2466 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:50.933856 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=11.4574 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:54:55.091716 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3595 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:01.094354 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=9.9356 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:05.163605 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1869 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:12.667527 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.6254 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:16.711321 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer3.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1855 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:24.113642 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=13.8985 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:28.216368 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1701 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:33.543651 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=7.7208 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:37.635031 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1314 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:45.028306 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._input_quantizer  : TensorQuantizer(8bit fake per-tensor amax=13.8985 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:49.186156 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.0.downsample.0._weight_quantizer : TensorQuantizer(8bit fake per-tensor amax=0.3209 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:54.378028 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=11.1746 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:55:58.530471 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv1._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1011 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:56:03.624593 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._input_quantizer         : TensorQuantizer(8bit fake per-tensor amax=8.5384 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:56:07.800776 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlayer4.1.conv2._weight_quantizer        : TensorQuantizer(8bit fake per-tensor amax=0.1074 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:56:07.804430 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._input_quantizer                     : TensorQuantizer(8bit fake per-tensor amax=17.8715 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "W0305 18:56:07.807187 140311821633344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfc._weight_quantizer                    : TensorQuantizer(8bit fake axis=0 amax=[0.0803, 0.1389](10) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.72821   |\n",
      "|      Average Precision       |   0.7185    |\n",
      "|        Average Recall        |   0.72061   |\n",
      "|       Average F1 Score       |   0.7189    |\n",
      "|         Average Loss         |   0.82186   |\n",
      "|       Average Latency        |  76.962 ms  |\n",
      "|   Average GPU Power Usage    |  21.211 W   |\n",
      "| Inference Energy Consumption | 0.45346 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_10/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, the 99% `percentile` clips too many values during the amax calibration, compromising the loss. However 99.99% demonstrates higher validation accuracy alongside `mse` and `entropy` for `jsc-toy`. For such a small model, the methods are not highly distinguished, however for larger models this calibration process will be important for ensuring the quantized model still performs well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3 Quantized Aware Training (QAT)\n",
    "\n",
    "The `tensorrt_fine_tune_transform_pass` is used to fine tune the quantized model. By default, when running the `tensorrt_engine_interface_pass` the fake quantized model will go through fine tuning however you stop this by setting the `fine_tune` in `passes.tensorrt.fine_tune` to false.\n",
    "\n",
    "For QAT it is typical to employ 10% of the original training epochs, starting at 1% of the initial training learning rate, and a cosine annealing learning rate schedule that follows the decreasing half of a cosine period, down to 1% of the initial fine tuning learning rate (0.01% of the initial training learning rate). However this default can be overidden by setting the `epochs`, `initial_learning_rate` and `final_learning_rate` in `passes.tensorrt.fine_tune`.\n",
    "\n",
    "The fine tuned checkpoints are stored in the ckpts/fine_tuning folder:\n",
    "\n",
    "```\n",
    "mase_output\n",
    "└── tensorrt\n",
    "    └── quantization\n",
    "        └──model_task_dataset_date\n",
    "            ├── cache\n",
    "            ├── ckpts\n",
    "            │   └── fine_tuning\n",
    "            ├── json\n",
    "            ├── onnx\n",
    "            └── trt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
      "I0305 18:56:37.920415 140311821633344 rank_zero.py:53] GPU available: True (cuda), used: True\n",
      "I0305 18:56:38.075425 140311821633344 rank_zero.py:53] TPU available: False, using: 0 TPU cores\n",
      "I0305 18:56:38.077213 140311821633344 rank_zero.py:53] IPU available: False, using: 0 IPUs\n",
      "I0305 18:56:38.079249 140311821633344 rank_zero.py:53] HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 18:57:16.153673 140311821633344 rank_zero.py:53] You are using a CUDA device ('NVIDIA GeForce RTX 3070 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 18:57:36.253819 140311821633344 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0305 18:57:36.266265 140311821633344 model_summary.py:90] \n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 11.2 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | loss_val  | MeanMetric         | 0     \n",
      "4 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 782/782 [01:49<00:00,  7.16it/s, v_num=1, train_acc_step=0.438, val_acc_epoch=0.618, val_loss_epoch=0.753]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 19:01:30.687032 140311821633344 rank_zero.py:53] `Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 782/782 [01:52<00:00,  6.92it/s, v_num=1, train_acc_step=0.438, val_acc_epoch=0.618, val_loss_epoch=0.753]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4 TensorRT Quantization\n",
    "\n",
    "After QAT, we are now ready to convert the model to a tensorRT engine so that it can be run with the superior inference speeds. To do so, we use the `tensorrt_engine_interface_pass` which converts the `MaseGraph`'s model from a Pytorch one to an ONNX format as an intermediate stage of the conversion.\n",
    "\n",
    "During the conversion process, the `.onnx` and `.trt` files are stored to their respective folders shown in [Section 1.3](#section-13-quantized-aware-training-qat).\n",
    "\n",
    "This interface pass returns a dictionary containing the `onnx_path` and `trt_engine_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/local/mase-gp/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/2025-03-05/version_3/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/local/mase-gp/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/2025-03-05/version_4/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/local/mase-gp/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/2025-03-05/version_5/model.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.5 Performance Analysis\n",
    "\n",
    "To showcase the improved inference speeds and to evaluate accuracy and other performance metrics, the `tensorrt_analysis_pass` can be used.\n",
    "\n",
    "The tensorRT engine path obtained the previous interface pass is now inputted into the the analysis pass. The same pass can take a MaseGraph as an input, as well as an ONNX graph. For this comparison, we will first run the anaylsis pass on the original unquantized model and then on the int8 quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72779   |\n",
      "|      Average Precision       |   0.71679   |\n",
      "|        Average Recall        |   0.71916   |\n",
      "|       Average F1 Score       |   0.71727   |\n",
      "|         Average Loss         |   0.82059   |\n",
      "|       Average Latency        |  130.06 ms  |\n",
      "|   Average GPU Power Usage    |   20.22 W   |\n",
      "| Inference Energy Consumption | 0.73052 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_11/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/05/2025-19:10:34] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.73647   |\n",
      "|      Average Precision       |   0.73873   |\n",
      "|        Average Recall        |   0.74017   |\n",
      "|       Average F1 Score       |   0.73847   |\n",
      "|         Average Loss         |    0.753    |\n",
      "|       Average Latency        |  19.869 ms  |\n",
      "|   Average GPU Power Usage    |  19.867 W   |\n",
      "| Inference Energy Consumption | 0.10965 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/tensorrt/version_0/model.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_, _ = runtime_analysis_pass(mg_original, pass_args=runtime_analysis_config)\n",
    "_, _ = runtime_analysis_pass(meta['trt_engine_path'], pass_args=runtime_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the latency has decreased around 6x with the `jsc-toy` model without compromising accuracy due to the well calibrated amax and quantization-aware fine tuning and additional runtime optimizations from TensorRT. The inference energy consumption has thus also dropped tremendously and this is an excellent demonstration for the need to quantize in industry especially for LLMs in order to reduce energy usage. \n",
    "\n",
    "## Section 2. FP16 Quantization\n",
    "\n",
    "We will now load in a new toml configuration that uses fp16 instead of int8, whilst keeping the other settings the exact same for a fair comparison. This time however, we will use chop from the terminal which runs all the passes showcased in [Section 1](#section-1---int8-quantization).\n",
    "\n",
    "Since float quantization does not require calibration, nor is it supported by `pytorch-quantization`, the model will not undergo fake quantization; for the time being this unfortunately means QAT is unavailable and only undergoes Post Training Quantization (PTQ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0305 23:32:00.152560 140320365823808 seed.py:54] Seed set to 0\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |       Default        | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |    \u001b[38;5;8mclassification\u001b[0m    |     cls      |                          |           cls            |\n",
      "| load_name               |         \u001b[38;5;8mNone\u001b[0m         |              | /root/local/mase-gp/mase | /root/local/mase-gp/mase |\n",
      "|                         |                      |              | _output/resnet18_cls_cif | _output/resnet18_cls_cif |\n",
      "|                         |                      |              | ar10_2025-03-05/software | ar10_2025-03-05/software |\n",
      "|                         |                      |              | /training_ckpts/best.ckp | /training_ckpts/best.ckp |\n",
      "|                         |                      |              |            t             |            t             |\n",
      "| load_type               |          \u001b[38;5;8mmz\u001b[0m          |              |            pl            |            pl            |\n",
      "| batch_size              |         \u001b[38;5;8m128\u001b[0m          |      64      |                          |            64            |\n",
      "| to_debug                |        False         |              |                          |          False           |\n",
      "| log_level               |         info         |              |                          |           info           |\n",
      "| report_to               |     tensorboard      |              |                          |       tensorboard        |\n",
      "| seed                    |          0           |              |                          |            0             |\n",
      "| quant_config            |         None         |              |                          |           None           |\n",
      "| training_optimizer      |         adam         |              |                          |           adam           |\n",
      "| trainer_precision       |       16-mixed       |              |                          |         16-mixed         |\n",
      "| learning_rate           |        \u001b[38;5;8m1e-05\u001b[0m         |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |          0           |              |                          |            0             |\n",
      "| max_epochs              |          \u001b[38;5;8m20\u001b[0m          |      10      |                          |            10            |\n",
      "| max_steps               |          -1          |              |                          |            -1            |\n",
      "| accumulate_grad_batches |          1           |              |                          |            1             |\n",
      "| log_every_n_steps       |          50          |              |                          |            50            |\n",
      "| num_workers             |          20          |              |                          |            20            |\n",
      "| num_devices             |          1           |              |                          |            1             |\n",
      "| num_nodes               |          1           |              |                          |            1             |\n",
      "| accelerator             |         \u001b[38;5;8mauto\u001b[0m         |     gpu      |                          |           gpu            |\n",
      "| strategy                |         auto         |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |        False         |              |                          |          False           |\n",
      "| github_ci               |        False         |              |                          |          False           |\n",
      "| disable_dataset_cache   |        False         |              |                          |          False           |\n",
      "| target                  | xcu250-figd2104-2L-e |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |         100          |              |                          |           100            |\n",
      "| is_pretrained           |        False         |              |                          |          False           |\n",
      "| max_token_len           |         512          |              |                          |           512            |\n",
      "| project_dir             |  /root/local/mase-   |              |                          |    /root/local/mase-     |\n",
      "|                         |    gp/mase_output    |              |                          |      gp/mase_output      |\n",
      "| project                 |         None         |              |                          |           None           |\n",
      "| model                   |         \u001b[38;5;8mNone\u001b[0m         |   resnet18   |                          |         resnet18         |\n",
      "| dataset                 |         \u001b[38;5;8mNone\u001b[0m         |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |          20          |              |                          |            20            |\n",
      "| eta_min                 |        1e-06         |              |                          |          1e-06           |\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'resnet18'...\u001b[0m\n",
      "self.args.model is resnet18\n",
      "model_info is MaseModelInfo(name='resnet', model_source=<ModelSource.TORCHVISION: 'torchvision'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'resnet18'...\u001b[0m\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'resnet18', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile', 'mse', 'entropy'], 'percentiles': [99.0, 99.9, 99.99], 'precision': 'fp16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'fine_tune': True}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05/software/training_ckpts/best.ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/local/mase-gp/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/2025-03-05/version_6/model.onnx\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/local/mase-gp/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/2025-03-05/version_7/model.trt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/local/mase-gp/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/2025-03-05/version_8/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72885   |\n",
      "|      Average Precision       |   0.7193    |\n",
      "|        Average Recall        |   0.72175   |\n",
      "|       Average F1 Score       |   0.71973   |\n",
      "|         Average Loss         |   0.81798   |\n",
      "|       Average Latency        |  31.991 ms  |\n",
      "|   Average GPU Power Usage    |  19.125 W   |\n",
      "| Inference Energy Consumption | 0.16995 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/mase_graph/version_12/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on resnet18-trt_quantized\u001b[0m\n",
      "[03/05/2025-23:34:30] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results resnet18-trt_quantized:\n",
      "+------------------------------+--------------+\n",
      "|      Metric (Per Batch)      |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73629    |\n",
      "|      Average Precision       |   0.73839    |\n",
      "|        Average Recall        |   0.73996    |\n",
      "|       Average F1 Score       |    0.7384    |\n",
      "|         Average Loss         |   0.74585    |\n",
      "|       Average Latency        |  3.5465 ms   |\n",
      "|   Average GPU Power Usage    |   17.923 W   |\n",
      "| Inference Energy Consumption | 0.017656 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/local/mase_output/tensorrt/quantization/resnet18_cls_cifar10_2025-03-05/tensorrt/version_1/model.json\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05/software/transform/transformed_ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "JSC_FP16_BY_TYPE_TOML = \"/root/local/mase-gp/docs/tutorials/tensorrt/resnet18_FP16_quantization_by_type.toml\"\n",
    "JSC_CHECKPOINT_PATH = \"/root/local/mase-gp/mase_output/resnet18_cls_cifar10_2025-03-05/software/training_ckpts/best.ckpt\"\n",
    "!python /root/local/mase-gp/src/ch transform --config {JSC_FP16_BY_TYPE_TOML} --load {JSC_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `fp16` acheives a slighty higher test accuracy but a slightly lower latency (~30%) from that of int8 quantization; it is still ~2.5x faster than the unquantized model. Now lets apply quantization to a more complicated model.\n",
    "\n",
    "## Section 3. Type-wise Mixed Precision on Larger Model\n",
    "We will now quantize `vgg7` which includes both convolutional and linear layers, however for this demonstration we want to quantize all layer types except the linear layers.\n",
    "\n",
    "In this case, we set:\n",
    "\n",
    "- The `by` parameter to `type`\n",
    "- The `quantize` parameter to true for `passes.tensorrt.conv2d.config` and `precision` parameter to 'int8'.\n",
    "- The `input` and `weight` quantize axis for the conv2d layers.\n",
    "- The default `passes.tensorrt.default.config` precision to true. \n",
    "\n",
    "During the TensorRT quantization, the model's conv2d layers will be converted to an int8 fake quantized form, whilst the linear layers are kept to their default 'fp16'. Calibration of the conv2d layers and then fine tuning will be undergone before quantization and inference.\n",
    "\n",
    "You may either download a pretrained model [here](https://imperiallondon-my.sharepoint.com/:f:/g/personal/zz7522_ic_ac_uk/Emh3VT7Q_qRFmnp8kDrcgDoBwGUuzLwwKNtX8ZAt368jJQ?e=gsKONa), otherwise train it yourself as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0306 15:06:31.029283 140282827188032 seed.py:54] Seed set to 0\n",
      "+-------------------------+----------------------+--------------+-----------------+----------------------+\n",
      "| Name                    |       Default        | Config. File | Manual Override |      Effective       |\n",
      "+-------------------------+----------------------+--------------+-----------------+----------------------+\n",
      "| task                    |    \u001b[38;5;8mclassification\u001b[0m    |     cls      |                 |         cls          |\n",
      "| load_name               |         None         |              |                 |         None         |\n",
      "| load_type               |          mz          |              |                 |          mz          |\n",
      "| batch_size              |         \u001b[38;5;8m128\u001b[0m          |      64      |                 |          64          |\n",
      "| to_debug                |        False         |              |                 |        False         |\n",
      "| log_level               |         info         |              |                 |         info         |\n",
      "| report_to               |     tensorboard      |              |                 |     tensorboard      |\n",
      "| seed                    |          0           |              |                 |          0           |\n",
      "| quant_config            |         None         |              |                 |         None         |\n",
      "| training_optimizer      |         adam         |              |                 |         adam         |\n",
      "| trainer_precision       |       16-mixed       |              |                 |       16-mixed       |\n",
      "| learning_rate           |        \u001b[38;5;8m1e-05\u001b[0m         |    0.001     |                 |        0.001         |\n",
      "| weight_decay            |          0           |              |                 |          0           |\n",
      "| max_epochs              |          \u001b[38;5;8m20\u001b[0m          |      10      |                 |          10          |\n",
      "| max_steps               |          -1          |              |                 |          -1          |\n",
      "| accumulate_grad_batches |          1           |              |                 |          1           |\n",
      "| log_every_n_steps       |          50          |              |                 |          50          |\n",
      "| num_workers             |          20          |              |                 |          20          |\n",
      "| num_devices             |          1           |              |                 |          1           |\n",
      "| num_nodes               |          1           |              |                 |          1           |\n",
      "| accelerator             |         \u001b[38;5;8mauto\u001b[0m         |     gpu      |                 |         gpu          |\n",
      "| strategy                |         auto         |              |                 |         auto         |\n",
      "| is_to_auto_requeue      |        False         |              |                 |        False         |\n",
      "| github_ci               |        False         |              |                 |        False         |\n",
      "| disable_dataset_cache   |        False         |              |                 |        False         |\n",
      "| target                  | xcu250-figd2104-2L-e |              |                 | xcu250-figd2104-2L-e |\n",
      "| num_targets             |         100          |              |                 |         100          |\n",
      "| is_pretrained           |        False         |              |                 |        False         |\n",
      "| max_token_len           |         512          |              |                 |         512          |\n",
      "| project_dir             |  /root/local/mase-   |              |                 |  /root/local/mase-   |\n",
      "|                         |    gp/mase_output    |              |                 |    gp/mase_output    |\n",
      "| project                 |         None         |              |                 |         None         |\n",
      "| model                   |         \u001b[38;5;8mNone\u001b[0m         |  vgg7_cifar  |                 |      vgg7_cifar      |\n",
      "| dataset                 |         \u001b[38;5;8mNone\u001b[0m         |   cifar10    |                 |       cifar10        |\n",
      "| t_max                   |          20          |              |                 |          20          |\n",
      "| eta_min                 |        1e-06         |              |                 |        1e-06         |\n",
      "+-------------------------+----------------------+--------------+-----------------+----------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7_cifar'...\u001b[0m\n",
      "self.args.model is vgg7_cifar\n",
      "model_info is MaseModelInfo(name='vgg7_cifar', model_source=<ModelSource.VISION_OTHERS: 'vision_others'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining model 'vgg7_cifar'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m##### WEIGHT DECAY ##### 0\u001b[0m\n",
      "I0306 15:06:32.428992 140282827188032 rank_zero.py:53] Using 16bit Automatic Mixed Precision (AMP)\n",
      "I0306 15:06:32.450018 140282827188032 rank_zero.py:53] GPU available: True (cuda), used: True\n",
      "I0306 15:06:32.556042 140282827188032 rank_zero.py:53] TPU available: False, using: 0 TPU cores\n",
      "I0306 15:06:32.556262 140282827188032 rank_zero.py:53] IPU available: False, using: 0 IPUs\n",
      "I0306 15:06:32.556331 140282827188032 rank_zero.py:53] HPU available: False, using: 0 HPUs\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "I0306 15:07:33.638568 140282827188032 cuda.py:58] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0306 15:07:33.643197 140282827188032 model_summary.py:90] \n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | VGG7               | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | loss_val  | MeanMetric         | 0     \n",
      "4 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 782/782 [00:24<00:00, 31.63it/s, v_num=1, train_acc_step=0.250]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 70.71it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 75.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 52.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 60.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 63.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 42.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 48.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 52.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.73it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 57.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 52.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 54.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 53.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 54.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 56.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 57.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 59.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 60.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 61.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 62.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 64.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:02, 65.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 66.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 67.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 67.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 68.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 69.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 70.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 70.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 71.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 72.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 72.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 74.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 74.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 75.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 75.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 76.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 76.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 76.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 77.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 77.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 77.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 77.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 76.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 76.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 76.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 76.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 76.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 76.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 76.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 76.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 76.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 76.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 76.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 76.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 76.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 76.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 76.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 76.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 76.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 76.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 76.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 76.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 76.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 76.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 76.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 76.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 76.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 76.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 76.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 76.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 76.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 76.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 76.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 76.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 76.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 76.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 76.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 76.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 76.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 76.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 76.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 76.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 76.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 76.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 76.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 76.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 76.71it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 782/782 [00:24<00:00, 31.30it/s, v_num=1, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:01, 89.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:01, 90.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 48.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 54.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 58.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 58.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 61.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 64.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 66.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 69.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 71.18it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 71.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 69.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 70.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:01, 72.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 73.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 74.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 75.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 76.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 77.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 76.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 77.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 78.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 76.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 78.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 79.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 80.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 80.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 81.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 81.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 81.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 82.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 82.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 83.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 83.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 83.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 82.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 83.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 82.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 82.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 82.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 82.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 82.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 82.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 82.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 81.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 81.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 81.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 81.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 81.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 81.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 81.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 81.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 81.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 81.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 81.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 81.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 81.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 81.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 80.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 80.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 80.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 80.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 80.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 80.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 80.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 80.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 80.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 80.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 80.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 80.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 80.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 79.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 79.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 79.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 79.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 79.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 79.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 79.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 79.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 79.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 79.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 79.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 79.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 79.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 79.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 79.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 79.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 79.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 79.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 79.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 79.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 79.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 79.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 79.41it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 79.47it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 782/782 [00:24<00:00, 31.84it/s, v_num=1, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 73.55it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 62.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:05, 27.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 33.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 38.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 42.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 44.75it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 48.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 51.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 56.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 59.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 61.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 62.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 64.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 65.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 67.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 68.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 69.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 71.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 72.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 73.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 69.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 70.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 71.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 72.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 73.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 75.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 75.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 76.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 77.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 78.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 79.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 79.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 80.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 81.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 81.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 81.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 81.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 81.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 81.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 81.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 81.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 81.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 81.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 81.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 80.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 81.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 80.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 80.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 80.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 80.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 80.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 80.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 80.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 80.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 80.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 80.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 80.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 80.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 80.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 80.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 80.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 80.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 80.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 80.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 80.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 79.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 79.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 79.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 79.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 79.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 79.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 79.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 79.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 79.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 79.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 79.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 79.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 79.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 79.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 79.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 79.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 79.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 79.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 79.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 79.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 79.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 79.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 79.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 79.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 79.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 78.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 79.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 78.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 79.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 79.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 79.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 78.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 78.98it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 782/782 [00:24<00:00, 31.84it/s, v_num=1, train_acc_step=0.625,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 41.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 52.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 59.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 64.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 68.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 71.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:01, 74.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:01, 76.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:01, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:01, 80.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:01, 82.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:01, 83.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:01, 84.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:01, 85.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:01, 86.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:01, 86.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:01, 87.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 78.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 79.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 80.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 81.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 82.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 82.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 82.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 83.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 83.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 83.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 84.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 84.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 83.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 83.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 84.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 84.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 84.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 84.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 84.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 84.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 84.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 84.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 84.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 83.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 83.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 83.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 83.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 83.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 83.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 83.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 83.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 83.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 83.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 83.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 82.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 82.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 82.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 82.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 82.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 82.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 82.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 82.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 82.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 82.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 82.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 81.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 81.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 81.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 81.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 81.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 81.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 81.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 81.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 81.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 81.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:00, 81.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 81.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 81.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 81.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 81.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:00<00:00, 81.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 81.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 80.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 80.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 80.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 80.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 80.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 80.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 80.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 80.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 80.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 80.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 80.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 80.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 80.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 80.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 80.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 80.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 80.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 80.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 80.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 80.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 80.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 80.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 80.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 80.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 80.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 80.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 80.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 81.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 81.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 81.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 81.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 81.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 81.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 81.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 80.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 80.99it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 782/782 [00:24<00:00, 31.58it/s, v_num=1, train_acc_step=0.250,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 76.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 76.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:03, 42.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:05, 30.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 34.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 39.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 42.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 49.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 51.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 54.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 56.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.11it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 59.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 61.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 58.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 60.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 61.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 62.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 64.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 65.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:02, 66.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 67.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 66.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 67.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 68.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 69.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 70.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 70.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 71.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 71.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 72.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 72.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 73.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 73.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 74.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 74.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 75.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 75.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 75.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 75.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 74.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 75.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 74.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 74.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 74.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 74.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 75.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 75.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 75.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 75.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 75.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 75.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 74.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 75.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 74.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 74.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 74.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 74.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 74.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 74.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 74.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 74.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 74.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 74.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 74.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 73.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 73.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 74.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 74.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 74.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 74.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:01<00:01, 74.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:01<00:01, 74.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 74.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 74.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 74.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 74.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:01, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:01, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 74.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 74.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 74.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 74.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 74.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 74.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 74.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 74.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 74.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 74.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 74.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 74.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 74.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 74.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 74.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 74.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 74.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 74.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 74.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 74.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 74.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 74.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 74.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 74.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 74.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 74.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 74.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 74.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 74.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 74.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 74.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 74.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 74.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 74.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 74.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 74.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 74.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 74.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 74.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 74.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 74.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 74.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 75.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 75.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 75.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 75.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 75.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 75.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 75.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 75.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 75.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 75.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 75.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 75.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:02<00:00, 75.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 75.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 75.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 75.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 75.77it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 75.85it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 782/782 [00:24<00:00, 31.28it/s, v_num=1, train_acc_step=0.625,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:05, 27.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 32.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:03, 39.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:03, 44.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 41.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 45.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 49.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 51.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 54.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 57.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 59.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 60.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 62.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 63.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 65.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 66.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 67.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 68.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 70.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 71.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 72.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 73.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 73.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 73.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 73.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 74.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 75.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 76.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 77.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 78.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 79.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 79.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 80.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 80.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 80.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 80.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 80.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 80.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 80.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 79.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 79.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 78.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 78.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 78.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 78.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 78.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 78.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 78.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 78.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 78.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:01, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:01, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:00, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 78.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 78.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 78.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 78.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 78.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 78.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 78.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 78.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 78.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 78.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 78.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 78.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 78.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 78.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 78.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 78.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 78.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 78.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 78.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 78.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 78.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 78.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 78.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 78.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 78.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 78.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 79.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 79.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 79.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 79.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 79.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 79.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 79.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 79.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 79.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 79.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 79.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 79.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 79.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 79.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 79.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 79.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 79.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 79.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 79.28it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 79.33it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 782/782 [00:24<00:00, 31.31it/s, v_num=1, train_acc_step=0.688,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 71.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:03, 47.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:02, 58.28it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 65.70it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 70.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 74.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:01, 77.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:01, 80.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:01, 82.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 60.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 56.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 58.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 60.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 62.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 64.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 65.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 67.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 68.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 69.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 70.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 71.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 71.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 72.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 73.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 74.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 75.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 75.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 76.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 77.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 77.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 78.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 79.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 79.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 80.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 80.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 80.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 80.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 80.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 80.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 80.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 81.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 81.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 81.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 81.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 81.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 80.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 80.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 80.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 80.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 80.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 80.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 80.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 80.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 80.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 80.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 80.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 80.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 80.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 80.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 80.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 80.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 80.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 80.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 80.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:00<00:00, 80.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:00<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 80.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 79.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 79.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 79.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 79.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 79.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 79.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 79.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 80.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 80.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 80.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 80.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 80.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 80.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 79.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 79.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 79.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 79.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 79.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 79.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 79.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 79.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 79.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 79.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 79.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 79.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 79.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 79.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 79.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 79.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 79.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 79.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 80.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 80.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 80.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 80.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 80.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 80.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 80.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 80.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 80.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 80.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 80.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 80.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 80.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 80.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 80.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 80.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 80.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 80.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 80.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 80.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 80.36it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 80.40it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 782/782 [00:25<00:00, 31.23it/s, v_num=1, train_acc_step=0.438,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:02, 75.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:01, 82.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:01, 82.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:01, 80.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 44.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 48.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 51.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 54.43it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 57.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 59.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 62.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 63.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 63.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 65.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 66.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 68.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 69.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 70.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 71.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 72.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 73.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 74.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 75.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 76.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 72.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 73.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 73.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 74.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 75.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 75.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 76.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 77.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 78.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 78.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 79.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 78.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 79.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 78.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 78.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 78.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 78.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 78.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 78.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 78.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 78.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 78.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 78.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 78.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 78.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 78.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 78.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 78.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 78.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 78.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 78.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 78.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 78.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 77.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 77.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 77.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:01, 77.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:00<00:01, 78.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:00, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 78.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 78.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 78.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 78.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 77.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 77.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 77.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 77.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 78.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 77.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 78.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 77.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 78.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 78.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 78.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 78.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 78.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 78.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 78.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 78.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 78.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 78.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 78.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 78.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 78.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 78.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 78.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 78.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 78.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 78.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 78.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 78.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 78.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 78.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 78.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 78.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 78.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 78.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 78.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 78.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 78.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 78.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:01<00:00, 79.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:01<00:00, 78.99it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:01<00:00, 79.03it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 782/782 [00:25<00:00, 31.22it/s, v_num=1, train_acc_step=0.750,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:02, 70.43it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:01, 79.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:01, 81.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:02, 68.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:02, 73.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:02, 66.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:02, 64.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:02, 53.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:02, 56.42it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 56.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 58.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 60.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 62.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 64.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 66.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 67.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 68.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 69.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:01, 70.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:01, 71.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:01, 72.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 73.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 73.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:01, 74.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 74.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 75.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 75.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 72.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 72.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 72.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 73.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 74.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 74.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 75.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 76.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 76.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 77.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 78.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 78.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 78.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 78.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 78.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 78.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 78.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 78.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 78.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 78.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 78.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 78.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 78.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 78.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 78.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 78.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 78.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 78.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 78.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 78.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 78.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 78.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 78.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 78.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 78.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 78.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 77.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 77.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 78.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 77.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:00<00:01, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:00, 77.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 77.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 77.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 77.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 77.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 77.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 77.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 77.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 77.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 77.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 77.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 77.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 77.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 77.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 77.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 77.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 77.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 77.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 77.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 77.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 77.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 77.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 77.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 77.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 77.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 77.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 77.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 77.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 77.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 77.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 77.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 77.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 77.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 77.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 77.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 77.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 77.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 77.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 77.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 77.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 77.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 77.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 77.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 77.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 77.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 77.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 77.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 77.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:01<00:00, 77.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:01<00:00, 77.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 77.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 77.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 77.48it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 782/782 [00:25<00:00, 31.23it/s, v_num=1, train_acc_step=0.625,\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|                  | 1/157 [00:00<00:01, 78.31it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/157 [00:00<00:06, 25.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 3/157 [00:00<00:04, 31.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 4/157 [00:00<00:04, 31.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 5/157 [00:00<00:04, 36.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 6/157 [00:00<00:03, 39.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                 | 7/157 [00:00<00:03, 42.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 8/157 [00:00<00:03, 46.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 9/157 [00:00<00:03, 48.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 10/157 [00:00<00:02, 51.26it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 11/157 [00:00<00:02, 53.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 12/157 [00:00<00:02, 55.39it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 13/157 [00:00<00:02, 56.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 14/157 [00:00<00:02, 58.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▌               | 15/157 [00:00<00:02, 60.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 16/157 [00:00<00:02, 61.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 17/157 [00:00<00:02, 63.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 18/157 [00:00<00:02, 64.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 19/157 [00:00<00:02, 65.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 20/157 [00:00<00:02, 67.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 21/157 [00:00<00:02, 67.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 22/157 [00:00<00:01, 69.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 23/157 [00:00<00:01, 69.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 24/157 [00:00<00:02, 66.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 25/157 [00:00<00:01, 67.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 26/157 [00:00<00:01, 67.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 27/157 [00:00<00:01, 68.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 28/157 [00:00<00:01, 68.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 29/157 [00:00<00:01, 69.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 30/157 [00:00<00:01, 70.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 31/157 [00:00<00:01, 70.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 32/157 [00:00<00:01, 71.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 33/157 [00:00<00:01, 72.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 34/157 [00:00<00:01, 72.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 35/157 [00:00<00:01, 73.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 36/157 [00:00<00:01, 73.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 37/157 [00:00<00:01, 74.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 38/157 [00:00<00:01, 75.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 39/157 [00:00<00:01, 75.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 40/157 [00:00<00:01, 76.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 41/157 [00:00<00:01, 76.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 42/157 [00:00<00:01, 76.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▋            | 43/157 [00:00<00:01, 76.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 44/157 [00:00<00:01, 76.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▊            | 45/157 [00:00<00:01, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 46/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 47/157 [00:00<00:01, 76.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 48/157 [00:00<00:01, 76.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 49/157 [00:00<00:01, 76.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 50/157 [00:00<00:01, 76.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▌           | 51/157 [00:00<00:01, 76.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 52/157 [00:00<00:01, 76.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 53/157 [00:00<00:01, 76.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 54/157 [00:00<00:01, 76.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 55/157 [00:00<00:01, 76.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 56/157 [00:00<00:01, 76.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 57/157 [00:00<00:01, 76.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 58/157 [00:00<00:01, 76.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 59/157 [00:00<00:01, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/157 [00:00<00:01, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 61/157 [00:00<00:01, 76.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 62/157 [00:00<00:01, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 63/157 [00:00<00:01, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 64/157 [00:00<00:01, 76.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████          | 65/157 [00:00<00:01, 76.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 66/157 [00:00<00:01, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 67/157 [00:00<00:01, 76.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 68/157 [00:00<00:01, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 69/157 [00:00<00:01, 76.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 70/157 [00:00<00:01, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 71/157 [00:00<00:01, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 72/157 [00:00<00:01, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▉         | 73/157 [00:00<00:01, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████         | 74/157 [00:00<00:01, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 75/157 [00:00<00:01, 76.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 76/157 [00:00<00:01, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 77/157 [00:01<00:01, 76.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 78/157 [00:01<00:01, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 79/157 [00:01<00:01, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 80/157 [00:01<00:01, 76.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 81/157 [00:01<00:00, 76.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 82/157 [00:01<00:00, 76.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▉        | 83/157 [00:01<00:00, 76.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████        | 84/157 [00:01<00:00, 76.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 85/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 86/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 87/157 [00:01<00:00, 76.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 88/157 [00:01<00:00, 76.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 89/157 [00:01<00:00, 76.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 90/157 [00:01<00:00, 76.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 91/157 [00:01<00:00, 76.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▉       | 92/157 [00:01<00:00, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 93/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 94/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 95/157 [00:01<00:00, 75.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 96/157 [00:01<00:00, 75.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 97/157 [00:01<00:00, 75.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 98/157 [00:01<00:00, 75.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 99/157 [00:01<00:00, 75.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 100/157 [00:01<00:00, 75.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 101/157 [00:01<00:00, 75.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 102/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 103/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 104/157 [00:01<00:00, 75.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 105/157 [00:01<00:00, 75.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 106/157 [00:01<00:00, 75.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 107/157 [00:01<00:00, 75.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 108/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 109/157 [00:01<00:00, 75.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 110/157 [00:01<00:00, 75.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 111/157 [00:01<00:00, 75.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 112/157 [00:01<00:00, 75.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 113/157 [00:01<00:00, 75.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▌    | 114/157 [00:01<00:00, 75.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 115/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 116/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 117/157 [00:01<00:00, 75.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 118/157 [00:01<00:00, 75.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 119/157 [00:01<00:00, 75.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 120/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 121/157 [00:01<00:00, 76.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 122/157 [00:01<00:00, 75.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 123/157 [00:01<00:00, 75.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 124/157 [00:01<00:00, 76.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▋   | 125/157 [00:01<00:00, 76.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 126/157 [00:01<00:00, 76.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 127/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 128/157 [00:01<00:00, 76.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 129/157 [00:01<00:00, 76.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 130/157 [00:01<00:00, 76.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 131/157 [00:01<00:00, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 132/157 [00:01<00:00, 76.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 133/157 [00:01<00:00, 76.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 134/157 [00:01<00:00, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 135/157 [00:01<00:00, 76.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 136/157 [00:01<00:00, 76.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 137/157 [00:01<00:00, 76.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 138/157 [00:01<00:00, 76.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 139/157 [00:01<00:00, 76.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 140/157 [00:01<00:00, 76.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 141/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 142/157 [00:01<00:00, 76.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 143/157 [00:01<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 144/157 [00:01<00:00, 76.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 145/157 [00:01<00:00, 76.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 146/157 [00:01<00:00, 76.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|██████████████▉ | 147/157 [00:01<00:00, 76.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 148/157 [00:01<00:00, 76.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 149/157 [00:01<00:00, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 150/157 [00:01<00:00, 76.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 151/157 [00:01<00:00, 76.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 152/157 [00:01<00:00, 76.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 153/157 [00:02<00:00, 76.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 154/157 [00:02<00:00, 76.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 155/157 [00:02<00:00, 76.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 156/157 [00:02<00:00, 76.44it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 157/157 [00:02<00:00, 76.48it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 782/782 [00:27<00:00, 28.21it/s, v_num=1, train_acc_step=0.625,\u001b[AI0306 15:13:40.852709 140282827188032 rank_zero.py:53] `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Epoch 9: 100%|█| 782/782 [00:35<00:00, 21.99it/s, v_num=1, train_acc_step=0.625,\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTraining is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "VGG_TYPEWISE_TOML = \"vgg7_typewise_mixed_precision.toml\"\n",
    "\n",
    "!python /root/local/mase-gp/src/ch train --config {VGG_TYPEWISE_TOML}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the checkpoint in, quantize the model and compare it to the unquantized version as we did in [Section 1.5](#section-15-performance-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_TYPEWISE_TOML = \"vgg7_typewise_mixed_precision.toml\"\n",
    "# Change this checkpoint path accordingly\n",
    "VGG_CHECKPOINT_PATH = \"/root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06/software/training_ckpts/best.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0306 15:58:00.578649 140271219853120 seed.py:54] Seed set to 0\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |       Default        | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |    \u001b[38;5;8mclassification\u001b[0m    |     cls      |                          |           cls            |\n",
      "| load_name               |         \u001b[38;5;8mNone\u001b[0m         |              | /root/local/mase-gp/mase | /root/local/mase-gp/mase |\n",
      "|                         |                      |              | _output/vgg7_cifar_cls_c | _output/vgg7_cifar_cls_c |\n",
      "|                         |                      |              | ifar10_2025-03-06/softwa | ifar10_2025-03-06/softwa |\n",
      "|                         |                      |              | re/training_ckpts/best.c | re/training_ckpts/best.c |\n",
      "|                         |                      |              |           kpt            |           kpt            |\n",
      "| load_type               |          \u001b[38;5;8mmz\u001b[0m          |              |            pl            |            pl            |\n",
      "| batch_size              |         \u001b[38;5;8m128\u001b[0m          |      64      |                          |            64            |\n",
      "| to_debug                |        False         |              |                          |          False           |\n",
      "| log_level               |         info         |              |                          |           info           |\n",
      "| report_to               |     tensorboard      |              |                          |       tensorboard        |\n",
      "| seed                    |          0           |              |                          |            0             |\n",
      "| quant_config            |         None         |              |                          |           None           |\n",
      "| training_optimizer      |         adam         |              |                          |           adam           |\n",
      "| trainer_precision       |       16-mixed       |              |                          |         16-mixed         |\n",
      "| learning_rate           |        \u001b[38;5;8m1e-05\u001b[0m         |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |          0           |              |                          |            0             |\n",
      "| max_epochs              |          \u001b[38;5;8m20\u001b[0m          |      10      |                          |            10            |\n",
      "| max_steps               |          -1          |              |                          |            -1            |\n",
      "| accumulate_grad_batches |          1           |              |                          |            1             |\n",
      "| log_every_n_steps       |          50          |              |                          |            50            |\n",
      "| num_workers             |          20          |              |                          |            20            |\n",
      "| num_devices             |          1           |              |                          |            1             |\n",
      "| num_nodes               |          1           |              |                          |            1             |\n",
      "| accelerator             |         \u001b[38;5;8mauto\u001b[0m         |     gpu      |                          |           gpu            |\n",
      "| strategy                |         auto         |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |        False         |              |                          |          False           |\n",
      "| github_ci               |        False         |              |                          |          False           |\n",
      "| disable_dataset_cache   |        False         |              |                          |          False           |\n",
      "| target                  | xcu250-figd2104-2L-e |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |         100          |              |                          |           100            |\n",
      "| is_pretrained           |        False         |              |                          |          False           |\n",
      "| max_token_len           |         512          |              |                          |           512            |\n",
      "| project_dir             |  /root/local/mase-   |              |                          |    /root/local/mase-     |\n",
      "|                         |    gp/mase_output    |              |                          |      gp/mase_output      |\n",
      "| project                 |         None         |              |                          |           None           |\n",
      "| model                   |         \u001b[38;5;8mNone\u001b[0m         |  vgg7_cifar  |                          |        vgg7_cifar        |\n",
      "| dataset                 |         \u001b[38;5;8mNone\u001b[0m         |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |          20          |              |                          |            20            |\n",
      "| eta_min                 |        1e-06         |              |                          |          1e-06           |\n",
      "+-------------------------+----------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7_cifar'...\u001b[0m\n",
      "self.args.model is vgg7_cifar\n",
      "model_info is MaseModelInfo(name='vgg7_cifar', model_source=<ModelSource.VISION_OTHERS: 'vision_others'>, task_type=<ModelTaskType.VISION: 'vision'>, image_classification=True, physical_data_point_classification=False, sequence_classification=False, seq2seqLM=False, causal_LM=False, is_quantized=False, is_lora=False, is_sparse=False, is_fx_traceable=True)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'vgg7_cifar'...\u001b[0m\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06/software/training_ckpts/best.ckpt\u001b[0m\n",
      "{'model': 'vgg7_cifar', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001, 'accelerator': 'gpu', 'task': 'cls', 'transform': {'style': 'graph'}, 'passes': {'tensorrt': {'by': 'type', 'num_calibration_batches': 10, 'post_calibration_analysis': True, 'default': {'config': {'quantize': True, 'calibrators': ['percentile'], 'percentiles': [99.0, 99.9], 'precision': 'int8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'linear': {'config': {'quantize': False}}, 'conv2d': {'config': {'quantize': True, 'precision': 'int8', 'calibrator': 'histogram'}, 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}, 'runtime_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}}\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/local/mase-gp/mase_output/vgg7_cifar_cls_cifar10_2025-03-06/software/training_ckpts/best.ckpt\u001b[0m\n",
      "tensor([[[[3.4464e-01, 6.9910e-01, 6.0161e-01, 5.1311e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [6.6311e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.8461e-01, 4.2010e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.7113e-01, 0.0000e+00],\n",
      "          [6.4409e-01, 8.5504e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [7.9605e-01, 2.5626e+00, 3.4395e+00, 3.1523e+00]],\n",
      "\n",
      "         [[8.0510e-01, 8.0670e-01, 1.3297e-01, 0.0000e+00],\n",
      "          [2.5205e-01, 5.3269e-01, 9.2034e-01, 9.8315e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 1.8874e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.9280e-01, 6.0597e-01, 1.4563e-01, 0.0000e+00],\n",
      "          [2.8448e-01, 7.9045e-01, 6.4207e-01, 1.2590e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1534e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 6.2839e-01, 8.3748e-01],\n",
      "          [0.0000e+00, 6.9968e-01, 2.5770e-01, 1.0547e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.6658e-01, 1.2895e-01, 2.4467e-01, 5.4618e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[3.4050e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.4691e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [4.9779e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.0582e-01, 4.2151e-01, 4.6160e-01, 3.9830e-01]],\n",
      "\n",
      "         [[0.0000e+00, 4.3441e-01, 1.1964e+00, 7.0995e-01],\n",
      "          [5.5145e-01, 2.4369e+00, 7.4727e-01, 0.0000e+00],\n",
      "          [1.4729e-01, 2.4310e+00, 1.3350e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2677e-01]],\n",
      "\n",
      "         [[4.6489e-02, 3.8742e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.3351e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.6955e-01, 6.7727e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.9228e-02, 2.2171e-01, 5.2112e-01, 8.8665e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4068e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 6.6314e-03, 1.0431e+00]],\n",
      "\n",
      "         [[2.7404e-01, 0.0000e+00, 0.0000e+00, 5.1160e-01],\n",
      "          [3.1893e-02, 0.0000e+00, 0.0000e+00, 1.7702e-01],\n",
      "          [9.6126e-01, 1.0711e+00, 0.0000e+00, 6.0377e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[7.8424e-01, 4.9169e-01, 0.0000e+00, 7.4105e-01],\n",
      "          [1.2236e-02, 0.0000e+00, 0.0000e+00, 5.8802e-01],\n",
      "          [1.5900e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.5348e-01, 0.0000e+00, 0.0000e+00, 5.2442e-01]]],\n",
      "\n",
      "\n",
      "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0309e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.2175e-01, 1.8725e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.1506e-01, 1.3181e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.5812e-02, 0.0000e+00, 3.7893e-02, 8.1935e-01],\n",
      "          [0.0000e+00, 1.1055e-02, 1.2085e+00, 2.1328e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 2.5364e-02, 1.1291e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 1.8021e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5488e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8639e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.4830e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [5.7112e-02, 0.0000e+00, 0.0000e+00, 4.6543e-02]],\n",
      "\n",
      "         [[4.4452e-01, 2.5066e-01, 4.8193e-01, 1.4843e-01],\n",
      "          [0.0000e+00, 7.2661e-01, 1.4449e+00, 1.2516e+00],\n",
      "          [0.0000e+00, 1.3113e-01, 2.1405e+00, 1.9119e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0507e-01]],\n",
      "\n",
      "         [[7.8332e-01, 5.0053e-02, 4.9883e-01, 4.6011e-01],\n",
      "          [8.3906e-01, 0.0000e+00, 0.0000e+00, 9.1762e-02],\n",
      "          [9.4349e-01, 3.7160e-01, 4.7023e-01, 5.2874e-01],\n",
      "          [7.8920e-01, 6.4321e-01, 5.4258e-01, 7.3443e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[2.0336e-01, 2.7439e-01, 2.9018e-01, 4.7235e-01],\n",
      "          [6.3775e-02, 7.5851e-02, 0.0000e+00, 2.6987e-01],\n",
      "          [7.0484e-04, 3.0807e-01, 2.1616e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.9833e-01, 3.4737e-01]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 1.0248e+00, 4.3724e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 8.7463e-01, 6.0394e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.8190e-02, 8.3974e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1064e-01]],\n",
      "\n",
      "         [[1.0165e-01, 0.0000e+00, 1.3019e-01, 2.4331e-01],\n",
      "          [1.7269e-01, 0.0000e+00, 0.0000e+00, 2.4024e-01],\n",
      "          [1.2771e-01, 0.0000e+00, 9.5308e-02, 5.7793e-01],\n",
      "          [2.2411e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 4.0006e-02, 5.6858e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9407e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 3.5714e-03, 3.9732e-01],\n",
      "          [0.0000e+00, 4.0023e-02, 1.8024e-01, 8.8047e-01]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3977e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 6.1690e-01, 8.3333e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.5187e-01, 1.4085e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 3.5957e-02, 0.0000e+00]],\n",
      "\n",
      "         [[3.0180e-01, 2.5117e-01, 5.2555e-02, 3.5095e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 2.2309e-01, 1.7721e-01],\n",
      "          [0.0000e+00, 2.1539e-01, 4.8005e-01, 0.0000e+00],\n",
      "          [2.0686e-01, 5.5049e-01, 6.9588e-01, 5.3676e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.3578e-01, 1.4642e-01, 4.3261e-01, 4.4860e-01],\n",
      "          [0.0000e+00, 7.3825e-03, 2.8840e-01, 2.4194e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.8339e-02, 2.4439e-01],\n",
      "          [8.7670e-01, 4.0882e-01, 9.3262e-02, 8.7157e-02]],\n",
      "\n",
      "         [[4.2232e-01, 1.2441e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0412e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.2080e-01, 3.9040e-01, 2.5791e-02, 0.0000e+00],\n",
      "          [7.0812e-01, 2.7725e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.2837e+00, 7.7720e-01, 0.0000e+00, 1.3088e-01],\n",
      "          [1.3061e+00, 1.1616e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.2070e-01, 7.3110e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 2.5153e-02, 7.3822e-02, 1.2710e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.8607e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.7074e-01, 6.4452e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [1.0762e+00, 9.5151e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0280e-02]],\n",
      "\n",
      "         [[9.8293e-01, 3.8151e-02, 0.0000e+00, 0.0000e+00],\n",
      "          [1.9849e+00, 1.3293e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [6.6384e-01, 7.4747e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[5.0846e-01, 1.3086e-01, 1.8741e-01, 4.5585e-01],\n",
      "          [3.5816e-01, 0.0000e+00, 0.0000e+00, 2.7572e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1126e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0979e-01]]],\n",
      "\n",
      "\n",
      "        [[[2.1595e-01, 1.1665e-01, 0.0000e+00, 1.5170e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8622e-02],\n",
      "          [7.6539e-02, 0.0000e+00, 0.0000e+00, 3.9237e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4927e-01]],\n",
      "\n",
      "         [[7.6885e-02, 0.0000e+00, 0.0000e+00, 2.2066e-02],\n",
      "          [2.1685e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.9086e-01, 1.4497e-01, 1.7847e-01, 0.0000e+00],\n",
      "          [4.2632e-01, 1.2138e+00, 8.4704e-01, 0.0000e+00]],\n",
      "\n",
      "         [[4.0809e-01, 0.0000e+00, 2.1326e-01, 3.9940e-01],\n",
      "          [5.2010e-01, 6.7214e-03, 3.6464e-01, 5.8390e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 4.8838e-01, 2.3755e-01],\n",
      "          [1.3527e-01, 0.0000e+00, 1.2194e-01, 0.0000e+00],\n",
      "          [1.0103e-01, 3.5915e-01, 0.0000e+00, 1.5934e-01],\n",
      "          [0.0000e+00, 1.3741e+00, 1.6263e+00, 6.2613e-01]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 2.2307e-01, 9.2222e-02],\n",
      "          [6.4332e-01, 1.3732e-01, 1.2238e+00, 5.5892e-01],\n",
      "          [1.7521e-01, 2.3195e-01, 4.3394e-01, 3.9164e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 5.9715e-01, 6.8072e-01]],\n",
      "\n",
      "         [[4.6217e-01, 6.5594e-01, 7.3069e-01, 8.3431e-01],\n",
      "          [0.0000e+00, 0.0000e+00, 9.2466e-01, 1.2812e+00],\n",
      "          [1.0521e+00, 2.2856e-01, 0.0000e+00, 6.4906e-01],\n",
      "          [4.9690e-01, 3.0810e-01, 1.3593e+00, 1.1906e+00]]]], device='cuda:0',\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "!python /root/local/mase-gp/src/ch transform --config {VGG_TYPEWISE_TOML} --load {VGG_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By quantizing all convolutional layers to INT8 and maintaining fp16 precision for the linear layers we see a marginal decrease in latency whilst maintaining a comparable accuracy. By experimenting with precisions on a per type basis, you may find insights that work best for your model. \n",
    "\n",
    "## Section 4. Layer-wise Mixed Precision\n",
    "\n",
    "So far we have strictly quantized either in int8 or fp16. Now, we will show how to conduct layerwise mixed precision using the same `vgg7` model. In this case we will show how for instance, layer 0 and 1 can be set to fp16, while the remaining layers can be int8 quantized. \n",
    "\n",
    "For this, we set:\n",
    "- The `by` parameter to `name`\n",
    "- The `precision` to 'int8' for `passes.tensorrt.default.config`\n",
    "- The `precision` to 'fp16' for `passes.tensorrt.feature_layers_0.config and passes.tensorrt.feature_layers_1.config`\n",
    "- The `precision` to 'int8' for `passes.tensorrt.feature_layers_2.config and passes.tensorrt.feature_layers_3.config` (although this is not necessary since the default is already set to 'int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-28 23:25:51,157] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0328 23:25:54.303634 140449214740288 seed.py:54] Seed set to 0\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |        Default         | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |     \u001b[38;5;8mclassification\u001b[0m     |     cls      |                          |           cls            |\n",
      "| load_name               |          \u001b[38;5;8mNone\u001b[0m          |              | /root/mase/mase_output/v | /root/mase/mase_output/v |\n",
      "|                         |                        |              |  gg7-pre-trained/test-   |  gg7-pre-trained/test-   |\n",
      "|                         |                        |              |     accu-0.9332.ckpt     |     accu-0.9332.ckpt     |\n",
      "| load_type               |           \u001b[38;5;8mmz\u001b[0m           |              |            pl            |            pl            |\n",
      "| batch_size              |          \u001b[38;5;8m128\u001b[0m           |      64      |                          |            64            |\n",
      "| to_debug                |         False          |              |                          |          False           |\n",
      "| log_level               |          info          |              |                          |           info           |\n",
      "| report_to               |      tensorboard       |              |                          |       tensorboard        |\n",
      "| seed                    |           0            |              |                          |            0             |\n",
      "| quant_config            |          None          |              |                          |           None           |\n",
      "| training_optimizer      |          adam          |              |                          |           adam           |\n",
      "| trainer_precision       |        16-mixed        |              |                          |         16-mixed         |\n",
      "| learning_rate           |         \u001b[38;5;8m1e-05\u001b[0m          |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |           0            |              |                          |            0             |\n",
      "| max_epochs              |           \u001b[38;5;8m20\u001b[0m           |      10      |                          |            10            |\n",
      "| max_steps               |           -1           |              |                          |            -1            |\n",
      "| accumulate_grad_batches |           1            |              |                          |            1             |\n",
      "| log_every_n_steps       |           50           |              |                          |            50            |\n",
      "| num_workers             |           28           |              |                          |            28            |\n",
      "| num_devices             |           1            |              |                          |            1             |\n",
      "| num_nodes               |           1            |              |                          |            1             |\n",
      "| accelerator             |          \u001b[38;5;8mauto\u001b[0m          |     gpu      |                          |           gpu            |\n",
      "| strategy                |          auto          |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |         False          |              |                          |          False           |\n",
      "| github_ci               |         False          |              |                          |          False           |\n",
      "| disable_dataset_cache   |         False          |              |                          |          False           |\n",
      "| target                  |  xcu250-figd2104-2L-e  |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |          100           |              |                          |           100            |\n",
      "| is_pretrained           |         False          |              |                          |          False           |\n",
      "| max_token_len           |          512           |              |                          |           512            |\n",
      "| project_dir             | /root/mase/mase_output |              |                          |  /root/mase/mase_output  |\n",
      "| project                 |          None          |              |                          |           None           |\n",
      "| model                   |          \u001b[38;5;8mNone\u001b[0m          |     vgg7     |                          |           vgg7           |\n",
      "| dataset                 |          \u001b[38;5;8mNone\u001b[0m          |   cifar10    |                          |         cifar10          |\n",
      "| t_max                   |           20           |              |                          |            20            |\n",
      "| eta_min                 |         1e-06          |              |                          |          1e-06           |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7'...\u001b[0m\n",
      "I0328 23:25:54.313626 140449214740288 cli.py:846] Initialising model 'vgg7'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "I0328 23:25:54.417618 140449214740288 cli.py:874] Initialising dataset 'cifar10'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-28\u001b[0m\n",
      "I0328 23:25:54.418019 140449214740288 cli.py:910] Project will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-28\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'vgg7'...\u001b[0m\n",
      "I0328 23:25:54.535444 140449214740288 cli.py:370] Transforming model 'vgg7'...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\u001b[0m\n",
      "I0328 23:26:00.655963 140449214740288 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\u001b[0m\n",
      "I0328 23:26:00.777872 140449214740288 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0328 23:26:12.570783 140449214740288 utils.py:314] Applying fake quantization to PyTorch model...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0328 23:26:12.921518 140449214740288 utils.py:339] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "I0328 23:26:12.940881 140449214740288 summary.py:84] Quantized graph histogram:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm2d     | batch_norm2d |       6 |         0 |           6 |\n",
      "| Conv2d          | conv2d       |       6 |         5 |           1 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| MaxPool2d       | max_pool2d   |       3 |         0 |           3 |\n",
      "| ReLU            | relu         |       8 |         0 |           8 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| view            | view         |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
      "I0328 23:26:12.941653 140449214740288 summary.py:85] \n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm2d     | batch_norm2d |       6 |         0 |           6 |\n",
      "| Conv2d          | conv2d       |       6 |         5 |           1 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| MaxPool2d       | max_pool2d   |       3 |         0 |           3 |\n",
      "| ReLU            | relu         |       8 |         0 |           8 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| view            | view         |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0328 23:26:12.942434 140449214740288 calibrate.py:143] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.952852 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953087 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953220 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953323 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953427 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953537 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953635 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953727 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953824 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.953913 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954009 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954101 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954192 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954295 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954387 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0328 23:26:12.954478 140449214740288 calibrate.py:152] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.535559 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.535989 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536076 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536209 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536275 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536394 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536448 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536548 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536605 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536704 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536756 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.536853 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.536910 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537009 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537059 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537172 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537243 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537340 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537401 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537500 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537567 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537664 140449214740288 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537712 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537806 140449214740288 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.537872 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.537964 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.538011 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.538102 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.538153 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.538257 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0328 23:26:18.538305 140449214740288 calibrate.py:175] Enabling Quantization and Disabling Calibration\n",
      "W0328 23:26:18.538399 140449214740288 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0328 23:26:18.546847 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0328 23:26:18.546956 140449214740288 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2937 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.547093 140449214740288 calibrate.py:131] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2937 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.547413 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2366 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.547523 140449214740288 calibrate.py:131] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2366 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.547886 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=1.8330 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.548011 140449214740288 calibrate.py:131] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=1.8330 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.548304 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2296 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.548407 140449214740288 calibrate.py:131] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2296 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.548746 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.4681 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.548850 140449214740288 calibrate.py:131] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.4681 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.549174 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2080 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.549281 140449214740288 calibrate.py:131] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2080 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.549598 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.9284 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.549701 140449214740288 calibrate.py:131] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.9284 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.550004 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.550112 140449214740288 calibrate.py:131] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2013 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.550404 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.6127 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.550505 140449214740288 calibrate.py:131] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.6127 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.550795 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.1879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.550894 140449214740288 calibrate.py:131] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.1879 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.551028 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.551139 140449214740288 calibrate.py:131] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.551264 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([1024, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.551505 140449214740288 calibrate.py:131] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.551799 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=9.7654 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.551900 140449214740288 calibrate.py:131] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=9.7654 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.552192 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.0590 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.552289 140449214740288 calibrate.py:131] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.0590 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.552608 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=7.4475 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.552708 140449214740288 calibrate.py:131] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=7.4475 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:18.552994 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1019 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:18.553097 140449214740288 calibrate.py:131] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1019 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "I0328 23:26:18.554053 140449214740288 calibrate.py:105] Performing post calibration analysis for calibrator percentile_99.0...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on vgg7\u001b[0m\n",
      "I0328 23:26:18.554283 140449214740288 runtime_analysis.py:357] Starting transformation analysis on vgg7\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.87663   |\n",
      "|      Average Precision       |   0.87531   |\n",
      "|        Average Recall        |   0.87386   |\n",
      "|       Average F1 Score       |   0.87335   |\n",
      "|         Average Loss         |   0.65133   |\n",
      "|       Average Latency        |  17.901 ms  |\n",
      "|   Average GPU Power Usage    |  57.532 W   |\n",
      "| Inference Energy Consumption | 0.28607 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0328 23:26:29.263397 140449214740288 runtime_analysis.py:521] \n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.87663   |\n",
      "|      Average Precision       |   0.87531   |\n",
      "|        Average Recall        |   0.87386   |\n",
      "|       Average F1 Score       |   0.87335   |\n",
      "|         Average Loss         |   0.65133   |\n",
      "|       Average Latency        |  17.901 ms  |\n",
      "|   Average GPU Power Usage    |  57.532 W   |\n",
      "| Inference Energy Consumption | 0.28607 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_50/model.json\u001b[0m\n",
      "I0328 23:26:29.264865 140449214740288 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_50/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0328 23:26:29.265057 140449214740288 calibrate.py:118] Post calibration analysis complete.\n",
      "W0328 23:26:29.265783 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.9458 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.266022 140449214740288 calibrate.py:131] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.9458 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.266428 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3704 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.266630 140449214740288 calibrate.py:131] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3704 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.267089 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2568 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.267289 140449214740288 calibrate.py:131] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2568 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.267678 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.267869 140449214740288 calibrate.py:131] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3621 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.268326 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.4123 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.268521 140449214740288 calibrate.py:131] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.4123 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.268913 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2821 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.269103 140449214740288 calibrate.py:131] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2821 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.269515 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.9841 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.269702 140449214740288 calibrate.py:131] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.9841 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.270093 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2734 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.270280 140449214740288 calibrate.py:131] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2734 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.270668 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.7013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.270851 140449214740288 calibrate.py:131] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.7013 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.271238 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2519 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.271429 140449214740288 calibrate.py:131] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2519 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.271625 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.271768 140449214740288 calibrate.py:131] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=11.6545 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.271936 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([1024, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.272221 140449214740288 calibrate.py:131] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0158, 0.4703](1024) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.272616 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=38.3167 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.272804 140449214740288 calibrate.py:131] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=38.3167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.273202 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1175 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.273392 140449214740288 calibrate.py:131] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1175 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.273818 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=19.2420 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.274000 140449214740288 calibrate.py:131] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=19.2420 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0328 23:26:29.274388 140449214740288 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1626 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0328 23:26:29.274579 140449214740288 calibrate.py:131] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1626 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "I0328 23:26:29.275199 140449214740288 calibrate.py:105] Performing post calibration analysis for calibrator percentile_99.9...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on vgg7\u001b[0m\n",
      "I0328 23:26:29.275390 140449214740288 runtime_analysis.py:357] Starting transformation analysis on vgg7\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.92097   |\n",
      "|      Average Precision       |   0.91959   |\n",
      "|        Average Recall        |   0.91991   |\n",
      "|       Average F1 Score       |   0.91957   |\n",
      "|         Average Loss         |   0.23991   |\n",
      "|       Average Latency        |  18.132 ms  |\n",
      "|   Average GPU Power Usage    |  57.867 W   |\n",
      "| Inference Energy Consumption | 0.29145 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0328 23:26:40.146152 140449214740288 runtime_analysis.py:521] \n",
      "Results vgg7:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.92097   |\n",
      "|      Average Precision       |   0.91959   |\n",
      "|        Average Recall        |   0.91991   |\n",
      "|       Average F1 Score       |   0.91957   |\n",
      "|         Average Loss         |   0.23991   |\n",
      "|       Average Latency        |  18.132 ms  |\n",
      "|   Average GPU Power Usage    |  57.867 W   |\n",
      "| Inference Energy Consumption | 0.29145 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_51/model.json\u001b[0m\n",
      "I0328 23:26:40.148627 140449214740288 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_51/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0328 23:26:40.148960 140449214740288 calibrate.py:118] Post calibration analysis complete.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0328 23:26:40.149318 140449214740288 calibrate.py:213] Succeeded in calibrating the model in PyTorch!\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mFine tuning is disabled in the config. Skipping QAT fine tuning.\u001b[0m\n",
      "W0328 23:26:40.155524 140449214740288 fine_tune.py:92] Fine tuning is disabled in the config. Skipping QAT fine tuning.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0328 23:26:40.159881 140449214740288 quantize.py:209] Converting PyTorch model to ONNX...\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_43/model.onnx\u001b[0m\n",
      "I0328 23:26:48.207860 140449214740288 quantize.py:239] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_43/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0328 23:26:48.210374 140449214740288 quantize.py:102] Converting PyTorch model to TensorRT...\n",
      "[03/28/2024-23:26:55] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_40/model.trt\u001b[0m\n",
      "I0328 23:06:32.223787 139939454809920 quantize.py:202] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_40/model.trt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_41/model.json\u001b[0m\n",
      "I0328 23:06:32.581682 139939454809920 quantize.py:259] TensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/2024-03-28/version_41/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_46/model.json\u001b[0m\n",
      "I0328 23:06:47.114224 139939454809920 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_46/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on vgg7-trt_quantized\u001b[0m\n",
      "I0328 23:06:47.208054 139939454809920 runtime_analysis.py:357] Starting transformation analysis on vgg7-trt_quantized\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results vgg7-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.91823   |\n",
      "|      Average Precision       |   0.9121    |\n",
      "|        Average Recall        |   0.90467   |\n",
      "|       Average F1 Score       |   0.92419   |\n",
      "|         Average Loss         |   0.24202   |\n",
      "|       Average Latency        |  8.2307 ms  |\n",
      "|   Average GPU Power Usage    |  55.687 W   |\n",
      "| Inference Energy Consumption | 0.12102 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0328 23:07:00.676242 139939454809920 runtime_analysis.py:521] \n",
      "Results vgg7-trt_quantized:\n",
      "+------------------------------+-------------+\n",
      "|      Metric (Per Batch)      |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.91823   |\n",
      "|      Average Precision       |   0.9121    |\n",
      "|        Average Recall        |   0.90467   |\n",
      "|       Average F1 Score       |   0.92419   |\n",
      "|         Average Loss         |   0.24202   |\n",
      "|       Average Latency        |  8.2307 ms  |\n",
      "|   Average GPU Power Usage    |  55.687 W   |\n",
      "| Inference Energy Consumption |  0.12102 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/tensorrt/version_8/model.json\u001b[0m\n",
      "I0328 23:07:00.677799 139939454809920 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/tensorrt/version_8/model.json\n"
     ]
    }
   ],
   "source": [
    "VGG_LAYERWISE_TOML = \"../../../machop/configs/tensorrt/vgg7_layerwise_mixed_precision.toml\"\n",
    "\n",
    "!python /root/local/mase-gp/src/ch transform --config {VGG_LAYERWISE_TOML} --load {VGG_CHECKPOINT_PATH} --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see through the quantized summary that one convolutional layer (feature_layers_1) has not been quantized as its precision will be configured to 'fp16' in the tensorrt engine conversion stage whilst the remaining convolutional and linear layers have been quantized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
